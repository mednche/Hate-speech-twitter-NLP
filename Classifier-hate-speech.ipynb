{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier to recognise hate speech on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.3.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mednche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import operator \n",
    "\n",
    "import nltk\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mednche\\\\Desktop\\\\Hate-speech-twitter-NLP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(\"/Users/natachachenevoy/Documents/Twitter Analysis/Classifier\")\n",
    "os.chdir(\"C:/Users/mednche/Desktop/Hate-speech-twitter-NLP/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pre-labeled Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainingTweets.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>well yes i mean you started off saying third l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fucking fascist fucking liberal fucking racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>fucking annoying when meat dairy and eggs are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i hate people i was wrong when i said 97.5 of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "305  well yes i mean you started off saying third l...      0\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "307  fucking fascist fucking liberal fucking racist...      1\n",
       "308  fucking annoying when meat dairy and eggs are ...      0\n",
       "309  i hate people i was wrong when i said 97.5 of ...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that each tweet has already been manually labeled in the column 'class'.\n",
    "- 1 means hateful\n",
    "- 0 means non-hateful\n",
    "It is essential for what we are going to do (supervised machine learning) that the data be labeled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Ensure balance in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many hateful vs non-hateful tweets there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 hateful tweets\n",
      "209 non-hateful tweets\n"
     ]
    }
   ],
   "source": [
    "pos_tweets = data[data[\"class\"] == 1]\n",
    "print(\"{} hateful tweets\".format(len(pos_tweets)))\n",
    "\n",
    "neg_tweets = data[data[\"class\"] == 0]\n",
    "print(\"{} non-hateful tweets\".format(len(neg_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims go to fucking home bye bye #brexit'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Muslim scum terrorists'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Go home you immigrant'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Polish vermin'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Polish bastard'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet  class\n",
       "0  Muslims go to fucking home bye bye #brexit'      1\n",
       "1                     'Muslim scum terrorists'      1\n",
       "2                      'Go home you immigrant'      1\n",
       "3                              'Polish vermin'      1\n",
       "4                             'Polish bastard'      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>'@16po @realDonaldTrump I don\\'t_ terrorism is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>'I am absolutely dreading tomorrow. I hate it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>'RT @washingtonpost: The Rev. William Barber d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>'RT @RedP1llReport: David Icke_ Political Corr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>'My best RTs this week came from: @SkimmySkinn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  class\n",
       "59  '@16po @realDonaldTrump I don\\'t_ terrorism is...      0\n",
       "60  'I am absolutely dreading tomorrow. I hate it ...      0\n",
       "61  'RT @washingtonpost: The Rev. William Barber d...      0\n",
       "62  'RT @RedP1llReport: David Icke_ Political Corr...      0\n",
       "63  'My best RTs this week came from: @SkimmySkinn...      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Select as many hateful as non-hateful tweets for an equal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in balanced dataset: 202\n"
     ]
    }
   ],
   "source": [
    "num = min(len(pos_tweets), len(neg_tweets))\n",
    "\n",
    "pos_tweets = shuffle(pos_tweets)\n",
    "neg_tweets = shuffle(neg_tweets)\n",
    "\n",
    "data_balanced = pos_tweets[0:num].append(neg_tweets[0:num], ignore_index=True)\n",
    "\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They used to tie both ends of a niggers legs t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Fucking niggers'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"I hate immigrants\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good night fags and fagettes (that's the femal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck off you cunts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  They used to tie both ends of a niggers legs t...      1\n",
       "1                                  'Fucking niggers'      1\n",
       "2                                \"I hate immigrants\"      1\n",
       "3  Good night fags and fagettes (that's the femal...      1\n",
       "4                                 fuck off you cunts      1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to ''\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Remove the RT before the @user \n",
    "    tweet = re.sub('rt','',tweet) \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove non ASCII characters (emojies)\n",
    "    tweet= re.sub(r'[^\\x00-\\x7F]+','', tweet)\n",
    "    #Remove punctuation \n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    #Trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #Remove beginning and end space\n",
    "    tweet = tweet.strip()\n",
    "\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "#  apply cleaning function on each tweet of the pandas dataframe\n",
    "data_balanced['tweet'] = data_balanced['tweet'].apply(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they used to tie both ends of a niggers legs t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fucking niggers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i hate immigrants</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>good night fags and fagettes thats the female ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck off you cunts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  they used to tie both ends of a niggers legs t...      1\n",
       "1                                    fucking niggers      1\n",
       "2                                  i hate immigrants      1\n",
       "3  good night fags and fagettes thats the female ...      1\n",
       "4                                 fuck off you cunts      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty tweets ('') by NA\n",
    "data_balanced['tweet'].replace('', np.nan, inplace=True)\n",
    "# Delete all NA rows\n",
    "data_balanced.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How many empty tweets were removed in the process? (Hint: use the shape attribute of the pandas dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the text of each tweet is a string. We would like to separate each word in that string so the model can 'read' them separately. \n",
    "\n",
    "In NLP, this is called 'tokenising': each tweet (intially a string of text) is chopped into a list of tokens (i.e. a list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised = data_balanced.copy()\n",
    "\n",
    "data_tokenised['tweet'] = data_tokenised['tweet'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[they, used, to, tie, both, ends, of, a, nigge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fucking, niggers]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, hate, immigrants]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[good, night, fags, and, fagettes, thats, the,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[fuck, off, you, cunts]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  [they, used, to, tie, both, ends, of, a, nigge...      1\n",
       "1                                 [fucking, niggers]      1\n",
       "2                              [i, hate, immigrants]      1\n",
       "3  [good, night, fags, and, fagettes, thats, the,...      1\n",
       "4                            [fuck, off, you, cunts]      1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that there are many words in the tweets that don't bring any meaning such as 'it', 'i', 'of' 'to' etc. These are called stopwords and need to be removed so that the classifier can focus on words that matter when telling the difference between hate and non-hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'has', 'but', \"haven't\", 'couldn', 'only', 'with', 'she', 'doesn', 'through', 'below', 'should', \"it's\", 'there', 'some', 'doing', \"wouldn't\", 'being', 'during', 'those', 'for', 'yourself', 'theirs', 'an', 'further', 'above', 'and', 'ain', 'which', 'against', 'your', 'after', \"should've\", \"you're\", 'before', 'does', 'is', 'once', 'was', \"mightn't\", 'm', 'such', 'when', 'down', 've', 'why', 'who', \"isn't\", 'am', 'yourselves', 'don', 'more', 'from', 'so', 'about', \"didn't\", 'while', 'himself', 'on', \"you'd\", 'a', 'me', \"you've\", 'shouldn', 'of', 'by', 's', 'each', \"she's\", 'wasn', 't', \"mustn't\", 'having', \"weren't\", 'over', 'than', 'too', 'both', 'until', \"hadn't\", 'how', 'few', 'it', 'isn', 'were', 'do', 'own', \"needn't\", 'didn', 'itself', 'herself', 'him', 'whom', 'mightn', 'themselves', 'what', 'o', 'again', 'd', 'you', 'off', 'at', \"couldn't\", 'hadn', 'to', 'i', 'had', 'ma', 'up', 'they', 'ours', 'them', 'y', \"aren't\", 'any', 'wouldn', 'won', 'myself', 'my', 'no', \"shan't\", \"wasn't\", 'between', 'can', 'll', 'here', 'most', 'have', 'now', 'then', 'because', \"hasn't\", 'we', 'their', 'her', 'hers', 'the', 'out', 'needn', 'hasn', 'he', 'if', 'that', 'just', 're', 'been', 'into', 'in', 'our', 'weren', 'other', 'or', 'did', 'very', 'ourselves', 'these', 'its', 'all', 'are', \"you'll\", 'mustn', 'his', 'will', \"won't\", 'not', 'aren', \"don't\", 'as', \"shouldn't\", 'this', \"that'll\", 'under', 'where', 'nor', 'shan', 'yours', 'be', \"doesn't\", 'haven', 'same'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these generic English stopwords could actually be useful in our context of hatespeech. For example 'them', 'out', 'off' could all be part of sentences like 'fuck off'. We will take these out of the list of stopwords. Also, we'll add our own stopwords based on some common spelling errors observed in the tweets ('youre', 'dont', 'us')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'aint', 'all', 'am', 'amp', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'cant', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doesnt', 'doing', 'don', \"don't\", 'dont', 'down', 'during', 'each', 'few', 'for', 'further', 'gonna', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'hes', 'him', 'himself', 'his', 'how', 'https', 'i', 'if', 'im', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'over', 'own', 'r', 're', 's', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'shouldnt', 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'thats', 'the', 'their', 'theirs', 'then', 'there', 'theres', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'u', 'under', 'until', 'up', 'ur', 'us', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'wudnt', 'ww', 'y', 'ya', \"you'd\", \"you'll\", \"you're\", \"you're\", \"you've\", 'your', 'youre', 'yours', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#remove these from stopwords\n",
    "item_to_delete = ['you', 'out', 'off', 'them', 'themselves', 'yourself', 'from', 'same']\n",
    "stopWords = [e for e in stops if e not in item_to_delete]\n",
    "\n",
    "# Add these to stopwords\n",
    "item_to_add = [\"youre\", \"r\", \"you're\", \"us\", \"doesnt\", \"im\", \"hes\", \"u\", \"ya\", \"ww\", \"dont\", \"https\", \"aint\", \"theres\", \"shouldnt\", \"thats\", \"amp\", \"wudnt\", \"gonna\", \"ur\", \"cant\"]\n",
    "for e in item_to_add:\n",
    "    stopWords.append(e)\n",
    "\n",
    "print(sorted(stopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokenised_stpwd = data_tokenised.copy()\n",
    "\n",
    "data_tokenised_stpwd['tweet'] = data_tokenised_stpwd['tweet'].apply(lambda x: [item for item in x if item not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[used, tie, ends, niggers, legs, 2, different,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fucking, niggers]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hate, immigrants]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[good, night, fags, fagettes, female, version,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[fuck, off, you, cunts]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  [used, tie, ends, niggers, legs, 2, different,...      1\n",
       "1                                 [fucking, niggers]      1\n",
       "2                                 [hate, immigrants]      1\n",
       "3  [good, night, fags, fagettes, female, version,...      1\n",
       "4                            [fuck, off, you, cunts]      1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised_stpwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, stemming is the process of turning words back into their stem, base or root form.\n",
    "Examples:\n",
    "- 'cats' --> 'cat'\n",
    "- 'fishing', 'fished' --> 'fish'\n",
    "\n",
    "This step is important so the classifier understands that the singualr and the plural form of a noun are the same concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_data = data_tokenised_stpwd.copy()\n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "pre_processed_data['tweet'] = pre_processed_data['tweet'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [use, tie, end, nigger, leg, 2, differ, hors, ...\n",
       "1                                       [fuck, nigger]\n",
       "2                                       [hate, immigr]\n",
       "3    [good, night, fag, fagett, femal, version, fag...\n",
       "4                               [fuck, off, you, cunt]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the pre-processing. We now have a dataset that has been cleaned, stemmed and tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model to learn anything, we need to give it a set of criteria to use in deciding whether a tweet is hateful or not. This kind of criteria is known as feature. We can define one or more feature(s) to train our classifier.\n",
    "\n",
    "In Part 2., we'll see how to convert the words into features so that we can feed it to a classifier for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare the data to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What feature shall we give to the model?\n",
    "\n",
    "We could give it a list of words but text cannot be used by machine learning models. They expect their input to be numeric. So we need to transform words into numeric features in a meaningful way. \n",
    "\n",
    "To do so, we are going to set a list of words (called vocabulary) and provide the classifier with boolean values indicating whether a each feature of the vobaculary is present or not: \n",
    "- 'bastard' : True (present)\n",
    "- 'road' : False (abscent)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a vocabulary: a set of unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1206\n"
     ]
    }
   ],
   "source": [
    "vocab = [word for tweet in pre_processed_data['tweet'] for word in tweet]\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use', 'tie', 'end', 'nigger', 'leg', '2', 'differ', 'hors', 'beat', 'hors', 'spread', 'leg', 'break', 'smh', 'fuck', 'nigger', 'hate', 'immigr', 'good', 'night', 'fag', 'fagett', 'femal', 'version', 'fag', 'like', 'fagett', 'make', 'femal', 'hate', 'you', 'fuck', 'off', 'you', 'cunt', 'nigger', 'you', 'fuck', 'irish', 'cow', 'enough', 'fuck', 'paki', 'man', 'time', 'leav', 'europ', 'muslim', 'scum', 'terrorist', 'hate', 'faggot', 'like', 'you', 'black', 'bitch', 'next', 'shut', 'fuck', 'paki', 'cunt', 'least', 'look', 'like', 'jefre', 'starr', 'faggot', 'saw', 'graffiti', 'train', 'today', 'said', 'fuck', 'off', 'home', 'you', 'black', 'slagfound', 'pretti', 'funni', 'you', 'ass', 'prick', 'muslim', 'terrorist', 'same', 'fuck', 'terrorist', 'everywher', 'get', 'rid', 'them', 'bastard', 'muslim', 'fuck', 'faggot', 'sit', 'cinema', 'next', 'fuck', 'fascist', 'fuck', 'liber', 'fuck', 'racist', 'fuck', 'traitor', 'fuck', 'petrolhead', 'fuck', 'treehugg', 'steve', 'you', 'steph', 'mate', 'polish', 'bastard', 'polish', 'vermin', 'depo', 'bastard', 'immigr', 'keep', 'britain', 'british', 'fat', 'littl', 'wanker', 'you', 'problem', 'say', 'face', 'behind', 'back', 'faggot', 'local', 'paki', 'shop', 'disgust', 'even', 'allow', 'countri', 'colour', 'fucker', 'leav', 'countri', 'sli', 'bitch', 'you', 'fuck', 'faggot', 'fuck', 'off', 'back', 'europ', 'you', 'job', 'steal', 'immigr', 'muslim', 'go', 'fuck', 'home', 'bye', 'bye', 'brexit', 'go', 'home', 'you', 'immigr', 'scottish', 'bastard', 'get', 'rid', 'them', 'along', 'europ', 'cunt', 'fuck', 'off', 'stupid', 'bitch', 'fuck', 'white', 'bitch', 'road', 'need', 'out', 'fuck', 'paki', 'fuck', 'chink', 'fuck', 'jezani', 'cunt', 'cbb', 'mani', 'chink', 'london', 'fuck', 'dyke', 'hate', 'nigger', 'hate', 'faggot', 'hate', 'spicskkk', 'ralli', 'fuck', 'sand', 'monkey', 'immigr', 'fuck', 'terrorist', 'fuck', 'off', 'myriah', 'chavcentr', 'chav', 'central', 'you', 'muslim', 'prick', 'you', 'even', 'countri', 'you', 'paki', 'come', 'out', 'you', 'black', 'bastard', 'nigger', 'everywher', 'day', 'make', 'britain', 'great', 'fuck', 'you', 'nigger', 'sheboon', 'hope', 'you', 'strung', 'like', 'nigger', 'whitepow', '1488', 'fuck', 'off', 'you', 'fuck', 'paki', 'bastard', 'attent', 'seek', 'bitch', 'paki', 'cunt', 'hate', 'bitch', 'shut', 'fuck', 'mani', 'wog', 'leav', 'countri', 'brexit', 'tire', 'polish', 'bitch', 'fuck', 'off', 'kill', 'you', 'you', 'paki', 'bastard', 'fuck', 'polish', 'alien', 'come', 'take', 'job', 'unaccept', 'bastard', 'muslim', 'dog', 'cheat', 'fuck', 'stupid', 'ass', 'prick', 'brexit', 'time', 'fuck', 'off', 'you', 'polish', 'alien', 'last', 'time', 'preston', 'anymor', 'inconsider', 'twat', 'm6m61', 'way', 'home', 'gon', 'na', 'cut', 'bitch', 'nigger', 'road', 'better', 'stop', 'shout', 'go', 'round', 'shut', 'them', 'leav', 'fuck', 'countri', 'you', 'white', 'bunch', 'paki', 'perv', 'least', 'nigger', 'lmfao', 'fuck', 'immigr', 'smelli', 'paki', 'fuck', 'off', 'you', 'back', 'paki', 'bastard', 'fuck', 'off', 'home', 'you', 'foreign', 'bastard', 'fuck', 'bitch', 'fuck', 'hate', 'you', 'nigger', 'bruh', 'get', 'out', 'countri', 'you', 'fuck', 'foreign', 'polish', 'bitch', 'fuck', 'off', 'immigr', 'brexit', 'go', 'fuck', 'yourself', 'you', 'cunt', 'fuck', 'hate', 'allah', 'lover', 'thick', 'scottish', 'bastard', 'fuck', 'off', 'you', 'cunt', 'diy', 'terrorist', 'religion', 'fuck', 'joke', 'you', 'go', 'around', 'scream', 'allah', 'akbar', 'terrorist', 'shit', 'diy', 'faggot', 'polish', 'scum', 'hate', 'immigr', 'fuck', 'anyon', 'british', 'bastard', 'muslim', 'terrorist', 'bastard', 'wog', 'road', 'annoy', 'hate', 'immigr', 'hate', 'yellow', 'fucker', 'neighbour', 'call', 'paki', 'peopl', 'respect', 'racist', 'sexist', 'perv', 'clearli', 'want', 'fuck', 'daughter', 'disgust', 'fuck', 'off', 'back', 'poland', 'paki', 'cunt', 'wear', 'veil', 'benjamin', 'fuck', 'faggot', 'black', 'nigger', 'bitch', 'shut', 'nigger', 'whore', 'hope', 'get', 'rape', 'one', 'anim', 'might', 'chang', 'tune', 'fuck', 'off', 'you', 'scum', 'man', 'call', 'black', 'bastard', 'go', 'back', 'countri', 'rude', 'local', 'mosqu', 'let', 'terrorist', 'out', 'diy', 'paki', 'paki', 'fuck', 'off', 'pleas', 'hate', 'peopl', 'wrong', 'said', '975', 'peopl', 'cunt', 'much', 'much', 'higher', 'neighbour', 'complain', 'shed', 'front', 'garden', 'who', 'neighbour', 'church', 'england', 'bunch', 'god', 'bother', 'cunt', 'go', 'burn', 'hell', 'bitch', 'scum', 'game', 'throne', 'univers', 'write', 'margat', 'aug', '56', 'sponsor', 'anni', 'make', 'browni', 'you', 'expect', 'employ', 'photograph', 'crazi', 'car', 'crash', 'compil', 'pa', '321', 'fuck', 'qualiti', 'big', 'man', 'job', 'might', 'great', 'fit', 'you', 'senior', 'engin', 'engin', 'westgreenwich', 'ri', 'hire', 'careerarc', 'go', 'home', 'oreo', 'drunk', 'spend', 'live', 'live', 'gangsta', 'paradis', 'littl', 'ive', 'switch', 'off', 'still', 'work', 'well', 'last', 'weekend', 'take', 'advantag', '10', 'off', 'discount', 'believ', 'mani', 'never', 'turn', 'back', 'bird', 'sea', 'shepherd', 'conserv', 'societi', 'terrorist', 'organizationwh', 'love', '70', 'polyest', 'suit', 'profil', 'you', 'go', 'far', 'noh', 'pier', 'exampl', 'kinda', 'glad', 'difficult', 'deal', 'understand', 'know', 'someon', 'stay', 'around', 'truli', 'want', 'promis', 'p', 'dude', 'smoke', 'reggi', 'bush', 'you', 'love', 'someon', 'tell', 'them', 'thing', 'right', 'time', 'right', 'place', 'long', 'pope', 'franci', 'visit', 'former', 'german', 'nazi', 'concentr', 'camp', 'auschwitzbirkenau', 'even', 'midnight', 'east', 'coast', 'basebal', 'alreadi', 'antiauster', 'toryscum', 'innov', 'from', 'cwt', 'reduc', 'personalis', 'travel', 'cost', 'go', 'holiday', '15', 'day', 'like', 'even', 'readi', 'weedl', 'pidgey', 'scum', 'eah', 'mother', 'juandaearli', 'caller', 'learn', 'hate', 'polit', 'correctnessbut', 'love', 'common', 'decencyampcommon', 'sens', 'morn', 'alarm', 'went', 'off', '0230', 'didnt', 'know', 'fuck', 'go', 'x', 'fuck', 'leagu', 'meme', 'hilari', 'merri', 'achrostmad', 'villa', 'countri', 'hous', 'hotel', 'wrea', 'green', 'retweet', 'gazet', 'krtk', 'baskn', 'oran', 'akp', 'de', 'fethullahyd', 'specialist', 'intim', 'wax', 'guidenot', 'sure', 'much', 'take', 'off', 'mayb', 'much', 'leav', 'technolog', 'decis', 'leader', 'hold', 'let', 'go', 'fuck', 'huge', 'men', 'hate', 'women', 'worldwid', 'south', 'korea', 'contend', 'gamerg', 'tshi', 'npr', 'get', 'antialias', 'font', 'from', 'googl', 'font', 'devseo', 'blog', 'get', 'distanc', 'two', 'address', 'devseo', 'blog', 'saw', 'crow', 'gallop', 'across', 'road', 'mmkay', 'wut', '128514', 'get', 'chelsea', 'ye', 'funni', 'hold', 'shi', 'chest', 'seen', 'strip', 'second', 'ago', 'looool', 'scoobi', 'doo', 'dog', 'bitch', 'forget', 'get', 'jar', 'fyld', 'coast', 'green', 'tomato', 'chutney', 'vintag', '2', 'year', 'winckley', 'wow', 'come', 'long', 'way', 'far', 'go', 'operationdiningroom', 'compani', 'still', 'hasnt', 'kick', 'off', 'back', 'shithol', 'go', 'x', 'rohith', 'vemula', 'awaken', 'let', 'sacrific', 'go', 'vain', 'dalitsnotcow', 'muslimsnotcow', 'dalitmusl', 'get', 'you', 'out', 'hand', 'nuala', 'nowplay', 'sun', 'lancast', 'seminari', 'bore', 'out', 'fuck', 'mind', 'help', 'unlucki', 'peopl', 'total', 'forgot', 'reflect', 'exist', 'photo', 'pic', 'preview', 'park', 'bo', 'gum', 'kim', 'yoo', 'jung', 'jinyoung', 'b1a4', 'di', 'drama', 'kb', 'moonlight', 'drawn', 'cloud', 'kwi', 'mani', 'lad', 'you', 'fight', 'off', 'suitabl', 'impress', 'number', 'may', 'convinc', 'lend', 'suppo', 'rhose', 'word', 'end', 'you', 'think', 'wrong', 'want', 'work', 'citi', 'hire', 'jacksonvil', 'fl', 'click', 'detail', 'job', 'citicar', 'job', 'careerarc', 'peopl', 'uncivil', 'david', 'duke', 'wouldnt', 'republican', 'vote', 'new', 'reason', 'tlot', 'amagi', 'tcot', 'tire', 'thank', 'you', 'post', 'fine', 'exampl', 'braveri', 'inspir', 'see', 'firefli', 'squid', 'japan', 'look', 'fab', 'id', 'polish', 'off', 'worri', 'fact', 'work', 'out', 'chanyeol', 'achiev', 'give', 'chill', 'news', 'review', 'car', 'review', 'sd', 'v', 'fpace', 'rr', 'porsch', 'macan', 'googl', 'adsens', 'mobil', 'text', 'ad', 'get', 'new', 'look', 'right', 'bear', 'arm', 'kill', 'mani', 'peopl', 'you', 'usconstitut', 'nono', 'yo', 'organic', 'el', 'horario', 'meejooor', 'que', 'te', 'haya', 'quedado', 'asii', 'tess2016', 'you', 'need', 'prove', 'consist', 'time', 'scare', 'father', 'christma', 'even', 'though', 'want', 'go', 'hous', 'xxxx', 'white', 'hat', 'seo', 'vs', 'black', 'hat', 'seo', 'less', '100', 'word', 'never', 'never', 'accept', 'alway', 'repo', 'racist', 'islamophob', 'hate', 'crime', 'alwaysalway', 'fuck', 'annoy', 'meat', 'dairi', 'egg', 'tourou', 'anim', 'id', 'love', 'follow', 'paleo', 'diet', 'danger', 'vegan', 'bigger', 'ever', 'shoe', 'sale', '75', 'off', 'mani', 'shoe', 'gorgeou', 'elderrumb', 'wordno', 'sho', 'outreach', 'email', 'get', 'you', 'link', 'sho', 'outreach', 'email', 'get', 'you', 'link', 'frontmen', 'go', 'freddi', 'mercuri', 'immens', 'construct', 'i86', 'direct', 'from', 'town', 'ellicott', 'town', 'poland', 'line', 'exit', '14', 'ny', '62', 'frewsburg', 'road', 'time', 'set', 'off', 'off', 'bed', 'big', 'day', 'tomorrow', 'villa', 'home', 'dublin', 'away', 'pnefc', 'mayo4sam', '6', 'lesson', 'learn', 'from', 'rais', '2', 'million', 'entrepreneur', 'alway', 'good', 'day', 'you', 'get', 'phantom', 'out', 'avenham', 'park', 'off', 'you', 'go', 'christma', 'thankx', 'lot', 'wonder', 'tipshealthtipsbymsgil', 'sure', 'follow', 'live', 'webcam', 'rev', 'william', 'barber', 'drop', 'mic', 'fresh', 'out', 'pack', 'bed', 'fuck', 'yeah', 'remind', 'prospect', 'arent', 'woh', 'nearli', 'much', 'you', 'think', 'callin', 'daddi', 'daddi', 'need', 'daddi', 'wont', 'you', 'daddi', 'daddi', 'come', 'make', 'rain', 'one', 'hour', 'go', 'lectur', 'end', 'januari', 'go', 'drag', 'freedom', 'fuck', 'metal', 'imagin', 'train', 'prepar', 'one', 'dig', 'take', 'immigr', 'poignantli', 'point', 'out', 'disgrac', 'moral', 'pygmi', 'paul', 'ryan', 'mitch', 'mcconnel', 'realli', 'enjoy', 'box', 'tonight', 'great', 'fight', 'joshuawhyt', 'let', 'get', 'readi', 'rumbl', 'joshua', 'forget', 'robot', 'lyft', 'coupon', 'free', '50', 'ride', 'credit', 'lyft', 'exp', '222', 'coupon', 'lie', 'trump', 'out', 'perman', 'ban', 'muslim', 'immigr', 'let', 'fool', 'you', 'scarlettjohansson', 'time', 'test', 'advic', 'becom', 'physic', 'fit', 'scarlett', 'johansson', 'realli', 'gerald', 'life', 'kyungi', 'turn', 'yall', 'proud', 'baekhyun', 'work', 'ab', 'gotten', 'realli', 'skinni', 'pope', 'wrap', 'proimmigr', 'messag', 'polish', 'flag', 'crux', 'cover', 'thing', 'cathol', 'crux', 'cover', 'th']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary contains a list of all unique words in our pre-processed tweets. You'll notice some of the words don't look very english. It's because they are the stem of the initial word (recall the stemming process)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of hate in the English language is more complex than just presence of a word. Sometimes it's the combination of 2 or more words that becomes hateful. For example 'shut up', 'fuck off' or 'send them home'. \n",
    "\n",
    "In NLP, these combinations of 2 or more words are called ngrams:\n",
    "- bigram: ('back', 'off')\n",
    "- trigram: ('send', 'them', 'home')\n",
    "\n",
    "We need to add bigrams and trigrams to our vocabulary of features alongside single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mngrams\u001b[1;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-7f859c653806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processed_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vocabulary size: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-7f859c653806>\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(tweets)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#trigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "def get_vocabulary(tweets):\n",
    "    all_words = []\n",
    "    for word_list in tweets:\n",
    "        # unigrams\n",
    "        all_words.extend(word_list)\n",
    "        \n",
    "        # bigrams\n",
    "        bigrams = list(ngrams(word_list, 2))\n",
    "        \n",
    "        #trigrams \n",
    "        trigrams = list(ngrams(word_list, 3))\n",
    "        \n",
    "        all_words.extend(bigrams)\n",
    "        all_words.extend(trigrams)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "vocab = get_vocabulary(pre_processed_data['tweet'])\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbour',\n",
       " 'called',\n",
       " 'paki',\n",
       " 'people',\n",
       " 'respect',\n",
       " ('neighbour', 'called'),\n",
       " ('called', 'paki'),\n",
       " ('paki', 'people'),\n",
       " ('people', 'respect'),\n",
       " ('neighbour', 'called', 'paki'),\n",
       " ('called', 'paki', 'people'),\n",
       " ('paki', 'people', 'respect'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'cheat',\n",
       " 'fucking',\n",
       " 'stupid',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('cheat', 'fucking'),\n",
       " ('fucking', 'stupid'),\n",
       " ('stupid', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('cheat', 'fucking', 'stupid'),\n",
       " ('fucking', 'stupid', 'ass'),\n",
       " ('stupid', 'ass', 'prick'),\n",
       " 'saw',\n",
       " 'graffiti',\n",
       " 'train',\n",
       " 'today',\n",
       " 'said',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'black',\n",
       " 'slagfound',\n",
       " 'pretty',\n",
       " 'funny',\n",
       " ('saw', 'graffiti'),\n",
       " ('graffiti', 'train'),\n",
       " ('train', 'today'),\n",
       " ('today', 'said'),\n",
       " ('said', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'home'),\n",
       " ('home', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'slagfound'),\n",
       " ('slagfound', 'pretty'),\n",
       " ('pretty', 'funny'),\n",
       " ('saw', 'graffiti', 'train'),\n",
       " ('graffiti', 'train', 'today'),\n",
       " ('train', 'today', 'said'),\n",
       " ('today', 'said', 'fuck'),\n",
       " ('said', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'home'),\n",
       " ('off', 'home', 'you'),\n",
       " ('home', 'you', 'black'),\n",
       " ('you', 'black', 'slagfound'),\n",
       " ('black', 'slagfound', 'pretty'),\n",
       " ('slagfound', 'pretty', 'funny'),\n",
       " 'hate',\n",
       " 'people',\n",
       " 'wrong',\n",
       " 'said',\n",
       " '975',\n",
       " 'people',\n",
       " 'cunts',\n",
       " 'much',\n",
       " 'much',\n",
       " 'higher',\n",
       " ('hate', 'people'),\n",
       " ('people', 'wrong'),\n",
       " ('wrong', 'said'),\n",
       " ('said', '975'),\n",
       " ('975', 'people'),\n",
       " ('people', 'cunts'),\n",
       " ('cunts', 'much'),\n",
       " ('much', 'much'),\n",
       " ('much', 'higher'),\n",
       " ('hate', 'people', 'wrong'),\n",
       " ('people', 'wrong', 'said'),\n",
       " ('wrong', 'said', '975'),\n",
       " ('said', '975', 'people'),\n",
       " ('975', 'people', 'cunts'),\n",
       " ('people', 'cunts', 'much'),\n",
       " ('cunts', 'much', 'much'),\n",
       " ('much', 'much', 'higher'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'terrorists'),\n",
       " ('bastard', 'muslim', 'terrorists'),\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " 'same',\n",
       " ('muslim', 'terrorists'),\n",
       " ('terrorists', 'same'),\n",
       " ('muslim', 'terrorists', 'same'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'wearing',\n",
       " 'veil',\n",
       " ('paki', 'cunt'),\n",
       " ('cunt', 'wearing'),\n",
       " ('wearing', 'veil'),\n",
       " ('paki', 'cunt', 'wearing'),\n",
       " ('cunt', 'wearing', 'veil'),\n",
       " 'fucking',\n",
       " 'fascist',\n",
       " 'fucking',\n",
       " 'liberal',\n",
       " 'fucking',\n",
       " 'racist',\n",
       " 'fucking',\n",
       " 'traitor',\n",
       " 'fucking',\n",
       " 'petrolhead',\n",
       " 'fucking',\n",
       " 'treehugger',\n",
       " 'steve',\n",
       " 'you',\n",
       " 'steph',\n",
       " 'mate',\n",
       " ('fucking', 'fascist'),\n",
       " ('fascist', 'fucking'),\n",
       " ('fucking', 'liberal'),\n",
       " ('liberal', 'fucking'),\n",
       " ('fucking', 'racist'),\n",
       " ('racist', 'fucking'),\n",
       " ('fucking', 'traitor'),\n",
       " ('traitor', 'fucking'),\n",
       " ('fucking', 'petrolhead'),\n",
       " ('petrolhead', 'fucking'),\n",
       " ('fucking', 'treehugger'),\n",
       " ('treehugger', 'steve'),\n",
       " ('steve', 'you'),\n",
       " ('you', 'steph'),\n",
       " ('steph', 'mate'),\n",
       " ('fucking', 'fascist', 'fucking'),\n",
       " ('fascist', 'fucking', 'liberal'),\n",
       " ('fucking', 'liberal', 'fucking'),\n",
       " ('liberal', 'fucking', 'racist'),\n",
       " ('fucking', 'racist', 'fucking'),\n",
       " ('racist', 'fucking', 'traitor'),\n",
       " ('fucking', 'traitor', 'fucking'),\n",
       " ('traitor', 'fucking', 'petrolhead'),\n",
       " ('fucking', 'petrolhead', 'fucking'),\n",
       " ('petrolhead', 'fucking', 'treehugger'),\n",
       " ('fucking', 'treehugger', 'steve'),\n",
       " ('treehugger', 'steve', 'you'),\n",
       " ('steve', 'you', 'steph'),\n",
       " ('you', 'steph', 'mate'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunt'),\n",
       " 'man',\n",
       " 'called',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'back',\n",
       " 'country',\n",
       " 'rude',\n",
       " ('man', 'called'),\n",
       " ('called', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('bastard', 'go'),\n",
       " ('go', 'back'),\n",
       " ('back', 'country'),\n",
       " ('country', 'rude'),\n",
       " ('man', 'called', 'black'),\n",
       " ('called', 'black', 'bastard'),\n",
       " ('black', 'bastard', 'go'),\n",
       " ('bastard', 'go', 'back'),\n",
       " ('go', 'back', 'country'),\n",
       " ('back', 'country', 'rude'),\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'along',\n",
       " 'europe',\n",
       " ('scottish', 'bastard'),\n",
       " ('bastard', 'get'),\n",
       " ('get', 'rid'),\n",
       " ('rid', 'them'),\n",
       " ('them', 'along'),\n",
       " ('along', 'europe'),\n",
       " ('scottish', 'bastard', 'get'),\n",
       " ('bastard', 'get', 'rid'),\n",
       " ('get', 'rid', 'them'),\n",
       " ('rid', 'them', 'along'),\n",
       " ('them', 'along', 'europe'),\n",
       " 'many',\n",
       " 'chinks',\n",
       " 'london',\n",
       " ('many', 'chinks'),\n",
       " ('chinks', 'london'),\n",
       " ('many', 'chinks', 'london'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'back',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'back'),\n",
       " ('back', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'back'),\n",
       " ('you', 'back', 'paki'),\n",
       " ('back', 'paki', 'bastard'),\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'like',\n",
       " 'you',\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'like'),\n",
       " ('like', 'you'),\n",
       " ('hate', 'faggots', 'like'),\n",
       " ('faggots', 'like', 'you'),\n",
       " 'shut',\n",
       " 'nigger',\n",
       " 'whore',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'raped',\n",
       " 'one',\n",
       " 'animals',\n",
       " 'might',\n",
       " 'change',\n",
       " 'tune',\n",
       " ('shut', 'nigger'),\n",
       " ('nigger', 'whore'),\n",
       " ('whore', 'hope'),\n",
       " ('hope', 'get'),\n",
       " ('get', 'raped'),\n",
       " ('raped', 'one'),\n",
       " ('one', 'animals'),\n",
       " ('animals', 'might'),\n",
       " ('might', 'change'),\n",
       " ('change', 'tune'),\n",
       " ('shut', 'nigger', 'whore'),\n",
       " ('nigger', 'whore', 'hope'),\n",
       " ('whore', 'hope', 'get'),\n",
       " ('hope', 'get', 'raped'),\n",
       " ('get', 'raped', 'one'),\n",
       " ('raped', 'one', 'animals'),\n",
       " ('one', 'animals', 'might'),\n",
       " ('animals', 'might', 'change'),\n",
       " ('might', 'change', 'tune'),\n",
       " 'black',\n",
       " 'bitch',\n",
       " 'next',\n",
       " 'shut',\n",
       " ('black', 'bitch'),\n",
       " ('bitch', 'next'),\n",
       " ('next', 'shut'),\n",
       " ('black', 'bitch', 'next'),\n",
       " ('bitch', 'next', 'shut'),\n",
       " 'fucking',\n",
       " 'chinks',\n",
       " ('fucking', 'chinks'),\n",
       " 'coloured',\n",
       " 'fuckers',\n",
       " 'leave',\n",
       " 'country',\n",
       " ('coloured', 'fuckers'),\n",
       " ('fuckers', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('coloured', 'fuckers', 'leave'),\n",
       " ('fuckers', 'leave', 'country'),\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'cunt'),\n",
       " ('fucking', 'paki', 'cunt'),\n",
       " 'brexit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " ('brexit', 'time'),\n",
       " ('time', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('brexit', 'time', 'fuck'),\n",
       " ('time', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'polish'),\n",
       " ('you', 'polish', 'aliens'),\n",
       " 'black',\n",
       " 'nigger',\n",
       " ('black', 'nigger'),\n",
       " 'least',\n",
       " 'nigger',\n",
       " 'lmfao',\n",
       " ('least', 'nigger'),\n",
       " ('nigger', 'lmfao'),\n",
       " ('least', 'nigger', 'lmfao'),\n",
       " 'neighbours',\n",
       " 'complained',\n",
       " 'shed',\n",
       " 'front',\n",
       " 'garden',\n",
       " 'whos',\n",
       " 'neighbour',\n",
       " 'church',\n",
       " 'england',\n",
       " 'bunch',\n",
       " 'god',\n",
       " 'bothering',\n",
       " 'cunts',\n",
       " ('neighbours', 'complained'),\n",
       " ('complained', 'shed'),\n",
       " ('shed', 'front'),\n",
       " ('front', 'garden'),\n",
       " ('garden', 'whos'),\n",
       " ('whos', 'neighbour'),\n",
       " ('neighbour', 'church'),\n",
       " ('church', 'england'),\n",
       " ('england', 'bunch'),\n",
       " ('bunch', 'god'),\n",
       " ('god', 'bothering'),\n",
       " ('bothering', 'cunts'),\n",
       " ('neighbours', 'complained', 'shed'),\n",
       " ('complained', 'shed', 'front'),\n",
       " ('shed', 'front', 'garden'),\n",
       " ('front', 'garden', 'whos'),\n",
       " ('garden', 'whos', 'neighbour'),\n",
       " ('whos', 'neighbour', 'church'),\n",
       " ('neighbour', 'church', 'england'),\n",
       " ('church', 'england', 'bunch'),\n",
       " ('england', 'bunch', 'god'),\n",
       " ('bunch', 'god', 'bothering'),\n",
       " ('god', 'bothering', 'cunts'),\n",
       " 'bunch',\n",
       " 'paki',\n",
       " 'perves',\n",
       " ('bunch', 'paki'),\n",
       " ('paki', 'perves'),\n",
       " ('bunch', 'paki', 'perves'),\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'allah',\n",
       " 'lovers',\n",
       " ('fucking', 'hate'),\n",
       " ('hate', 'allah'),\n",
       " ('allah', 'lovers'),\n",
       " ('fucking', 'hate', 'allah'),\n",
       " ('hate', 'allah', 'lovers'),\n",
       " 'fucking',\n",
       " 'immigrants',\n",
       " ('fucking', 'immigrants'),\n",
       " 'fucking',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " 'coming',\n",
       " 'taking',\n",
       " 'jobs',\n",
       " 'unacceptable',\n",
       " ('fucking', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('aliens', 'coming'),\n",
       " ('coming', 'taking'),\n",
       " ('taking', 'jobs'),\n",
       " ('jobs', 'unacceptable'),\n",
       " ('fucking', 'polish', 'aliens'),\n",
       " ('polish', 'aliens', 'coming'),\n",
       " ('aliens', 'coming', 'taking'),\n",
       " ('coming', 'taking', 'jobs'),\n",
       " ('taking', 'jobs', 'unacceptable'),\n",
       " 'local',\n",
       " 'mosque',\n",
       " 'let',\n",
       " 'terrorists',\n",
       " 'out',\n",
       " ('local', 'mosque'),\n",
       " ('mosque', 'let'),\n",
       " ('let', 'terrorists'),\n",
       " ('terrorists', 'out'),\n",
       " ('local', 'mosque', 'let'),\n",
       " ('mosque', 'let', 'terrorists'),\n",
       " ('let', 'terrorists', 'out'),\n",
       " 'racist',\n",
       " 'sexist',\n",
       " 'perv',\n",
       " 'clearly',\n",
       " 'wants',\n",
       " 'fuck',\n",
       " 'daughter',\n",
       " 'disgusting',\n",
       " ('racist', 'sexist'),\n",
       " ('sexist', 'perv'),\n",
       " ('perv', 'clearly'),\n",
       " ('clearly', 'wants'),\n",
       " ('wants', 'fuck'),\n",
       " ('fuck', 'daughter'),\n",
       " ('daughter', 'disgusting'),\n",
       " ('racist', 'sexist', 'perv'),\n",
       " ('sexist', 'perv', 'clearly'),\n",
       " ('perv', 'clearly', 'wants'),\n",
       " ('clearly', 'wants', 'fuck'),\n",
       " ('wants', 'fuck', 'daughter'),\n",
       " ('fuck', 'daughter', 'disgusting'),\n",
       " 'pakis',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'please',\n",
       " ('pakis', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'please'),\n",
       " ('pakis', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'please'),\n",
       " 'hate',\n",
       " 'niggers',\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'hate',\n",
       " 'spicskkk',\n",
       " 'rally',\n",
       " ('hate', 'niggers'),\n",
       " ('niggers', 'hate'),\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'hate'),\n",
       " ('hate', 'spicskkk'),\n",
       " ('spicskkk', 'rally'),\n",
       " ('hate', 'niggers', 'hate'),\n",
       " ('niggers', 'hate', 'faggots'),\n",
       " ('hate', 'faggots', 'hate'),\n",
       " ('faggots', 'hate', 'spicskkk'),\n",
       " ('hate', 'spicskkk', 'rally'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunts',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunts'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunts'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'paki'),\n",
       " ('fucking', 'paki', 'bastard'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'myriah',\n",
       " 'chavcentral',\n",
       " 'chav',\n",
       " 'central',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'myriah'),\n",
       " ('myriah', 'chavcentral'),\n",
       " ('chavcentral', 'chav'),\n",
       " ('chav', 'central'),\n",
       " ('fuck', 'off', 'myriah'),\n",
       " ('off', 'myriah', 'chavcentral'),\n",
       " ('myriah', 'chavcentral', 'chav'),\n",
       " ('chavcentral', 'chav', 'central'),\n",
       " 'diy',\n",
       " 'terrorist',\n",
       " 'religion',\n",
       " 'fucking',\n",
       " 'joke',\n",
       " 'you',\n",
       " 'go',\n",
       " 'around',\n",
       " 'screaming',\n",
       " 'allah',\n",
       " 'akbar',\n",
       " 'terrorist',\n",
       " 'shit',\n",
       " 'diy',\n",
       " 'faggot',\n",
       " ('diy', 'terrorist'),\n",
       " ('terrorist', 'religion'),\n",
       " ('religion', 'fucking'),\n",
       " ('fucking', 'joke'),\n",
       " ('joke', 'you'),\n",
       " ('you', 'go'),\n",
       " ('go', 'around'),\n",
       " ('around', 'screaming'),\n",
       " ('screaming', 'allah'),\n",
       " ('allah', 'akbar'),\n",
       " ('akbar', 'terrorist'),\n",
       " ('terrorist', 'shit'),\n",
       " ('shit', 'diy'),\n",
       " ('diy', 'faggot'),\n",
       " ('diy', 'terrorist', 'religion'),\n",
       " ('terrorist', 'religion', 'fucking'),\n",
       " ('religion', 'fucking', 'joke'),\n",
       " ('fucking', 'joke', 'you'),\n",
       " ('joke', 'you', 'go'),\n",
       " ('you', 'go', 'around'),\n",
       " ('go', 'around', 'screaming'),\n",
       " ('around', 'screaming', 'allah'),\n",
       " ('screaming', 'allah', 'akbar'),\n",
       " ('allah', 'akbar', 'terrorist'),\n",
       " ('akbar', 'terrorist', 'shit'),\n",
       " ('terrorist', 'shit', 'diy'),\n",
       " ('shit', 'diy', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'poland',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'back'),\n",
       " ('back', 'poland'),\n",
       " ('fuck', 'off', 'back'),\n",
       " ('off', 'back', 'poland'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'scum',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'scum'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'scum'),\n",
       " 'shut',\n",
       " 'fuck',\n",
       " ('shut', 'fuck'),\n",
       " 'fucking',\n",
       " 'sand',\n",
       " 'monkeys',\n",
       " ('fucking', 'sand'),\n",
       " ('sand', 'monkeys'),\n",
       " ('fucking', 'sand', 'monkeys'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('you', 'fucking', 'faggot'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'irish',\n",
       " 'cow',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'irish'),\n",
       " ('irish', 'cow'),\n",
       " ('you', 'fucking', 'irish'),\n",
       " ('fucking', 'irish', 'cow'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('paki', 'cunt'),\n",
       " 'you',\n",
       " 'muslim',\n",
       " 'prick',\n",
       " ('you', 'muslim'),\n",
       " ('muslim', 'prick'),\n",
       " ('you', 'muslim', 'prick'),\n",
       " 'many',\n",
       " 'wogs',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'brexit',\n",
       " ('many', 'wogs'),\n",
       " ('wogs', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('country', 'brexit'),\n",
       " ('many', 'wogs', 'leave'),\n",
       " ('wogs', 'leave', 'country'),\n",
       " ('leave', 'country', 'brexit'),\n",
       " 'least',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jefree',\n",
       " 'starr',\n",
       " 'faggot',\n",
       " ('least', 'look'),\n",
       " ('look', 'like'),\n",
       " ('like', 'jefree'),\n",
       " ('jefree', 'starr'),\n",
       " ('starr', 'faggot'),\n",
       " ('least', 'look', 'like'),\n",
       " ('look', 'like', 'jefree'),\n",
       " ('like', 'jefree', 'starr'),\n",
       " ('jefree', 'starr', 'faggot'),\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " ('stupid', 'bitch'),\n",
       " 'benjamin',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('benjamin', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('benjamin', 'fucking', 'faggot'),\n",
       " 'good',\n",
       " 'night',\n",
       " 'fags',\n",
       " 'fagettes',\n",
       " 'female',\n",
       " 'version',\n",
       " 'fags',\n",
       " 'like',\n",
       " 'fagettes',\n",
       " 'make',\n",
       " 'female',\n",
       " 'hate',\n",
       " 'you',\n",
       " ('good', 'night'),\n",
       " ('night', 'fags'),\n",
       " ('fags', 'fagettes'),\n",
       " ('fagettes', 'female'),\n",
       " ('female', 'version'),\n",
       " ('version', 'fags'),\n",
       " ('fags', 'like'),\n",
       " ('like', 'fagettes'),\n",
       " ('fagettes', 'make'),\n",
       " ('make', 'female'),\n",
       " ('female', 'hate'),\n",
       " ('hate', 'you'),\n",
       " ('good', 'night', 'fags'),\n",
       " ('night', 'fags', 'fagettes'),\n",
       " ('fags', 'fagettes', 'female'),\n",
       " ('fagettes', 'female', 'version'),\n",
       " ('female', 'version', 'fags'),\n",
       " ('version', 'fags', 'like'),\n",
       " ('fags', 'like', 'fagettes'),\n",
       " ('like', 'fagettes', 'make'),\n",
       " ('fagettes', 'make', 'female'),\n",
       " ('make', 'female', 'hate'),\n",
       " ('female', 'hate', 'you'),\n",
       " 'thick',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " ('thick', 'scottish'),\n",
       " ('scottish', 'bastard'),\n",
       " ('thick', 'scottish', 'bastard'),\n",
       " 'bastard',\n",
       " 'wog',\n",
       " 'road',\n",
       " 'annoying',\n",
       " ('bastard', 'wog'),\n",
       " ('wog', 'road'),\n",
       " ('road', 'annoying'),\n",
       " ('bastard', 'wog', 'road'),\n",
       " ('wog', 'road', 'annoying'),\n",
       " 'polish',\n",
       " 'vermin',\n",
       " ('polish', 'vermin'),\n",
       " 'diy',\n",
       " 'paki',\n",
       " ('diy', 'paki'),\n",
       " 'used',\n",
       " 'tie',\n",
       " 'ends',\n",
       " 'niggers',\n",
       " 'legs',\n",
       " '2',\n",
       " 'different',\n",
       " 'horses',\n",
       " 'beat',\n",
       " 'horses',\n",
       " 'spread',\n",
       " 'legs',\n",
       " 'break',\n",
       " 'smh',\n",
       " ('used', 'tie'),\n",
       " ('tie', 'ends'),\n",
       " ('ends', 'niggers'),\n",
       " ('niggers', 'legs'),\n",
       " ('legs', '2'),\n",
       " ('2', 'different'),\n",
       " ('different', 'horses'),\n",
       " ('horses', 'beat'),\n",
       " ('beat', 'horses'),\n",
       " ('horses', 'spread'),\n",
       " ('spread', 'legs'),\n",
       " ('legs', 'break'),\n",
       " ('break', 'smh'),\n",
       " ('used', 'tie', 'ends'),\n",
       " ('tie', 'ends', 'niggers'),\n",
       " ('ends', 'niggers', 'legs'),\n",
       " ('niggers', 'legs', '2'),\n",
       " ('legs', '2', 'different'),\n",
       " ('2', 'different', 'horses'),\n",
       " ('different', 'horses', 'beat'),\n",
       " ('horses', 'beat', 'horses'),\n",
       " ('beat', 'horses', 'spread'),\n",
       " ('horses', 'spread', 'legs'),\n",
       " ('spread', 'legs', 'break'),\n",
       " ('legs', 'break', 'smh'),\n",
       " 'polish',\n",
       " 'bitch',\n",
       " ('polish', 'bitch'),\n",
       " 'last',\n",
       " 'time',\n",
       " 'preston',\n",
       " 'anymore',\n",
       " 'inconsiderate',\n",
       " 'twats',\n",
       " 'm6m61',\n",
       " 'way',\n",
       " 'home',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'cut',\n",
       " 'bitch',\n",
       " ('last', 'time'),\n",
       " ('time', 'preston'),\n",
       " ('preston', 'anymore'),\n",
       " ('anymore', 'inconsiderate'),\n",
       " ('inconsiderate', 'twats'),\n",
       " ('twats', 'm6m61'),\n",
       " ('m6m61', 'way'),\n",
       " ('way', 'home'),\n",
       " ('home', 'gon'),\n",
       " ('gon', 'na'),\n",
       " ('na', 'cut'),\n",
       " ('cut', 'bitch'),\n",
       " ('last', 'time', 'preston'),\n",
       " ('time', 'preston', 'anymore'),\n",
       " ('preston', 'anymore', 'inconsiderate'),\n",
       " ('anymore', 'inconsiderate', 'twats'),\n",
       " ('inconsiderate', 'twats', 'm6m61'),\n",
       " ('twats', 'm6m61', 'way'),\n",
       " ('m6m61', 'way', 'home'),\n",
       " ('way', 'home', 'gon'),\n",
       " ('home', 'gon', 'na'),\n",
       " ('gon', 'na', 'cut'),\n",
       " ('na', 'cut', 'bitch'),\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'fuck',\n",
       " 'anyone',\n",
       " 'british',\n",
       " ('hate', 'immigrants'),\n",
       " ('immigrants', 'fuck'),\n",
       " ('fuck', 'anyone'),\n",
       " ('anyone', 'british'),\n",
       " ('hate', 'immigrants', 'fuck'),\n",
       " ('immigrants', 'fuck', 'anyone'),\n",
       " ('fuck', 'anyone', 'british'),\n",
       " 'you',\n",
       " 'even',\n",
       " 'country',\n",
       " 'you',\n",
       " 'pakis',\n",
       " ('you', 'even'),\n",
       " ('even', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'pakis'),\n",
       " ('you', 'even', 'country'),\n",
       " ('even', 'country', 'you'),\n",
       " ('country', 'you', 'pakis'),\n",
       " 'fucking',\n",
       " 'white',\n",
       " 'bitch',\n",
       " 'road',\n",
       " 'needs',\n",
       " 'out',\n",
       " ('fucking', 'white'),\n",
       " ('white', 'bitch'),\n",
       " ('bitch', 'road'),\n",
       " ('road', 'needs'),\n",
       " ('needs', 'out'),\n",
       " ('fucking', 'white', 'bitch'),\n",
       " ('white', 'bitch', 'road'),\n",
       " ('bitch', 'road', 'needs'),\n",
       " ('road', 'needs', 'out'),\n",
       " 'get',\n",
       " 'out',\n",
       " 'country',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'foreigners',\n",
       " ('get', 'out'),\n",
       " ('out', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'foreigners'),\n",
       " ('get', 'out', 'country'),\n",
       " ('out', 'country', 'you'),\n",
       " ('country', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'foreigners'),\n",
       " 'enough',\n",
       " 'fucking',\n",
       " 'pakis',\n",
       " 'man',\n",
       " 'time',\n",
       " 'leave',\n",
       " 'europe',\n",
       " ('enough', 'fucking'),\n",
       " ('fucking', 'pakis'),\n",
       " ('pakis', 'man'),\n",
       " ('man', 'time'),\n",
       " ('time', 'leave'),\n",
       " ('leave', 'europe'),\n",
       " ('enough', 'fucking', 'pakis'),\n",
       " ('fucking', 'pakis', 'man'),\n",
       " ('pakis', 'man', 'time'),\n",
       " ('man', 'time', 'leave'),\n",
       " ('time', 'leave', 'europe'),\n",
       " 'kill',\n",
       " 'you',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('kill', 'you'),\n",
       " ('you', 'you'),\n",
       " ('you', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('kill', 'you', 'you'),\n",
       " ('you', 'you', 'paki'),\n",
       " ('you', 'paki', 'bastard'),\n",
       " 'you',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('you', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('you', 'ass', 'prick'),\n",
       " 'smelly',\n",
       " 'pakis',\n",
       " ('smelly', 'pakis'),\n",
       " 'niggers',\n",
       " 'road',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'shouting',\n",
       " 'go',\n",
       " 'round',\n",
       " 'shut',\n",
       " 'them',\n",
       " ('niggers', 'road'),\n",
       " ('road', 'better'),\n",
       " ('better', 'stop'),\n",
       " ('stop', 'shouting'),\n",
       " ('shouting', 'go'),\n",
       " ('go', 'round'),\n",
       " ('round', 'shut'),\n",
       " ('shut', 'them'),\n",
       " ('niggers', 'road', 'better'),\n",
       " ('road', 'better', 'stop'),\n",
       " ('better', 'stop', 'shouting'),\n",
       " ('stop', 'shouting', 'go'),\n",
       " ('shouting', 'go', 'round'),\n",
       " ('go', 'round', 'shut'),\n",
       " ('round', 'shut', 'them'),\n",
       " 'fucking',\n",
       " 'bitch',\n",
       " ('fucking', 'bitch'),\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " ('cunt', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('cunt', 'fuck', 'off'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'dogs',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'dogs'),\n",
       " ('bastard', 'muslim', 'dogs'),\n",
       " 'fuck',\n",
       " 'dykes',\n",
       " ('fuck', 'dykes'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'fuck',\n",
       " 'jezanie',\n",
       " 'cunts',\n",
       " 'cbb',\n",
       " ('fuck', 'jezanie'),\n",
       " ('jezanie', 'cunts'),\n",
       " ('cunts', 'cbb'),\n",
       " ('fuck', 'jezanie', 'cunts'),\n",
       " ('jezanie', 'cunts', 'cbb'),\n",
       " 'fat',\n",
       " 'little',\n",
       " 'wanker',\n",
       " 'you',\n",
       " 'problem',\n",
       " 'say',\n",
       " 'face',\n",
       " 'behind',\n",
       " 'back',\n",
       " 'faggot',\n",
       " ('fat', 'little'),\n",
       " ('little', 'wanker'),\n",
       " ('wanker', 'you'),\n",
       " ('you', 'problem'),\n",
       " ('problem', 'say'),\n",
       " ('say', 'face'),\n",
       " ('face', 'behind'),\n",
       " ('behind', 'back'),\n",
       " ('back', 'faggot'),\n",
       " ('fat', 'little', 'wanker'),\n",
       " ('little', 'wanker', 'you'),\n",
       " ('wanker', 'you', 'problem'),\n",
       " ('you', 'problem', 'say'),\n",
       " ('problem', 'say', 'face'),\n",
       " ('say', 'face', 'behind'),\n",
       " ('face', 'behind', 'back'),\n",
       " ('behind', 'back', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'immigrants',\n",
       " 'brexit',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'immigrants'),\n",
       " ('immigrants', 'brexit'),\n",
       " ('fuck', 'off', 'immigrants'),\n",
       " ('off', 'immigrants', 'brexit'),\n",
       " 'fuck',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'sheboon',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'strung',\n",
       " 'like',\n",
       " 'niggers',\n",
       " 'whitepower',\n",
       " '1488',\n",
       " ('fuck', 'you'),\n",
       " ('you', 'nigger'),\n",
       " ('nigger', 'sheboon'),\n",
       " ('sheboon', 'hope'),\n",
       " ('hope', 'you'),\n",
       " ('you', 'strung'),\n",
       " ('strung', 'like'),\n",
       " ('like', 'niggers'),\n",
       " ('niggers', 'whitepower'),\n",
       " ('whitepower', '1488'),\n",
       " ('fuck', 'you', 'nigger'),\n",
       " ('you', 'nigger', 'sheboon'),\n",
       " ('nigger', 'sheboon', 'hope'),\n",
       " ('sheboon', 'hope', 'you'),\n",
       " ('hope', 'you', 'strung'),\n",
       " ('you', 'strung', 'like'),\n",
       " ('strung', 'like', 'niggers'),\n",
       " ('like', 'niggers', 'whitepower'),\n",
       " ('niggers', 'whitepower', '1488'),\n",
       " 'bitch',\n",
       " 'immigrants',\n",
       " 'fucking',\n",
       " 'terrorists',\n",
       " ('immigrants', 'fucking'),\n",
       " ('fucking', 'terrorists'),\n",
       " ('immigrants', 'fucking', 'terrorists'),\n",
       " 'come',\n",
       " 'out',\n",
       " 'you',\n",
       " 'black',\n",
       " 'bastard',\n",
       " ('come', 'out'),\n",
       " ('out', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('come', 'out', 'you'),\n",
       " ('out', 'you', 'black'),\n",
       " ('you', 'black', 'bastard'),\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('go', 'fuck'),\n",
       " ('fuck', 'yourself'),\n",
       " ('yourself', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('go', 'fuck', 'yourself'),\n",
       " ('fuck', 'yourself', 'you'),\n",
       " ('yourself', 'you', 'cunt'),\n",
       " 'go',\n",
       " 'home',\n",
       " ...]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of the tokens are duplicates. This is  because either (1) they are repeated within a tweet or (2) they are present in multiple tweets. \n",
    "\n",
    "Don't worry though, we'll get unique features out of it soon, before training the classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that features are tokens or combination of tokens (ngrams).\n",
    "\n",
    "Let's have a look at how frequent each feature is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(vocab)\n",
    "fd.plot(20, cumulative=True)\n",
    "#fd.xlabel('Most common features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fuck', 60), ('you', 47), ('off', 26), ('go', 20), ('hate', 16), ('paki', 15), ('get', 14), ('bastard', 14), ('bitch', 13), ('nigger', 12), ('immigr', 11), ('out', 11), ('cunt', 10), ('terrorist', 9), ('polish', 9), ('muslim', 8), ('faggot', 8), ('countri', 8), ('time', 7), ('home', 7), ('back', 7), ('black', 6), ('mani', 6), ('peopl', 6), ('like', 5), ('leav', 5), ('scum', 5), ('even', 5), ('job', 5), ('road', 5), ('let', 5), ('much', 5), ('daddi', 5), ('make', 4), ('shut', 4), ('them', 4), ('brexit', 4), ('come', 4), ('day', 4), ('take', 4), ('want', 4), ('work', 4), ('love', 4), ('from', 4), ('end', 3), ('2', 3), ('man', 3), ('europ', 3), ('look', 3), ('prick', 3)]\n"
     ]
    }
   ],
   "source": [
    "print(fd.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to select a sample of this vocabulary of features. We only want to keep the features that truely matter in identifying hatespeech.\n",
    "\n",
    "This step is important to reduce the  running time of our model as well as improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing rare and common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping the 50 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rare features are only present in one or two tweet. We know that these are not going to be very useful to teach the model to recognise hate speech.  \n",
    "\n",
    "Let's only keep the top 50 most frequent features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Features of interest here are the top 50 most frequent words. THIS IS NOT THE BEST WAY OF DOING IT! IT SHOULD REMOVE THE MOST FREQUENT WORDS!</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chosen features: 50/1206\n"
     ]
    }
   ],
   "source": [
    "def get_word_features(wordlist, n):\n",
    "    fd = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    word_features = sorted(fd.items(), key=operator.itemgetter(1), reverse=True)[0:n] \n",
    "    word_features = [i[0] for i in word_features ]\n",
    "    return word_features\n",
    "\n",
    "# Only keep the top 50 most frequent words\n",
    "chosen_features = get_word_features(vocab, 50)\n",
    "print('Number of chosen features: {}/{}'.format(len(chosen_features), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck', 'you', 'off', 'go', 'hate', 'paki', 'get', 'bastard', 'bitch', 'nigger', 'immigr', 'out', 'cunt', 'terrorist', 'polish', 'muslim', 'faggot', 'countri', 'time', 'home', 'back', 'black', 'mani', 'peopl', 'like', 'leav', 'scum', 'even', 'job', 'road', 'let', 'much', 'daddi', 'make', 'shut', 'them', 'brexit', 'come', 'day', 'take', 'want', 'work', 'love', 'from', 'end', '2', 'man', 'europ', 'look', 'prick']\n"
     ]
    }
   ],
   "source": [
    "print(chosen_features[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have chosen a sample of features that we think are important for the model to learn to identify hateful speech.\n",
    "\n",
    "However, at this stage the classfier won't be able to know which features are responsible for a tweet being labelled as 'hateful'. Is it because of the word 'road' or the word 'bastard' in that tweet?\n",
    "\n",
    "To be able to learn what counts as hateful and what doesn't, the classifier needs to know the 'hateful value' of each feature in the vocabulary.\n",
    "\n",
    "In short, we need to tell the model:\n",
    "- which features are typically present in hateful tweets and which are not,\n",
    "- which features are typically present in non-hateful tweets and which are not.\n",
    "\n",
    "This precious information is available in our dataset because it has been manually labelled. So far we have not used the 'class' column in our dataset. We are now going to make use of it!\n",
    "\n",
    "The idea is to tell the model: \n",
    "- for each hateful tweet: these are the features present, and the ones not present. \n",
    "- for each non-hateful tweet: these are the features present, and the ones not present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the features present in each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    feature_set = {}\n",
    "    for feature in chosen_features:\n",
    "        feature_set['contains({})'.format(feature)] = (feature in document_words)\n",
    "    return feature_set\n",
    "\n",
    "tweets = [tuple(x) for x in pre_processed_data.values]\n",
    "\n",
    "feature_set = nltk.classify.apply_features(extract_features, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in training_set: 198\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets in training_set: {}'.format(len(feature_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look at the first tweet. When scrolling all the way to the end, we see that this is a hateful tweet (label = 1). This will help the classifier know which features lead to being labelled hateful and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'contains(fuck)': False, 'contains(you)': False, 'contains(off)': False, 'contains(go)': False, 'contains(hate)': False, 'contains(paki)': False, 'contains(get)': False, 'contains(bastard)': False, 'contains(bitch)': False, 'contains(nigger)': True, 'contains(immigr)': False, 'contains(out)': False, 'contains(cunt)': False, 'contains(terrorist)': False, 'contains(polish)': False, 'contains(muslim)': False, 'contains(faggot)': False, 'contains(countri)': False, 'contains(time)': False, 'contains(home)': False, 'contains(back)': False, 'contains(black)': False, 'contains(mani)': False, 'contains(peopl)': False, 'contains(like)': False, 'contains(leav)': False, 'contains(scum)': False, 'contains(even)': False, 'contains(job)': False, 'contains(road)': False, 'contains(let)': False, 'contains(much)': False, 'contains(daddi)': False, 'contains(make)': False, 'contains(shut)': False, 'contains(them)': False, 'contains(brexit)': False, 'contains(come)': False, 'contains(day)': False, 'contains(take)': False, 'contains(want)': False, 'contains(work)': False, 'contains(love)': False, 'contains(from)': False, 'contains(end)': True, 'contains(2)': True, 'contains(man)': False, 'contains(europ)': False, 'contains(look)': False, 'contains(prick)': False}, 1)\n"
     ]
    }
   ],
   "source": [
    "print(feature_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is pretty simple. For each tweet, we are looping through our chosen_features and setting a boolean to True if the tweet contains that feature, False otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is what the classifier needs as input.\n",
    "\n",
    "We can now train the classifier with this training_set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train vs test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train the classifer and then test its classifying ability on a brand new dataset that it has never seen before. \n",
    "\n",
    "Generally, 80/20 percent is a fair split between training and testing set:\n",
    "- training dataset (80% of the data)\n",
    "- testing dataset (20% of the data)\n",
    "\n",
    "Sklearn provides a function called train_test_split to do this easily. Let's split our feature_set into train_data and test_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in train data: 158\n",
      "Number of tweets in test data: 40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(feature_set, test_size=0.20, train_size=0.80)\n",
    "print('Number of tweets in train data: {}'.format(len(train_data)))\n",
    "print('Number of tweets in test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of model to use as for classifying text data. The most common one is called Naive Bayesion Classifier and that is the one we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(bitch) = True                1 : 0      =      7.9 : 1.0\n",
      "           contains(get) = True                0 : 1      =      6.7 : 1.0\n",
      "        contains(immigr) = True                1 : 0      =      6.0 : 1.0\n",
      "          contains(fuck) = True                1 : 0      =      4.3 : 1.0\n",
      "        contains(polish) = True                1 : 0      =      4.1 : 1.0\n",
      "          contains(hate) = True                1 : 0      =      4.0 : 1.0\n",
      "     contains(terrorist) = True                1 : 0      =      3.5 : 1.0\n",
      "            contains(go) = True                0 : 1      =      3.2 : 1.0\n",
      "          contains(home) = True                1 : 0      =      2.9 : 1.0\n",
      "         contains(black) = True                1 : 0      =      2.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_data)\n",
    "# SHOW FEATURES\n",
    "classifier1.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "# Save the model into a pickle file\n",
    "import pickle\n",
    "f = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier1, f)\n",
    "f.close()\n",
    "\n",
    "# DecisionTree\n",
    "#classifier2 = nltk.classify.DecisionTreeClassifier.train(training_set, entropy_cutoff=0,support_cutoff=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The model has been trained on the train_data.\n",
    "\n",
    "We can see which features the model considers important to decide between hateful speech and non-hateful speech.\n",
    "\n",
    "- Column 3 shows the ratio of occurence of each informative feature in both labels (hate is 1 : non-hate is 0).\n",
    "- Column 2 shows the direction of the ratio (which label occurs more frequently). The label on the left is the label most associated with the corresponding feature.\n",
    "\n",
    "For example, tweets containing the word 'immigrants' are <span style=\"color:red\">5.7 times</span> more likely to be hateful than not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the accuracy of our model on the test_data that we set aside earlier. These are tweets that the model has never seen before. We'll ask the model to classify them and see how its outcome compares with the true label of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =  nltk.classify.util.accuracy(classifier1, test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Use the classifier to identify hateful speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try our classifier on a new tweet of your choice. First we need to preprocess the tweet (clean, tokenize, stem and remove stopwords). Then we need to extract its features to look like the right input for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweet = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tweet: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tweet\n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # clean the tweet\n",
    "    tweet = cleanTweet(testTweet)\n",
    "    \n",
    "    # tokenize the cleaned tweet\n",
    "    tokenised_tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stop words\n",
    "    tokenised_tweet_stpwd = [item for item in tokenised_tweet if item not in stopWords]\n",
    "    \n",
    "    # stem\n",
    "    pre_processed_tweet = [ps.stem(word) for word in tokenised_tweet_stpwd]\n",
    "    \n",
    "    print('Preprocessed tweet: {}'.format(pre_processed_tweet))\n",
    "    \n",
    "    return pre_processed_tweet\n",
    "\n",
    "preprocessed_tweet = preprocessTweet(testTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(fuck)': False, 'contains(you)': False, 'contains(off)': False, 'contains(go)': False, 'contains(hate)': False, 'contains(paki)': False, 'contains(get)': False, 'contains(bastard)': False, 'contains(bitch)': False, 'contains(nigger)': False, 'contains(immigr)': False, 'contains(out)': False, 'contains(cunt)': False, 'contains(terrorist)': False, 'contains(polish)': False, 'contains(muslim)': False, 'contains(faggot)': False, 'contains(countri)': False, 'contains(time)': False, 'contains(home)': False, 'contains(back)': False, 'contains(black)': False, 'contains(mani)': False, 'contains(peopl)': False, 'contains(like)': False, 'contains(leav)': False, 'contains(scum)': False, 'contains(even)': False, 'contains(job)': False, 'contains(road)': False, 'contains(let)': False, 'contains(much)': False, 'contains(daddi)': False, 'contains(make)': False, 'contains(shut)': False, 'contains(them)': False, 'contains(brexit)': False, 'contains(come)': False, 'contains(day)': False, 'contains(take)': False, 'contains(want)': False, 'contains(work)': False, 'contains(love)': False, 'contains(from)': False, 'contains(end)': False, 'contains(2)': False, 'contains(man)': False, 'contains(europ)': False, 'contains(look)': False, 'contains(prick)': False}\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "print(tweet_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Classify\n",
    "print(classifier1.classify(tweet_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
