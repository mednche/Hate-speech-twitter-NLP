{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier to recognise hate speech on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nltk version is 3.3.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mednche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "#from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mednche\\\\Desktop\\\\Hate-speech-twitter-NLP'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(\"/Users/natachachenevoy/Documents/Twitter Analysis/Classifier\")\n",
    "os.chdir(\"C:/Users/mednche/Desktop/Hate-speech-twitter-NLP/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labeled Twitter datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainingTweets.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>well yes i mean you started off saying third l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fucking fascist fucking liberal fucking racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>fucking annoying when meat dairy and eggs are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i hate people i was wrong when i said 97.5 of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "305  well yes i mean you started off saying third l...      0\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "307  fucking fascist fucking liberal fucking racist...      1\n",
       "308  fucking annoying when meat dairy and eggs are ...      0\n",
       "309  i hate people i was wrong when i said 97.5 of ...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that each tweet has already been manually labeled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Ensure balance in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Turn data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 hateful tweets\n",
      "209 non hateful tweets\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "pos_tweets = data[data[\"class\"] == 1]\n",
    "pos_tweets = shuffle(pos_tweets)\n",
    "print(\"{} hateful tweets\".format(len(pos_tweets)))\n",
    "\n",
    "neg_tweets = data[data[\"class\"] == 0]\n",
    "neg_tweets = shuffle(neg_tweets)\n",
    "print(\"{} non hateful tweets\".format(len(neg_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>why cheat fucking stupid ass prick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>saw some graffiti on the train today, it said ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Had enough of the fucking pakis man, time to l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>\"go burn in hell bitch\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "296                 why cheat fucking stupid ass prick      1\n",
       "48   saw some graffiti on the train today, it said ...      1\n",
       "45   Had enough of the fucking pakis man, time to l...      1\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "286                            \"go burn in hell bitch\"      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>'RT @LazaroDaniella: if you love someone_ tell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>'@bxrnheater she nods_ confirming what the boy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>'Fucking league of memes. Is hilarious'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>i was aiming that message at the wafc fans tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>referee tonight at the winter gardens blackpoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "89   'RT @LazaroDaniella: if you love someone_ tell...      0\n",
       "134  '@bxrnheater she nods_ confirming what the boy...      0\n",
       "110            'Fucking league of memes. Is hilarious'      0\n",
       "225  i was aiming that message at the wafc fans tha...      0\n",
       "186  referee tonight at the winter gardens blackpoo...      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Select as many hateful as non-hateful tweets for an equal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in balanced dataset: 202\n"
     ]
    }
   ],
   "source": [
    "num = min(len(pos_tweets), len(neg_tweets))\n",
    "data_balanced = pos_tweets[0:num].append(neg_tweets[0:num], ignore_index=True)\n",
    "\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why cheat fucking stupid ass prick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saw some graffiti on the train today, it said ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Had enough of the fucking pakis man, time to l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"go burn in hell bitch\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0                 why cheat fucking stupid ass prick      1\n",
       "1  saw some graffiti on the train today, it said ...      1\n",
       "2  Had enough of the fucking pakis man, time to l...      1\n",
       "3  so my neighbours complained about my shed in t...      1\n",
       "4                            \"go burn in hell bitch\"      1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to ''\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    # removing the RT before the @user \n",
    "    tweet = re.sub('rt','',tweet) \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Convert @username to ''\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # remove non ASCII characters (emojies)\n",
    "    tweet= re.sub(r'[^\\x00-\\x7F]+','', tweet)\n",
    "    # remove punctuation \n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    # remove beginning and end space\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  apply processing function\n",
    "data_balanced['tweet'] = data_balanced['tweet'].apply(cleanTweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>retweeted gazete krtk  baskn oran akp de fethu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>probably gonna recolorshade this head crep</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>maniq gel polish on natural nails in champagne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>live on webcam</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>crazy car crash compilation pa 321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "197  retweeted gazete krtk  baskn oran akp de fethu...      0\n",
       "198         probably gonna recolorshade this head crep      0\n",
       "199  maniq gel polish on natural nails in champagne...      0\n",
       "200                                     live on webcam      0\n",
       "201                 crazy car crash compilation pa 321      0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the text of emtpy tweets '' by NA and then delete the row\n",
    "data_balanced['tweet'].replace('', np.nan, inplace=True)\n",
    "data_balanced.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How many empty tweets were removed in the process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the text of each tweet is a string. We would like to separate each word in that string. In NLP, this is called 'tokenising'.\n",
    "\n",
    "When tokenising, each tweet (intially a string of text) is chopped into a list of tokens. \n",
    "\n",
    "A token is a word or a combination of 2 (bigram) or 3 (trigram) words such as ('back', 'off)' or ('send', 'them', home'). In this example, the ngrams only have a hateful meaning when considered as a group but they are not hateful when taken individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[why, cheat, fucking, stupid, ass, prick]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[saw, some, graffiti, on, the, train, today, i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[had, enough, of, the, fucking, pakis, man, ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[so, my, neighbours, complained, about, my, sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[go, burn, in, hell, bitch]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0          [why, cheat, fucking, stupid, ass, prick]      1\n",
       "1  [saw, some, graffiti, on, the, train, today, i...      1\n",
       "2  [had, enough, of, the, fucking, pakis, man, ti...      1\n",
       "3  [so, my, neighbours, complained, about, my, sh...      1\n",
       "4                        [go, burn, in, hell, bitch]      1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised = data_balanced.copy()\n",
    "\n",
    "data_tokenised['tweet'] = data_tokenised['tweet'].apply(nltk.word_tokenize)\n",
    "\n",
    "data_tokenised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that there are many words in the tweets that don't bring any meaning such as 'it', 'i' etc. These are called stopwords and need to be removed so that the classifier can focus on words that matter when telling the difference between hate and non-hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " \"that'll\",\n",
       " 'needn',\n",
       " 'if',\n",
       " 'then',\n",
       " 'yourselves',\n",
       " 'a',\n",
       " \"shouldn't\",\n",
       " 'hers',\n",
       " \"aren't\",\n",
       " 'it',\n",
       " 'very',\n",
       " 'own',\n",
       " 'what',\n",
       " 'am',\n",
       " 'or',\n",
       " 'being',\n",
       " \"didn't\",\n",
       " \"couldn't\",\n",
       " 'while',\n",
       " 'o',\n",
       " \"needn't\",\n",
       " 'about',\n",
       " 'other',\n",
       " 'shan',\n",
       " \"won't\",\n",
       " \"hasn't\",\n",
       " 'won',\n",
       " 'with',\n",
       " 'don',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'too',\n",
       " 'has',\n",
       " 'under',\n",
       " 'd',\n",
       " 'ours',\n",
       " 'hasn',\n",
       " 'yours',\n",
       " 'i',\n",
       " 'to',\n",
       " \"you're\",\n",
       " \"should've\",\n",
       " 'no',\n",
       " 'how',\n",
       " 'did',\n",
       " \"you'll\",\n",
       " \"hadn't\",\n",
       " 'until',\n",
       " 'herself',\n",
       " 'theirs',\n",
       " 'more',\n",
       " 't',\n",
       " 's',\n",
       " 'shouldn',\n",
       " \"shan't\",\n",
       " 'hadn',\n",
       " 'only',\n",
       " 'most',\n",
       " 'on',\n",
       " 'not',\n",
       " 'at',\n",
       " 'between',\n",
       " 'through',\n",
       " 'll',\n",
       " \"weren't\",\n",
       " 'this',\n",
       " 'each',\n",
       " 'she',\n",
       " 'whom',\n",
       " 'for',\n",
       " 'we',\n",
       " 'm',\n",
       " 'down',\n",
       " 'are',\n",
       " 'up',\n",
       " 'there',\n",
       " 'haven',\n",
       " \"she's\",\n",
       " 'below',\n",
       " 'just',\n",
       " 'its',\n",
       " 'any',\n",
       " 'ain',\n",
       " 'but',\n",
       " 'some',\n",
       " 'be',\n",
       " \"don't\",\n",
       " \"you've\",\n",
       " 'here',\n",
       " 'before',\n",
       " 'after',\n",
       " 'as',\n",
       " \"mightn't\",\n",
       " 'itself',\n",
       " 'didn',\n",
       " 'mightn',\n",
       " 'should',\n",
       " 'their',\n",
       " 'against',\n",
       " 'aren',\n",
       " 'wasn',\n",
       " 'further',\n",
       " 'above',\n",
       " 'will',\n",
       " 'him',\n",
       " 'isn',\n",
       " \"mustn't\",\n",
       " 'into',\n",
       " 'he',\n",
       " \"isn't\",\n",
       " 'few',\n",
       " 'doesn',\n",
       " 'is',\n",
       " 'her',\n",
       " \"it's\",\n",
       " 'they',\n",
       " 'myself',\n",
       " 'our',\n",
       " 'does',\n",
       " 'these',\n",
       " 're',\n",
       " 'ourselves',\n",
       " 'an',\n",
       " 'so',\n",
       " 'when',\n",
       " 'have',\n",
       " 'again',\n",
       " \"doesn't\",\n",
       " 'by',\n",
       " \"wouldn't\",\n",
       " 'those',\n",
       " 'weren',\n",
       " 'because',\n",
       " 'now',\n",
       " 'himself',\n",
       " 'having',\n",
       " 'his',\n",
       " 'do',\n",
       " \"wasn't\",\n",
       " 'was',\n",
       " 'wouldn',\n",
       " 'who',\n",
       " 'couldn',\n",
       " 'my',\n",
       " 'both',\n",
       " 'mustn',\n",
       " 'such',\n",
       " 'of',\n",
       " 'that',\n",
       " 'had',\n",
       " 'can',\n",
       " 've',\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'been',\n",
       " 'all',\n",
       " 'nor',\n",
       " 'which',\n",
       " \"haven't\",\n",
       " 'doing',\n",
       " 'why',\n",
       " 'over',\n",
       " 'and',\n",
       " 'during',\n",
       " 'once',\n",
       " 'y',\n",
       " 'in',\n",
       " 'were',\n",
       " 'than',\n",
       " 'where',\n",
       " 'youre',\n",
       " 'r',\n",
       " \"you're\",\n",
       " 'us',\n",
       " 'doesnt',\n",
       " 'im',\n",
       " 'hes',\n",
       " 'u',\n",
       " 'ya',\n",
       " 'ww',\n",
       " 'dont',\n",
       " 'https',\n",
       " 'aint',\n",
       " 'theres',\n",
       " 'shouldnt',\n",
       " 'thats',\n",
       " 'amp',\n",
       " 'wudnt',\n",
       " 'gonna',\n",
       " 'ur',\n",
       " 'cant']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "#remove some words from the list:\n",
    "item_to_delete = ['you', 'out', 'off', 'them', 'themselves', 'yourself', 'from', 'same']\n",
    "stopWords = [e for e in stops if e not in item_to_delete]\n",
    "item_to_add = [\"youre\", \"r\", \"you're\", \"us\", \"doesnt\", \"im\", \"hes\", \"u\", \"ya\", \"ww\", \"dont\", \"https\", \"aint\", \"theres\", \"shouldnt\", \"thats\", \"amp\", \"wudnt\", \"gonna\", \"ur\", \"cant\"]\n",
    "for e in item_to_add:\n",
    "    stopWords.append(e)\n",
    "\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cheat, fucking, stupid, ass, prick]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[saw, graffiti, train, today, said, fuck, off,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[enough, fucking, pakis, man, time, leave, eur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[neighbours, complained, shed, front, garden, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[go, burn, hell, bitch]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0               [cheat, fucking, stupid, ass, prick]      1\n",
       "1  [saw, graffiti, train, today, said, fuck, off,...      1\n",
       "2  [enough, fucking, pakis, man, time, leave, eur...      1\n",
       "3  [neighbours, complained, shed, front, garden, ...      1\n",
       "4                            [go, burn, hell, bitch]      1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokenised_stpwd = data_tokenised.copy()\n",
    "\n",
    "data_tokenised_stpwd['tweet'] = data_tokenised_stpwd['tweet'].apply(lambda x: [item for item in x if item not in stopWords])\n",
    "\n",
    "data_tokenised_stpwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plural to singular, verb root, word stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [cheat, fuck, stupid, ass, prick]\n",
       "1    [saw, graffiti, train, today, said, fuck, off,...\n",
       "2         [enough, fuck, paki, man, time, leav, europ]\n",
       "3    [neighbour, complain, shed, front, garden, who...\n",
       "4                              [go, burn, hell, bitch]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data = data_tokenised_stpwd.copy()\n",
    "\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "#lemma = WordNetLemmatizer()\n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "pre_processed_data['tweet'] = pre_processed_data['tweet'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "pre_processed_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: At this point, we have a dataset that has been cleaned and tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the classfier won't be able to know which key words are responsible for a tweet being labelled 'hateful'. Is it because of the word 'everywhere' or the word 'fuck'?\n",
    "\n",
    "To be able to learn what counts as hateful and what doesn't, the classifier also needs to know the 'hateful value' of each word (or combination of word) in a tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text themselves cannot be used by machine learning models. They expect their input to be numeric. So we need some way that can transform input text into numeric feature in a meaningful way. There are several approaches for this and we’ll briefly go through some of them.\n",
    "\n",
    "In Part 2., we'll see how to convert the words into features so that we can feed it to a classifier for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare the data to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the text into any category, we need to define some criteria. On the basis of those criteria, our classifier will learn that a particular kind of text falls in a particular category. This kind of criteria is known as feature. We can define one or more feature to train our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way we can convert text to numeric feature is via binary encoding. In this scheme, we create a vocabulary by looking at each distinct word in the whole dataset (corpus). For each document, the output of this scheme will be a vector of size N where N is the total number of words in our vocabulary. Initially all entries in the vector will be 0. If the word in the given document exists in the vocabulary then vector element at that position is set to 1. Let’s implement this to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vocabulary: a set of unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1287\n"
     ]
    }
   ],
   "source": [
    "vocab = [word for tweet in pre_processed_data['tweet'] for word in tweet]\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheat',\n",
       " 'fuck',\n",
       " 'stupid',\n",
       " 'ass',\n",
       " 'prick',\n",
       " 'saw',\n",
       " 'graffiti',\n",
       " 'train',\n",
       " 'today',\n",
       " 'said',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'black',\n",
       " 'slagfound',\n",
       " 'pretti',\n",
       " 'funni',\n",
       " 'enough',\n",
       " 'fuck',\n",
       " 'paki',\n",
       " 'man',\n",
       " 'time',\n",
       " 'leav',\n",
       " 'europ',\n",
       " 'neighbour',\n",
       " 'complain',\n",
       " 'shed',\n",
       " 'front',\n",
       " 'garden',\n",
       " 'who',\n",
       " 'neighbour',\n",
       " 'church',\n",
       " 'england',\n",
       " 'bunch',\n",
       " 'god',\n",
       " 'bother',\n",
       " 'cunt',\n",
       " 'go',\n",
       " 'burn',\n",
       " 'hell',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'dyke',\n",
       " 'fuck',\n",
       " 'paki',\n",
       " 'come',\n",
       " 'out',\n",
       " 'you',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'you',\n",
       " 'muslim',\n",
       " 'prick',\n",
       " 'hate',\n",
       " 'immigr',\n",
       " 'you',\n",
       " 'even',\n",
       " 'countri',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'fuck',\n",
       " 'faggot',\n",
       " 'sit',\n",
       " 'cinema',\n",
       " 'next',\n",
       " 'sli',\n",
       " 'bitch',\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'terrorist',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'fuck',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'smelli',\n",
       " 'paki',\n",
       " 'go',\n",
       " 'home',\n",
       " 'you',\n",
       " 'immigr',\n",
       " 'black',\n",
       " 'bitch',\n",
       " 'next',\n",
       " 'shut',\n",
       " 'you',\n",
       " 'fuck',\n",
       " 'faggot',\n",
       " 'least',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jefre',\n",
       " 'starr',\n",
       " 'faggot',\n",
       " 'fuck',\n",
       " 'polish',\n",
       " 'alien',\n",
       " 'come',\n",
       " 'take',\n",
       " 'job',\n",
       " 'unaccept',\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'dog',\n",
       " 'fuck',\n",
       " 'hate',\n",
       " 'allah',\n",
       " 'lover',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'back',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'racist',\n",
       " 'sexist',\n",
       " 'perv',\n",
       " 'clearli',\n",
       " 'want',\n",
       " 'fuck',\n",
       " 'daughter',\n",
       " 'disgust',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " 'you',\n",
       " 'fuck',\n",
       " 'irish',\n",
       " 'cow',\n",
       " 'good',\n",
       " 'night',\n",
       " 'fag',\n",
       " 'fagett',\n",
       " 'femal',\n",
       " 'version',\n",
       " 'fag',\n",
       " 'like',\n",
       " 'fagett',\n",
       " 'make',\n",
       " 'femal',\n",
       " 'hate',\n",
       " 'you',\n",
       " 'mani',\n",
       " 'wog',\n",
       " 'leav',\n",
       " 'countri',\n",
       " 'brexit',\n",
       " 'hate',\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'faggot',\n",
       " 'hate',\n",
       " 'spicskkk',\n",
       " 'ralli',\n",
       " 'fat',\n",
       " 'littl',\n",
       " 'wanker',\n",
       " 'you',\n",
       " 'problem',\n",
       " 'say',\n",
       " 'face',\n",
       " 'behind',\n",
       " 'back',\n",
       " 'faggot',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'wear',\n",
       " 'veil',\n",
       " 'nigger',\n",
       " 'road',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'shout',\n",
       " 'go',\n",
       " 'round',\n",
       " 'shut',\n",
       " 'them',\n",
       " 'mani',\n",
       " 'chink',\n",
       " 'london',\n",
       " 'least',\n",
       " 'nigger',\n",
       " 'lmfao',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'myriah',\n",
       " 'chavcentr',\n",
       " 'chav',\n",
       " 'central',\n",
       " 'fuck',\n",
       " 'chink',\n",
       " 'fuck',\n",
       " 'fascist',\n",
       " 'fuck',\n",
       " 'liber',\n",
       " 'fuck',\n",
       " 'racist',\n",
       " 'fuck',\n",
       " 'traitor',\n",
       " 'fuck',\n",
       " 'petrolhead',\n",
       " 'fuck',\n",
       " 'treehugg',\n",
       " 'steve',\n",
       " 'you',\n",
       " 'steph',\n",
       " 'mate',\n",
       " 'hate',\n",
       " 'immigr',\n",
       " 'fuck',\n",
       " 'anyon',\n",
       " 'british',\n",
       " 'black',\n",
       " 'nigger',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'scum',\n",
       " 'diy',\n",
       " 'paki',\n",
       " 'fuck',\n",
       " 'sand',\n",
       " 'monkey',\n",
       " 'fuck',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'immigr',\n",
       " 'brexit',\n",
       " 'fuck',\n",
       " 'nigger',\n",
       " 'bastard',\n",
       " 'wog',\n",
       " 'road',\n",
       " 'annoy',\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'immigr',\n",
       " 'fuck',\n",
       " 'jezani',\n",
       " 'cunt',\n",
       " 'cbb',\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'cunt',\n",
       " 'brexit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'polish',\n",
       " 'alien',\n",
       " 'paki',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'pleas',\n",
       " 'get',\n",
       " 'out',\n",
       " 'countri',\n",
       " 'you',\n",
       " 'fuck',\n",
       " 'foreign',\n",
       " 'immigr',\n",
       " 'fuck',\n",
       " 'terrorist',\n",
       " 'you',\n",
       " 'ass',\n",
       " 'prick',\n",
       " 'muslim',\n",
       " 'scum',\n",
       " 'terrorist',\n",
       " 'bitch',\n",
       " 'shut',\n",
       " 'fuck',\n",
       " 'hate',\n",
       " 'immigr',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'europ',\n",
       " 'you',\n",
       " 'job',\n",
       " 'steal',\n",
       " 'immigr',\n",
       " 'bunch',\n",
       " 'paki',\n",
       " 'perv',\n",
       " 'man',\n",
       " 'call',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'back',\n",
       " 'countri',\n",
       " 'rude',\n",
       " 'tire',\n",
       " 'polish',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'fuck',\n",
       " 'white',\n",
       " 'bitch',\n",
       " 'road',\n",
       " 'need',\n",
       " 'out',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'along',\n",
       " 'europ',\n",
       " 'fuck',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'sheboon',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'strung',\n",
       " 'like',\n",
       " 'nigger',\n",
       " 'whitepow',\n",
       " '1488',\n",
       " 'last',\n",
       " 'time',\n",
       " 'preston',\n",
       " 'anymor',\n",
       " 'inconsider',\n",
       " 'twat',\n",
       " 'm6m61',\n",
       " 'way',\n",
       " 'home',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'cut',\n",
       " 'bitch',\n",
       " 'thick',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'polish',\n",
       " 'bitch',\n",
       " 'benjamin',\n",
       " 'fuck',\n",
       " 'faggot',\n",
       " 'hate',\n",
       " 'yellow',\n",
       " 'fucker',\n",
       " 'shut',\n",
       " 'nigger',\n",
       " 'whore',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'rape',\n",
       " 'one',\n",
       " 'anim',\n",
       " 'might',\n",
       " 'chang',\n",
       " 'tune',\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'faggot',\n",
       " 'like',\n",
       " 'you',\n",
       " 'hate',\n",
       " 'peopl',\n",
       " 'wrong',\n",
       " 'said',\n",
       " '975',\n",
       " 'peopl',\n",
       " 'cunt',\n",
       " 'much',\n",
       " 'much',\n",
       " 'higher',\n",
       " 'local',\n",
       " 'mosqu',\n",
       " 'let',\n",
       " 'terrorist',\n",
       " 'out',\n",
       " 'leav',\n",
       " 'fuck',\n",
       " 'countri',\n",
       " 'you',\n",
       " 'white',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'poland',\n",
       " 'hate',\n",
       " 'bitch',\n",
       " 'muslim',\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'home',\n",
       " 'bye',\n",
       " 'bye',\n",
       " 'brexit',\n",
       " 'polish',\n",
       " 'bastard',\n",
       " 'polish',\n",
       " 'vermin',\n",
       " 'diy',\n",
       " 'terrorist',\n",
       " 'religion',\n",
       " 'fuck',\n",
       " 'joke',\n",
       " 'you',\n",
       " 'go',\n",
       " 'around',\n",
       " 'scream',\n",
       " 'allah',\n",
       " 'akbar',\n",
       " 'terrorist',\n",
       " 'shit',\n",
       " 'diy',\n",
       " 'faggot',\n",
       " 'depo',\n",
       " 'bastard',\n",
       " 'immigr',\n",
       " 'keep',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'foreign',\n",
       " 'bastard',\n",
       " 'fuck',\n",
       " 'hate',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'bruh',\n",
       " 'polish',\n",
       " 'scum',\n",
       " 'muslim',\n",
       " 'terrorist',\n",
       " 'same',\n",
       " 'neighbour',\n",
       " 'call',\n",
       " 'paki',\n",
       " 'peopl',\n",
       " 'respect',\n",
       " 'fuck',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'use',\n",
       " 'tie',\n",
       " 'end',\n",
       " 'nigger',\n",
       " 'leg',\n",
       " '2',\n",
       " 'differ',\n",
       " 'hors',\n",
       " 'beat',\n",
       " 'hors',\n",
       " 'spread',\n",
       " 'leg',\n",
       " 'break',\n",
       " 'smh',\n",
       " 'kill',\n",
       " 'you',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'local',\n",
       " 'paki',\n",
       " 'shop',\n",
       " 'disgust',\n",
       " 'even',\n",
       " 'allow',\n",
       " 'countri',\n",
       " 'attent',\n",
       " 'seek',\n",
       " 'bitch',\n",
       " 'nigger',\n",
       " 'everywher',\n",
       " 'day',\n",
       " 'make',\n",
       " 'britain',\n",
       " 'great',\n",
       " 'colour',\n",
       " 'fucker',\n",
       " 'leav',\n",
       " 'countri',\n",
       " 'fuck',\n",
       " 'terrorist',\n",
       " 'everywher',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'you',\n",
       " 'love',\n",
       " 'someon',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'thing',\n",
       " 'right',\n",
       " 'time',\n",
       " 'right',\n",
       " 'place',\n",
       " 'long',\n",
       " 'nod',\n",
       " 'confirm',\n",
       " 'boy',\n",
       " 'true',\n",
       " 'realli',\n",
       " 'ask',\n",
       " 'eye',\n",
       " 'twinkl',\n",
       " 'said',\n",
       " 'would',\n",
       " 'accompani',\n",
       " 'fuck',\n",
       " 'leagu',\n",
       " 'meme',\n",
       " 'hilari',\n",
       " 'aim',\n",
       " 'messag',\n",
       " 'wafc',\n",
       " 'fan',\n",
       " 'happi',\n",
       " 'anoth',\n",
       " 'footbal',\n",
       " 'club',\n",
       " 'may',\n",
       " 'go',\n",
       " 'ie',\n",
       " 'bolton',\n",
       " 'refere',\n",
       " 'tonight',\n",
       " 'winter',\n",
       " 'garden',\n",
       " 'blackpool',\n",
       " 'hope',\n",
       " 'good',\n",
       " 'competit',\n",
       " 'fight',\n",
       " 'boxer',\n",
       " 'go',\n",
       " 'home',\n",
       " 'safe',\n",
       " 'healthi',\n",
       " 'fresh',\n",
       " 'out',\n",
       " 'pack',\n",
       " 'bed',\n",
       " 'fuck',\n",
       " 'yeah',\n",
       " 'good',\n",
       " 'news',\n",
       " 'tell',\n",
       " 'god',\n",
       " 'make',\n",
       " 'right',\n",
       " 'sight',\n",
       " 'accomplish',\n",
       " 'from',\n",
       " 'sta',\n",
       " 'finish',\n",
       " 'roman',\n",
       " '117',\n",
       " 'five',\n",
       " 'way',\n",
       " 'get',\n",
       " 'leg',\n",
       " 'competit',\n",
       " 'seo',\n",
       " 'feel',\n",
       " 'mani',\n",
       " 'today',\n",
       " 'set',\n",
       " 'off',\n",
       " 'watch',\n",
       " 'season',\n",
       " 'two',\n",
       " 'internet',\n",
       " 'decid',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'school',\n",
       " 'sta',\n",
       " 'pope',\n",
       " 'franci',\n",
       " 'visit',\n",
       " 'former',\n",
       " 'german',\n",
       " 'nazi',\n",
       " 'concentr',\n",
       " 'camp',\n",
       " 'auschwitzbirkenau',\n",
       " 'figur',\n",
       " 'shit',\n",
       " 'out',\n",
       " 'oop',\n",
       " 'freudian',\n",
       " 'slit',\n",
       " 'frontmen',\n",
       " 'go',\n",
       " 'freddi',\n",
       " 'mercuri',\n",
       " 'immens',\n",
       " 'fuckin',\n",
       " 'get',\n",
       " 'seminari',\n",
       " 'bore',\n",
       " 'out',\n",
       " 'fuck',\n",
       " 'mind',\n",
       " 'help',\n",
       " 'lighter',\n",
       " 'note',\n",
       " 'sky',\n",
       " 'pundit',\n",
       " 'get',\n",
       " 'cloth',\n",
       " 'budget',\n",
       " 'from',\n",
       " 'asda',\n",
       " 'dodgysuit',\n",
       " 'listen',\n",
       " 'eloq',\n",
       " 'qrion',\n",
       " 'beach',\n",
       " '20',\n",
       " 'move',\n",
       " 'castl',\n",
       " 'np',\n",
       " 'soundcloudnowplay',\n",
       " 'never',\n",
       " 'sleep',\n",
       " 'iv',\n",
       " 'got',\n",
       " 'get',\n",
       " 'dead',\n",
       " 'earli',\n",
       " 'realli',\n",
       " 'enjoy',\n",
       " 'box',\n",
       " 'tonight',\n",
       " 'great',\n",
       " 'fight',\n",
       " 'joshuawhyt',\n",
       " 'let',\n",
       " 'get',\n",
       " 'readi',\n",
       " 'rumbl',\n",
       " 'joshua',\n",
       " 'forget',\n",
       " 'robot',\n",
       " 'sho',\n",
       " 'outreach',\n",
       " 'email',\n",
       " 'get',\n",
       " 'you',\n",
       " 'link',\n",
       " 'decid',\n",
       " 'get',\n",
       " 'client',\n",
       " 'leav',\n",
       " 'comment',\n",
       " 'white',\n",
       " 'board',\n",
       " 'much',\n",
       " 'love',\n",
       " 'battl',\n",
       " 'rope',\n",
       " 'tbt',\n",
       " 'dean',\n",
       " 'went',\n",
       " 'go',\n",
       " 'ape',\n",
       " 'go',\n",
       " 'ape',\n",
       " 'rivington',\n",
       " 'coupon',\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'woeful',\n",
       " 'refer',\n",
       " 'sourc',\n",
       " 'you',\n",
       " 'crop',\n",
       " 'pass',\n",
       " 'off',\n",
       " 'hate',\n",
       " 'behaviour',\n",
       " 'forget',\n",
       " 'get',\n",
       " 'jar',\n",
       " 'fyld',\n",
       " 'coast',\n",
       " 'green',\n",
       " 'tomato',\n",
       " 'chutney',\n",
       " 'vintag',\n",
       " '2',\n",
       " 'year',\n",
       " 'winckley',\n",
       " 'get',\n",
       " 'you',\n",
       " 'out',\n",
       " 'hand',\n",
       " 'nuala',\n",
       " 'nowplay',\n",
       " 'sun',\n",
       " 'lancast',\n",
       " 'follow',\n",
       " 'topstori',\n",
       " 'spain',\n",
       " 'economi',\n",
       " 'expand',\n",
       " '07',\n",
       " 'despit',\n",
       " 'polit',\n",
       " 'deadlock',\n",
       " 'forb',\n",
       " 'here',\n",
       " 'oppo',\n",
       " 'good',\n",
       " 'buddi',\n",
       " 'resid',\n",
       " 'shredder',\n",
       " 'from',\n",
       " 'rockit',\n",
       " 'round',\n",
       " 'good',\n",
       " 'egg',\n",
       " 'get',\n",
       " 'well',\n",
       " 'soon',\n",
       " 'wayn',\n",
       " 'x',\n",
       " 'woolwoh',\n",
       " 'get',\n",
       " 'badreview',\n",
       " 'new',\n",
       " 'improv',\n",
       " 'rewardcard',\n",
       " 'off',\n",
       " 'you',\n",
       " 'go',\n",
       " 'christma',\n",
       " 'earn',\n",
       " 'mplusreward',\n",
       " 'pick',\n",
       " 'line',\n",
       " 'get',\n",
       " 'kinda',\n",
       " 'glad',\n",
       " 'difficult',\n",
       " 'deal',\n",
       " 'understand',\n",
       " 'know',\n",
       " 'someon',\n",
       " 'stay',\n",
       " 'around',\n",
       " 'truli',\n",
       " 'want',\n",
       " 'kyungi',\n",
       " 'turn',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'yall',\n",
       " 'proud',\n",
       " 'baekhyun',\n",
       " 'work',\n",
       " 'ab',\n",
       " 'gotten',\n",
       " 'realli',\n",
       " 'skinni',\n",
       " 'offer',\n",
       " 'oppoun',\n",
       " 'pay',\n",
       " 'you',\n",
       " '50',\n",
       " 'get',\n",
       " 'back',\n",
       " 'featur',\n",
       " 'lost',\n",
       " 'take',\n",
       " 'immigr',\n",
       " 'poignantli',\n",
       " 'point',\n",
       " 'out',\n",
       " 'disgrac',\n",
       " 'moral',\n",
       " 'pygmi',\n",
       " 'paul',\n",
       " 'ryan',\n",
       " 'mitch',\n",
       " 'mcconnel',\n",
       " 'anni',\n",
       " 'time',\n",
       " 'set',\n",
       " 'off',\n",
       " 'alway',\n",
       " 'good',\n",
       " 'day',\n",
       " 'you',\n",
       " 'get',\n",
       " 'phantom',\n",
       " 'out',\n",
       " 'avenham',\n",
       " 'park',\n",
       " 'win',\n",
       " 'loos',\n",
       " 'draw',\n",
       " 'get',\n",
       " 'sent',\n",
       " 'off',\n",
       " 'absolut',\n",
       " 'knobhead',\n",
       " 'saw',\n",
       " 'crow',\n",
       " 'gallop',\n",
       " 'across',\n",
       " 'road',\n",
       " 'mmkay',\n",
       " 'wut',\n",
       " '128514',\n",
       " 'innov',\n",
       " 'from',\n",
       " 'cwt',\n",
       " 'reduc',\n",
       " 'personalis',\n",
       " 'travel',\n",
       " 'cost',\n",
       " 'well',\n",
       " 'last',\n",
       " 'weekend',\n",
       " 'take',\n",
       " 'advantag',\n",
       " '10',\n",
       " 'off',\n",
       " 'discount',\n",
       " 'believ',\n",
       " 'mani',\n",
       " '6',\n",
       " 'lesson',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'rais',\n",
       " '2',\n",
       " 'million',\n",
       " 'entrepreneur',\n",
       " 'mom',\n",
       " 'want',\n",
       " 'buy',\n",
       " 'zebra',\n",
       " 'like',\n",
       " 'real',\n",
       " 'point',\n",
       " 'moron',\n",
       " 'trump',\n",
       " 'say',\n",
       " 'hell',\n",
       " 'put',\n",
       " 'american',\n",
       " 'work',\n",
       " 'yet',\n",
       " 'get',\n",
       " 'visa',\n",
       " 'immigr',\n",
       " 'work',\n",
       " 'golf',\n",
       " 'cours',\n",
       " 'fl',\n",
       " 'lilli',\n",
       " 'tri',\n",
       " 'climb',\n",
       " 'tree',\n",
       " 'bird',\n",
       " 'complet',\n",
       " 'fail',\n",
       " 'oh',\n",
       " 'god',\n",
       " '128514',\n",
       " 'you',\n",
       " 'fan',\n",
       " 'even',\n",
       " 'like',\n",
       " 'respect',\n",
       " 'famili',\n",
       " 'prayforhay',\n",
       " 'keep',\n",
       " 'see',\n",
       " 'ad',\n",
       " 'crisp',\n",
       " 'cover',\n",
       " 'gold',\n",
       " 'glitteri',\n",
       " 'star',\n",
       " 'you',\n",
       " 'get',\n",
       " 'them',\n",
       " 'mamp',\n",
       " 'cleveleysno',\n",
       " 'job',\n",
       " 'might',\n",
       " 'great',\n",
       " 'fit',\n",
       " 'you',\n",
       " 'senior',\n",
       " 'engin',\n",
       " 'engin',\n",
       " 'westgreenwich',\n",
       " 'ri',\n",
       " 'hire',\n",
       " 'careerarc',\n",
       " 'ive',\n",
       " 'heard',\n",
       " 'brexit',\n",
       " 'debat',\n",
       " 'american',\n",
       " 'made',\n",
       " 'stat',\n",
       " 'mud',\n",
       " 'sling',\n",
       " 'fed',\n",
       " 'hate',\n",
       " 'divis',\n",
       " 'thing',\n",
       " 'get',\n",
       " 'better',\n",
       " 'howard',\n",
       " 'jone',\n",
       " 'nowplay',\n",
       " 'sun',\n",
       " 'lancast',\n",
       " 'download',\n",
       " 'you',\n",
       " 'love',\n",
       " 'someon',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'thing',\n",
       " 'right',\n",
       " 'time',\n",
       " 'right',\n",
       " 'place',\n",
       " 'long',\n",
       " 'hello',\n",
       " 'goodonight',\n",
       " 'everyon',\n",
       " 'special',\n",
       " 'number',\n",
       " '1',\n",
       " 'station',\n",
       " 'tv',\n",
       " 'indonesia',\n",
       " 'pleas',\n",
       " 'invit',\n",
       " 'come',\n",
       " 'thankyou',\n",
       " 'you',\n",
       " 'sock',\n",
       " 'give',\n",
       " 'extra',\n",
       " 'ankl',\n",
       " 'suppo',\n",
       " 'off',\n",
       " 'road',\n",
       " 'run',\n",
       " 'fuck',\n",
       " 'qualiti',\n",
       " 'big',\n",
       " 'man',\n",
       " 'googl',\n",
       " 'adsens',\n",
       " 'mobil',\n",
       " 'text',\n",
       " 'ad',\n",
       " 'get',\n",
       " 'new',\n",
       " 'look',\n",
       " 'rohith',\n",
       " 'vemula',\n",
       " 'awaken',\n",
       " 'let',\n",
       " 'sacrific',\n",
       " 'go',\n",
       " 'vain',\n",
       " ...]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mngrams\u001b[1;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-e24b39d3bb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mall_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_processed_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vocabulary size: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-e24b39d3bb40>\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(tweets)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#trigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_vocabulary(tweets):\n",
    "    all_words = []\n",
    "    for word_list in tweets:\n",
    "        # unigrams\n",
    "        all_words.extend(word_list)\n",
    "        \n",
    "        # bigrams\n",
    "        bigrams = list(ngrams(word_list, 2))\n",
    "        \n",
    "        #trigrams \n",
    "        trigrams = list(ngrams(word_list, 3))\n",
    "        \n",
    "        all_words.extend(bigrams)\n",
    "        all_words.extend(trigrams)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "vocab = get_vocabulary(pre_processed_data['tweet'])\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbour',\n",
       " 'called',\n",
       " 'paki',\n",
       " 'people',\n",
       " 'respect',\n",
       " ('neighbour', 'called'),\n",
       " ('called', 'paki'),\n",
       " ('paki', 'people'),\n",
       " ('people', 'respect'),\n",
       " ('neighbour', 'called', 'paki'),\n",
       " ('called', 'paki', 'people'),\n",
       " ('paki', 'people', 'respect'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'cheat',\n",
       " 'fucking',\n",
       " 'stupid',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('cheat', 'fucking'),\n",
       " ('fucking', 'stupid'),\n",
       " ('stupid', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('cheat', 'fucking', 'stupid'),\n",
       " ('fucking', 'stupid', 'ass'),\n",
       " ('stupid', 'ass', 'prick'),\n",
       " 'saw',\n",
       " 'graffiti',\n",
       " 'train',\n",
       " 'today',\n",
       " 'said',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'black',\n",
       " 'slagfound',\n",
       " 'pretty',\n",
       " 'funny',\n",
       " ('saw', 'graffiti'),\n",
       " ('graffiti', 'train'),\n",
       " ('train', 'today'),\n",
       " ('today', 'said'),\n",
       " ('said', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'home'),\n",
       " ('home', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'slagfound'),\n",
       " ('slagfound', 'pretty'),\n",
       " ('pretty', 'funny'),\n",
       " ('saw', 'graffiti', 'train'),\n",
       " ('graffiti', 'train', 'today'),\n",
       " ('train', 'today', 'said'),\n",
       " ('today', 'said', 'fuck'),\n",
       " ('said', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'home'),\n",
       " ('off', 'home', 'you'),\n",
       " ('home', 'you', 'black'),\n",
       " ('you', 'black', 'slagfound'),\n",
       " ('black', 'slagfound', 'pretty'),\n",
       " ('slagfound', 'pretty', 'funny'),\n",
       " 'hate',\n",
       " 'people',\n",
       " 'wrong',\n",
       " 'said',\n",
       " '975',\n",
       " 'people',\n",
       " 'cunts',\n",
       " 'much',\n",
       " 'much',\n",
       " 'higher',\n",
       " ('hate', 'people'),\n",
       " ('people', 'wrong'),\n",
       " ('wrong', 'said'),\n",
       " ('said', '975'),\n",
       " ('975', 'people'),\n",
       " ('people', 'cunts'),\n",
       " ('cunts', 'much'),\n",
       " ('much', 'much'),\n",
       " ('much', 'higher'),\n",
       " ('hate', 'people', 'wrong'),\n",
       " ('people', 'wrong', 'said'),\n",
       " ('wrong', 'said', '975'),\n",
       " ('said', '975', 'people'),\n",
       " ('975', 'people', 'cunts'),\n",
       " ('people', 'cunts', 'much'),\n",
       " ('cunts', 'much', 'much'),\n",
       " ('much', 'much', 'higher'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'terrorists'),\n",
       " ('bastard', 'muslim', 'terrorists'),\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " 'same',\n",
       " ('muslim', 'terrorists'),\n",
       " ('terrorists', 'same'),\n",
       " ('muslim', 'terrorists', 'same'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'wearing',\n",
       " 'veil',\n",
       " ('paki', 'cunt'),\n",
       " ('cunt', 'wearing'),\n",
       " ('wearing', 'veil'),\n",
       " ('paki', 'cunt', 'wearing'),\n",
       " ('cunt', 'wearing', 'veil'),\n",
       " 'fucking',\n",
       " 'fascist',\n",
       " 'fucking',\n",
       " 'liberal',\n",
       " 'fucking',\n",
       " 'racist',\n",
       " 'fucking',\n",
       " 'traitor',\n",
       " 'fucking',\n",
       " 'petrolhead',\n",
       " 'fucking',\n",
       " 'treehugger',\n",
       " 'steve',\n",
       " 'you',\n",
       " 'steph',\n",
       " 'mate',\n",
       " ('fucking', 'fascist'),\n",
       " ('fascist', 'fucking'),\n",
       " ('fucking', 'liberal'),\n",
       " ('liberal', 'fucking'),\n",
       " ('fucking', 'racist'),\n",
       " ('racist', 'fucking'),\n",
       " ('fucking', 'traitor'),\n",
       " ('traitor', 'fucking'),\n",
       " ('fucking', 'petrolhead'),\n",
       " ('petrolhead', 'fucking'),\n",
       " ('fucking', 'treehugger'),\n",
       " ('treehugger', 'steve'),\n",
       " ('steve', 'you'),\n",
       " ('you', 'steph'),\n",
       " ('steph', 'mate'),\n",
       " ('fucking', 'fascist', 'fucking'),\n",
       " ('fascist', 'fucking', 'liberal'),\n",
       " ('fucking', 'liberal', 'fucking'),\n",
       " ('liberal', 'fucking', 'racist'),\n",
       " ('fucking', 'racist', 'fucking'),\n",
       " ('racist', 'fucking', 'traitor'),\n",
       " ('fucking', 'traitor', 'fucking'),\n",
       " ('traitor', 'fucking', 'petrolhead'),\n",
       " ('fucking', 'petrolhead', 'fucking'),\n",
       " ('petrolhead', 'fucking', 'treehugger'),\n",
       " ('fucking', 'treehugger', 'steve'),\n",
       " ('treehugger', 'steve', 'you'),\n",
       " ('steve', 'you', 'steph'),\n",
       " ('you', 'steph', 'mate'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunt'),\n",
       " 'man',\n",
       " 'called',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'back',\n",
       " 'country',\n",
       " 'rude',\n",
       " ('man', 'called'),\n",
       " ('called', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('bastard', 'go'),\n",
       " ('go', 'back'),\n",
       " ('back', 'country'),\n",
       " ('country', 'rude'),\n",
       " ('man', 'called', 'black'),\n",
       " ('called', 'black', 'bastard'),\n",
       " ('black', 'bastard', 'go'),\n",
       " ('bastard', 'go', 'back'),\n",
       " ('go', 'back', 'country'),\n",
       " ('back', 'country', 'rude'),\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'along',\n",
       " 'europe',\n",
       " ('scottish', 'bastard'),\n",
       " ('bastard', 'get'),\n",
       " ('get', 'rid'),\n",
       " ('rid', 'them'),\n",
       " ('them', 'along'),\n",
       " ('along', 'europe'),\n",
       " ('scottish', 'bastard', 'get'),\n",
       " ('bastard', 'get', 'rid'),\n",
       " ('get', 'rid', 'them'),\n",
       " ('rid', 'them', 'along'),\n",
       " ('them', 'along', 'europe'),\n",
       " 'many',\n",
       " 'chinks',\n",
       " 'london',\n",
       " ('many', 'chinks'),\n",
       " ('chinks', 'london'),\n",
       " ('many', 'chinks', 'london'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'back',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'back'),\n",
       " ('back', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'back'),\n",
       " ('you', 'back', 'paki'),\n",
       " ('back', 'paki', 'bastard'),\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'like',\n",
       " 'you',\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'like'),\n",
       " ('like', 'you'),\n",
       " ('hate', 'faggots', 'like'),\n",
       " ('faggots', 'like', 'you'),\n",
       " 'shut',\n",
       " 'nigger',\n",
       " 'whore',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'raped',\n",
       " 'one',\n",
       " 'animals',\n",
       " 'might',\n",
       " 'change',\n",
       " 'tune',\n",
       " ('shut', 'nigger'),\n",
       " ('nigger', 'whore'),\n",
       " ('whore', 'hope'),\n",
       " ('hope', 'get'),\n",
       " ('get', 'raped'),\n",
       " ('raped', 'one'),\n",
       " ('one', 'animals'),\n",
       " ('animals', 'might'),\n",
       " ('might', 'change'),\n",
       " ('change', 'tune'),\n",
       " ('shut', 'nigger', 'whore'),\n",
       " ('nigger', 'whore', 'hope'),\n",
       " ('whore', 'hope', 'get'),\n",
       " ('hope', 'get', 'raped'),\n",
       " ('get', 'raped', 'one'),\n",
       " ('raped', 'one', 'animals'),\n",
       " ('one', 'animals', 'might'),\n",
       " ('animals', 'might', 'change'),\n",
       " ('might', 'change', 'tune'),\n",
       " 'black',\n",
       " 'bitch',\n",
       " 'next',\n",
       " 'shut',\n",
       " ('black', 'bitch'),\n",
       " ('bitch', 'next'),\n",
       " ('next', 'shut'),\n",
       " ('black', 'bitch', 'next'),\n",
       " ('bitch', 'next', 'shut'),\n",
       " 'fucking',\n",
       " 'chinks',\n",
       " ('fucking', 'chinks'),\n",
       " 'coloured',\n",
       " 'fuckers',\n",
       " 'leave',\n",
       " 'country',\n",
       " ('coloured', 'fuckers'),\n",
       " ('fuckers', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('coloured', 'fuckers', 'leave'),\n",
       " ('fuckers', 'leave', 'country'),\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'cunt'),\n",
       " ('fucking', 'paki', 'cunt'),\n",
       " 'brexit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " ('brexit', 'time'),\n",
       " ('time', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('brexit', 'time', 'fuck'),\n",
       " ('time', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'polish'),\n",
       " ('you', 'polish', 'aliens'),\n",
       " 'black',\n",
       " 'nigger',\n",
       " ('black', 'nigger'),\n",
       " 'least',\n",
       " 'nigger',\n",
       " 'lmfao',\n",
       " ('least', 'nigger'),\n",
       " ('nigger', 'lmfao'),\n",
       " ('least', 'nigger', 'lmfao'),\n",
       " 'neighbours',\n",
       " 'complained',\n",
       " 'shed',\n",
       " 'front',\n",
       " 'garden',\n",
       " 'whos',\n",
       " 'neighbour',\n",
       " 'church',\n",
       " 'england',\n",
       " 'bunch',\n",
       " 'god',\n",
       " 'bothering',\n",
       " 'cunts',\n",
       " ('neighbours', 'complained'),\n",
       " ('complained', 'shed'),\n",
       " ('shed', 'front'),\n",
       " ('front', 'garden'),\n",
       " ('garden', 'whos'),\n",
       " ('whos', 'neighbour'),\n",
       " ('neighbour', 'church'),\n",
       " ('church', 'england'),\n",
       " ('england', 'bunch'),\n",
       " ('bunch', 'god'),\n",
       " ('god', 'bothering'),\n",
       " ('bothering', 'cunts'),\n",
       " ('neighbours', 'complained', 'shed'),\n",
       " ('complained', 'shed', 'front'),\n",
       " ('shed', 'front', 'garden'),\n",
       " ('front', 'garden', 'whos'),\n",
       " ('garden', 'whos', 'neighbour'),\n",
       " ('whos', 'neighbour', 'church'),\n",
       " ('neighbour', 'church', 'england'),\n",
       " ('church', 'england', 'bunch'),\n",
       " ('england', 'bunch', 'god'),\n",
       " ('bunch', 'god', 'bothering'),\n",
       " ('god', 'bothering', 'cunts'),\n",
       " 'bunch',\n",
       " 'paki',\n",
       " 'perves',\n",
       " ('bunch', 'paki'),\n",
       " ('paki', 'perves'),\n",
       " ('bunch', 'paki', 'perves'),\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'allah',\n",
       " 'lovers',\n",
       " ('fucking', 'hate'),\n",
       " ('hate', 'allah'),\n",
       " ('allah', 'lovers'),\n",
       " ('fucking', 'hate', 'allah'),\n",
       " ('hate', 'allah', 'lovers'),\n",
       " 'fucking',\n",
       " 'immigrants',\n",
       " ('fucking', 'immigrants'),\n",
       " 'fucking',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " 'coming',\n",
       " 'taking',\n",
       " 'jobs',\n",
       " 'unacceptable',\n",
       " ('fucking', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('aliens', 'coming'),\n",
       " ('coming', 'taking'),\n",
       " ('taking', 'jobs'),\n",
       " ('jobs', 'unacceptable'),\n",
       " ('fucking', 'polish', 'aliens'),\n",
       " ('polish', 'aliens', 'coming'),\n",
       " ('aliens', 'coming', 'taking'),\n",
       " ('coming', 'taking', 'jobs'),\n",
       " ('taking', 'jobs', 'unacceptable'),\n",
       " 'local',\n",
       " 'mosque',\n",
       " 'let',\n",
       " 'terrorists',\n",
       " 'out',\n",
       " ('local', 'mosque'),\n",
       " ('mosque', 'let'),\n",
       " ('let', 'terrorists'),\n",
       " ('terrorists', 'out'),\n",
       " ('local', 'mosque', 'let'),\n",
       " ('mosque', 'let', 'terrorists'),\n",
       " ('let', 'terrorists', 'out'),\n",
       " 'racist',\n",
       " 'sexist',\n",
       " 'perv',\n",
       " 'clearly',\n",
       " 'wants',\n",
       " 'fuck',\n",
       " 'daughter',\n",
       " 'disgusting',\n",
       " ('racist', 'sexist'),\n",
       " ('sexist', 'perv'),\n",
       " ('perv', 'clearly'),\n",
       " ('clearly', 'wants'),\n",
       " ('wants', 'fuck'),\n",
       " ('fuck', 'daughter'),\n",
       " ('daughter', 'disgusting'),\n",
       " ('racist', 'sexist', 'perv'),\n",
       " ('sexist', 'perv', 'clearly'),\n",
       " ('perv', 'clearly', 'wants'),\n",
       " ('clearly', 'wants', 'fuck'),\n",
       " ('wants', 'fuck', 'daughter'),\n",
       " ('fuck', 'daughter', 'disgusting'),\n",
       " 'pakis',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'please',\n",
       " ('pakis', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'please'),\n",
       " ('pakis', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'please'),\n",
       " 'hate',\n",
       " 'niggers',\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'hate',\n",
       " 'spicskkk',\n",
       " 'rally',\n",
       " ('hate', 'niggers'),\n",
       " ('niggers', 'hate'),\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'hate'),\n",
       " ('hate', 'spicskkk'),\n",
       " ('spicskkk', 'rally'),\n",
       " ('hate', 'niggers', 'hate'),\n",
       " ('niggers', 'hate', 'faggots'),\n",
       " ('hate', 'faggots', 'hate'),\n",
       " ('faggots', 'hate', 'spicskkk'),\n",
       " ('hate', 'spicskkk', 'rally'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunts',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunts'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunts'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'paki'),\n",
       " ('fucking', 'paki', 'bastard'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'myriah',\n",
       " 'chavcentral',\n",
       " 'chav',\n",
       " 'central',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'myriah'),\n",
       " ('myriah', 'chavcentral'),\n",
       " ('chavcentral', 'chav'),\n",
       " ('chav', 'central'),\n",
       " ('fuck', 'off', 'myriah'),\n",
       " ('off', 'myriah', 'chavcentral'),\n",
       " ('myriah', 'chavcentral', 'chav'),\n",
       " ('chavcentral', 'chav', 'central'),\n",
       " 'diy',\n",
       " 'terrorist',\n",
       " 'religion',\n",
       " 'fucking',\n",
       " 'joke',\n",
       " 'you',\n",
       " 'go',\n",
       " 'around',\n",
       " 'screaming',\n",
       " 'allah',\n",
       " 'akbar',\n",
       " 'terrorist',\n",
       " 'shit',\n",
       " 'diy',\n",
       " 'faggot',\n",
       " ('diy', 'terrorist'),\n",
       " ('terrorist', 'religion'),\n",
       " ('religion', 'fucking'),\n",
       " ('fucking', 'joke'),\n",
       " ('joke', 'you'),\n",
       " ('you', 'go'),\n",
       " ('go', 'around'),\n",
       " ('around', 'screaming'),\n",
       " ('screaming', 'allah'),\n",
       " ('allah', 'akbar'),\n",
       " ('akbar', 'terrorist'),\n",
       " ('terrorist', 'shit'),\n",
       " ('shit', 'diy'),\n",
       " ('diy', 'faggot'),\n",
       " ('diy', 'terrorist', 'religion'),\n",
       " ('terrorist', 'religion', 'fucking'),\n",
       " ('religion', 'fucking', 'joke'),\n",
       " ('fucking', 'joke', 'you'),\n",
       " ('joke', 'you', 'go'),\n",
       " ('you', 'go', 'around'),\n",
       " ('go', 'around', 'screaming'),\n",
       " ('around', 'screaming', 'allah'),\n",
       " ('screaming', 'allah', 'akbar'),\n",
       " ('allah', 'akbar', 'terrorist'),\n",
       " ('akbar', 'terrorist', 'shit'),\n",
       " ('terrorist', 'shit', 'diy'),\n",
       " ('shit', 'diy', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'poland',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'back'),\n",
       " ('back', 'poland'),\n",
       " ('fuck', 'off', 'back'),\n",
       " ('off', 'back', 'poland'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'scum',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'scum'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'scum'),\n",
       " 'shut',\n",
       " 'fuck',\n",
       " ('shut', 'fuck'),\n",
       " 'fucking',\n",
       " 'sand',\n",
       " 'monkeys',\n",
       " ('fucking', 'sand'),\n",
       " ('sand', 'monkeys'),\n",
       " ('fucking', 'sand', 'monkeys'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('you', 'fucking', 'faggot'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'irish',\n",
       " 'cow',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'irish'),\n",
       " ('irish', 'cow'),\n",
       " ('you', 'fucking', 'irish'),\n",
       " ('fucking', 'irish', 'cow'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('paki', 'cunt'),\n",
       " 'you',\n",
       " 'muslim',\n",
       " 'prick',\n",
       " ('you', 'muslim'),\n",
       " ('muslim', 'prick'),\n",
       " ('you', 'muslim', 'prick'),\n",
       " 'many',\n",
       " 'wogs',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'brexit',\n",
       " ('many', 'wogs'),\n",
       " ('wogs', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('country', 'brexit'),\n",
       " ('many', 'wogs', 'leave'),\n",
       " ('wogs', 'leave', 'country'),\n",
       " ('leave', 'country', 'brexit'),\n",
       " 'least',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jefree',\n",
       " 'starr',\n",
       " 'faggot',\n",
       " ('least', 'look'),\n",
       " ('look', 'like'),\n",
       " ('like', 'jefree'),\n",
       " ('jefree', 'starr'),\n",
       " ('starr', 'faggot'),\n",
       " ('least', 'look', 'like'),\n",
       " ('look', 'like', 'jefree'),\n",
       " ('like', 'jefree', 'starr'),\n",
       " ('jefree', 'starr', 'faggot'),\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " ('stupid', 'bitch'),\n",
       " 'benjamin',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('benjamin', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('benjamin', 'fucking', 'faggot'),\n",
       " 'good',\n",
       " 'night',\n",
       " 'fags',\n",
       " 'fagettes',\n",
       " 'female',\n",
       " 'version',\n",
       " 'fags',\n",
       " 'like',\n",
       " 'fagettes',\n",
       " 'make',\n",
       " 'female',\n",
       " 'hate',\n",
       " 'you',\n",
       " ('good', 'night'),\n",
       " ('night', 'fags'),\n",
       " ('fags', 'fagettes'),\n",
       " ('fagettes', 'female'),\n",
       " ('female', 'version'),\n",
       " ('version', 'fags'),\n",
       " ('fags', 'like'),\n",
       " ('like', 'fagettes'),\n",
       " ('fagettes', 'make'),\n",
       " ('make', 'female'),\n",
       " ('female', 'hate'),\n",
       " ('hate', 'you'),\n",
       " ('good', 'night', 'fags'),\n",
       " ('night', 'fags', 'fagettes'),\n",
       " ('fags', 'fagettes', 'female'),\n",
       " ('fagettes', 'female', 'version'),\n",
       " ('female', 'version', 'fags'),\n",
       " ('version', 'fags', 'like'),\n",
       " ('fags', 'like', 'fagettes'),\n",
       " ('like', 'fagettes', 'make'),\n",
       " ('fagettes', 'make', 'female'),\n",
       " ('make', 'female', 'hate'),\n",
       " ('female', 'hate', 'you'),\n",
       " 'thick',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " ('thick', 'scottish'),\n",
       " ('scottish', 'bastard'),\n",
       " ('thick', 'scottish', 'bastard'),\n",
       " 'bastard',\n",
       " 'wog',\n",
       " 'road',\n",
       " 'annoying',\n",
       " ('bastard', 'wog'),\n",
       " ('wog', 'road'),\n",
       " ('road', 'annoying'),\n",
       " ('bastard', 'wog', 'road'),\n",
       " ('wog', 'road', 'annoying'),\n",
       " 'polish',\n",
       " 'vermin',\n",
       " ('polish', 'vermin'),\n",
       " 'diy',\n",
       " 'paki',\n",
       " ('diy', 'paki'),\n",
       " 'used',\n",
       " 'tie',\n",
       " 'ends',\n",
       " 'niggers',\n",
       " 'legs',\n",
       " '2',\n",
       " 'different',\n",
       " 'horses',\n",
       " 'beat',\n",
       " 'horses',\n",
       " 'spread',\n",
       " 'legs',\n",
       " 'break',\n",
       " 'smh',\n",
       " ('used', 'tie'),\n",
       " ('tie', 'ends'),\n",
       " ('ends', 'niggers'),\n",
       " ('niggers', 'legs'),\n",
       " ('legs', '2'),\n",
       " ('2', 'different'),\n",
       " ('different', 'horses'),\n",
       " ('horses', 'beat'),\n",
       " ('beat', 'horses'),\n",
       " ('horses', 'spread'),\n",
       " ('spread', 'legs'),\n",
       " ('legs', 'break'),\n",
       " ('break', 'smh'),\n",
       " ('used', 'tie', 'ends'),\n",
       " ('tie', 'ends', 'niggers'),\n",
       " ('ends', 'niggers', 'legs'),\n",
       " ('niggers', 'legs', '2'),\n",
       " ('legs', '2', 'different'),\n",
       " ('2', 'different', 'horses'),\n",
       " ('different', 'horses', 'beat'),\n",
       " ('horses', 'beat', 'horses'),\n",
       " ('beat', 'horses', 'spread'),\n",
       " ('horses', 'spread', 'legs'),\n",
       " ('spread', 'legs', 'break'),\n",
       " ('legs', 'break', 'smh'),\n",
       " 'polish',\n",
       " 'bitch',\n",
       " ('polish', 'bitch'),\n",
       " 'last',\n",
       " 'time',\n",
       " 'preston',\n",
       " 'anymore',\n",
       " 'inconsiderate',\n",
       " 'twats',\n",
       " 'm6m61',\n",
       " 'way',\n",
       " 'home',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'cut',\n",
       " 'bitch',\n",
       " ('last', 'time'),\n",
       " ('time', 'preston'),\n",
       " ('preston', 'anymore'),\n",
       " ('anymore', 'inconsiderate'),\n",
       " ('inconsiderate', 'twats'),\n",
       " ('twats', 'm6m61'),\n",
       " ('m6m61', 'way'),\n",
       " ('way', 'home'),\n",
       " ('home', 'gon'),\n",
       " ('gon', 'na'),\n",
       " ('na', 'cut'),\n",
       " ('cut', 'bitch'),\n",
       " ('last', 'time', 'preston'),\n",
       " ('time', 'preston', 'anymore'),\n",
       " ('preston', 'anymore', 'inconsiderate'),\n",
       " ('anymore', 'inconsiderate', 'twats'),\n",
       " ('inconsiderate', 'twats', 'm6m61'),\n",
       " ('twats', 'm6m61', 'way'),\n",
       " ('m6m61', 'way', 'home'),\n",
       " ('way', 'home', 'gon'),\n",
       " ('home', 'gon', 'na'),\n",
       " ('gon', 'na', 'cut'),\n",
       " ('na', 'cut', 'bitch'),\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'fuck',\n",
       " 'anyone',\n",
       " 'british',\n",
       " ('hate', 'immigrants'),\n",
       " ('immigrants', 'fuck'),\n",
       " ('fuck', 'anyone'),\n",
       " ('anyone', 'british'),\n",
       " ('hate', 'immigrants', 'fuck'),\n",
       " ('immigrants', 'fuck', 'anyone'),\n",
       " ('fuck', 'anyone', 'british'),\n",
       " 'you',\n",
       " 'even',\n",
       " 'country',\n",
       " 'you',\n",
       " 'pakis',\n",
       " ('you', 'even'),\n",
       " ('even', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'pakis'),\n",
       " ('you', 'even', 'country'),\n",
       " ('even', 'country', 'you'),\n",
       " ('country', 'you', 'pakis'),\n",
       " 'fucking',\n",
       " 'white',\n",
       " 'bitch',\n",
       " 'road',\n",
       " 'needs',\n",
       " 'out',\n",
       " ('fucking', 'white'),\n",
       " ('white', 'bitch'),\n",
       " ('bitch', 'road'),\n",
       " ('road', 'needs'),\n",
       " ('needs', 'out'),\n",
       " ('fucking', 'white', 'bitch'),\n",
       " ('white', 'bitch', 'road'),\n",
       " ('bitch', 'road', 'needs'),\n",
       " ('road', 'needs', 'out'),\n",
       " 'get',\n",
       " 'out',\n",
       " 'country',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'foreigners',\n",
       " ('get', 'out'),\n",
       " ('out', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'foreigners'),\n",
       " ('get', 'out', 'country'),\n",
       " ('out', 'country', 'you'),\n",
       " ('country', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'foreigners'),\n",
       " 'enough',\n",
       " 'fucking',\n",
       " 'pakis',\n",
       " 'man',\n",
       " 'time',\n",
       " 'leave',\n",
       " 'europe',\n",
       " ('enough', 'fucking'),\n",
       " ('fucking', 'pakis'),\n",
       " ('pakis', 'man'),\n",
       " ('man', 'time'),\n",
       " ('time', 'leave'),\n",
       " ('leave', 'europe'),\n",
       " ('enough', 'fucking', 'pakis'),\n",
       " ('fucking', 'pakis', 'man'),\n",
       " ('pakis', 'man', 'time'),\n",
       " ('man', 'time', 'leave'),\n",
       " ('time', 'leave', 'europe'),\n",
       " 'kill',\n",
       " 'you',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('kill', 'you'),\n",
       " ('you', 'you'),\n",
       " ('you', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('kill', 'you', 'you'),\n",
       " ('you', 'you', 'paki'),\n",
       " ('you', 'paki', 'bastard'),\n",
       " 'you',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('you', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('you', 'ass', 'prick'),\n",
       " 'smelly',\n",
       " 'pakis',\n",
       " ('smelly', 'pakis'),\n",
       " 'niggers',\n",
       " 'road',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'shouting',\n",
       " 'go',\n",
       " 'round',\n",
       " 'shut',\n",
       " 'them',\n",
       " ('niggers', 'road'),\n",
       " ('road', 'better'),\n",
       " ('better', 'stop'),\n",
       " ('stop', 'shouting'),\n",
       " ('shouting', 'go'),\n",
       " ('go', 'round'),\n",
       " ('round', 'shut'),\n",
       " ('shut', 'them'),\n",
       " ('niggers', 'road', 'better'),\n",
       " ('road', 'better', 'stop'),\n",
       " ('better', 'stop', 'shouting'),\n",
       " ('stop', 'shouting', 'go'),\n",
       " ('shouting', 'go', 'round'),\n",
       " ('go', 'round', 'shut'),\n",
       " ('round', 'shut', 'them'),\n",
       " 'fucking',\n",
       " 'bitch',\n",
       " ('fucking', 'bitch'),\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " ('cunt', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('cunt', 'fuck', 'off'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'dogs',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'dogs'),\n",
       " ('bastard', 'muslim', 'dogs'),\n",
       " 'fuck',\n",
       " 'dykes',\n",
       " ('fuck', 'dykes'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'fuck',\n",
       " 'jezanie',\n",
       " 'cunts',\n",
       " 'cbb',\n",
       " ('fuck', 'jezanie'),\n",
       " ('jezanie', 'cunts'),\n",
       " ('cunts', 'cbb'),\n",
       " ('fuck', 'jezanie', 'cunts'),\n",
       " ('jezanie', 'cunts', 'cbb'),\n",
       " 'fat',\n",
       " 'little',\n",
       " 'wanker',\n",
       " 'you',\n",
       " 'problem',\n",
       " 'say',\n",
       " 'face',\n",
       " 'behind',\n",
       " 'back',\n",
       " 'faggot',\n",
       " ('fat', 'little'),\n",
       " ('little', 'wanker'),\n",
       " ('wanker', 'you'),\n",
       " ('you', 'problem'),\n",
       " ('problem', 'say'),\n",
       " ('say', 'face'),\n",
       " ('face', 'behind'),\n",
       " ('behind', 'back'),\n",
       " ('back', 'faggot'),\n",
       " ('fat', 'little', 'wanker'),\n",
       " ('little', 'wanker', 'you'),\n",
       " ('wanker', 'you', 'problem'),\n",
       " ('you', 'problem', 'say'),\n",
       " ('problem', 'say', 'face'),\n",
       " ('say', 'face', 'behind'),\n",
       " ('face', 'behind', 'back'),\n",
       " ('behind', 'back', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'immigrants',\n",
       " 'brexit',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'immigrants'),\n",
       " ('immigrants', 'brexit'),\n",
       " ('fuck', 'off', 'immigrants'),\n",
       " ('off', 'immigrants', 'brexit'),\n",
       " 'fuck',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'sheboon',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'strung',\n",
       " 'like',\n",
       " 'niggers',\n",
       " 'whitepower',\n",
       " '1488',\n",
       " ('fuck', 'you'),\n",
       " ('you', 'nigger'),\n",
       " ('nigger', 'sheboon'),\n",
       " ('sheboon', 'hope'),\n",
       " ('hope', 'you'),\n",
       " ('you', 'strung'),\n",
       " ('strung', 'like'),\n",
       " ('like', 'niggers'),\n",
       " ('niggers', 'whitepower'),\n",
       " ('whitepower', '1488'),\n",
       " ('fuck', 'you', 'nigger'),\n",
       " ('you', 'nigger', 'sheboon'),\n",
       " ('nigger', 'sheboon', 'hope'),\n",
       " ('sheboon', 'hope', 'you'),\n",
       " ('hope', 'you', 'strung'),\n",
       " ('you', 'strung', 'like'),\n",
       " ('strung', 'like', 'niggers'),\n",
       " ('like', 'niggers', 'whitepower'),\n",
       " ('niggers', 'whitepower', '1488'),\n",
       " 'bitch',\n",
       " 'immigrants',\n",
       " 'fucking',\n",
       " 'terrorists',\n",
       " ('immigrants', 'fucking'),\n",
       " ('fucking', 'terrorists'),\n",
       " ('immigrants', 'fucking', 'terrorists'),\n",
       " 'come',\n",
       " 'out',\n",
       " 'you',\n",
       " 'black',\n",
       " 'bastard',\n",
       " ('come', 'out'),\n",
       " ('out', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('come', 'out', 'you'),\n",
       " ('out', 'you', 'black'),\n",
       " ('you', 'black', 'bastard'),\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('go', 'fuck'),\n",
       " ('fuck', 'yourself'),\n",
       " ('yourself', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('go', 'fuck', 'yourself'),\n",
       " ('fuck', 'yourself', 'you'),\n",
       " ('yourself', 'you', 'cunt'),\n",
       " 'go',\n",
       " 'home',\n",
       " ...]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of the tokens are duplicates. This could be because (1) they are repeated within a tweet or (2) they are present in multiple tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that features are tokens or combination of tokens (ngrams).\n",
    "\n",
    "Let's have a look at how frequent each feature is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAE4CAYAAAC0d+/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(vocab)\n",
    "fd.plot(20, cumulative=True)\n",
    "#fd.xlabel('Most common features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 46),\n",
       " ('fucking', 33),\n",
       " ('off', 30),\n",
       " ('fuck', 27),\n",
       " ('get', 19),\n",
       " ('go', 18),\n",
       " ('hate', 17),\n",
       " (('fuck', 'off'), 16),\n",
       " ('bastard', 13),\n",
       " ('bitch', 12),\n",
       " ('out', 12),\n",
       " ('paki', 10),\n",
       " ('immigrants', 10),\n",
       " ('back', 10),\n",
       " ('home', 9),\n",
       " ('country', 9),\n",
       " ('like', 9),\n",
       " ('polish', 9),\n",
       " ('leave', 8),\n",
       " (('off', 'you'), 7),\n",
       " ('time', 7),\n",
       " ('niggers', 7),\n",
       " ('people', 6),\n",
       " ('black', 6),\n",
       " ('much', 6),\n",
       " ('terrorists', 6),\n",
       " ('cunt', 6),\n",
       " (('fuck', 'off', 'you'), 6),\n",
       " ('many', 6),\n",
       " ('faggot', 6),\n",
       " ('from', 6),\n",
       " ('never', 6),\n",
       " ('muslim', 5),\n",
       " ('nigger', 5),\n",
       " ('brexit', 5),\n",
       " ('let', 5),\n",
       " ('pakis', 5),\n",
       " ('road', 5),\n",
       " ('today', 4),\n",
       " ('cunts', 4),\n",
       " ('them', 4),\n",
       " ('shut', 4),\n",
       " ('one', 4),\n",
       " (('you', 'fucking'), 4),\n",
       " ('scum', 4),\n",
       " ('good', 4),\n",
       " ('right', 4),\n",
       " ('new', 4),\n",
       " ('really', 4),\n",
       " ('respect', 3)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important for model running time and accuracy of the model: only give the classifier what matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing rare and common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping the 50 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features (tokens of ngrams) are only present in one or two tweet. We know that these are not going to be very useful to teach the model to recognise hate speech.  Let's only keep the top 50 most frequent features in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of interest here are the top 50 most frequent words. THIS IS NOT THE BEST WAY OF DOING IT! IT SHOULD REMOVE THE MOST FREQUENT WORDS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chosen features: 50/3199\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "\n",
    "def get_word_features(wordlist, n):\n",
    "    fd = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    word_features = sorted(fd.items(), key=operator.itemgetter(1), reverse=True)[0:n] \n",
    "    word_features = [i[0] for i in word_features ]\n",
    "    return word_features\n",
    "\n",
    "# Only keep the top 50 most frequent words\n",
    "chosen_features = get_word_features(vocab, 50)\n",
    "print('Number of chosen features: {}/{}'.format(len(chosen_features), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'fucking', 'off', 'fuck', 'get', 'go', 'hate', ('fuck', 'off'), 'bastard', 'bitch', 'out', 'paki', 'immigrants', 'back', 'home', 'country', 'like', 'polish', 'leave', ('off', 'you'), 'time', 'niggers', 'people', 'black', 'much', 'terrorists', 'cunt', ('fuck', 'off', 'you'), 'many', 'faggot', 'from', 'never', 'muslim', 'nigger', 'brexit', 'let', 'pakis', 'road', 'today', 'cunts', 'them', 'shut', 'one', ('you', 'fucking'), 'scum', 'good', 'right', 'new', 'really', 'respect']\n"
     ]
    }
   ],
   "source": [
    "print(chosen_features[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have chosen a list of features that we think are important for the model to learn the difference between hateful speech and non-hateful speech.\n",
    "\n",
    "We now need to somehow tell the model:\n",
    "- which features are typically present in hateful tweets and which are not,\n",
    "- which features are typically present in non-hateful tweets and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pandas dataframes are handy to manipulate data, the classifier needs each tweet to be a dictionary of words mapped to True booleans. Something like this: \n",
    "\n",
    "[({'contains('go', 'home')' : TRUE, 'contains('good')' : FALSE}, 1), ({'contains('love')' : TRUE}, 0)]\n",
    "\n",
    "0 and 1 are the labels:\n",
    "- 1 means hateful\n",
    "- 0 means non-hateful\n",
    "\n",
    "Let's make this list of tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    feature_set = {}\n",
    "    for feature in chosen_features:\n",
    "        feature_set['contains({})'.format(feature)] = (feature in document_words)\n",
    "    return feature_set\n",
    "\n",
    "tweets = [tuple(x) for x in pre_processed_data.values]\n",
    "\n",
    "feature_set = nltk.classify.apply_features(extract_features, tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to feed in training_set: 202\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets to feed in training_set: {}'.format(len(feature_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look at the first tweet. When scrolling all the way to the end, we see that this is a hateful tweet (label = 1). This will help the classifier know which features lead to being labelled hateful and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\"contains(('fuck', 'off'))\": False,\n",
       "  \"contains(('fuck', 'off', 'you'))\": False,\n",
       "  \"contains(('off', 'you'))\": False,\n",
       "  \"contains(('you', 'fucking'))\": False,\n",
       "  'contains(back)': False,\n",
       "  'contains(bastard)': False,\n",
       "  'contains(bitch)': False,\n",
       "  'contains(black)': False,\n",
       "  'contains(brexit)': False,\n",
       "  'contains(country)': False,\n",
       "  'contains(cunt)': False,\n",
       "  'contains(cunts)': False,\n",
       "  'contains(faggot)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(fuck)': False,\n",
       "  'contains(fucking)': False,\n",
       "  'contains(get)': False,\n",
       "  'contains(go)': False,\n",
       "  'contains(good)': False,\n",
       "  'contains(hate)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(immigrants)': False,\n",
       "  'contains(leave)': False,\n",
       "  'contains(let)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(many)': False,\n",
       "  'contains(much)': False,\n",
       "  'contains(muslim)': False,\n",
       "  'contains(never)': False,\n",
       "  'contains(new)': False,\n",
       "  'contains(nigger)': False,\n",
       "  'contains(niggers)': False,\n",
       "  'contains(off)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(paki)': True,\n",
       "  'contains(pakis)': False,\n",
       "  'contains(people)': True,\n",
       "  'contains(polish)': False,\n",
       "  'contains(really)': False,\n",
       "  'contains(respect)': True,\n",
       "  'contains(right)': False,\n",
       "  'contains(road)': False,\n",
       "  'contains(scum)': False,\n",
       "  'contains(shut)': False,\n",
       "  'contains(terrorists)': False,\n",
       "  'contains(them)': False,\n",
       "  'contains(time)': False,\n",
       "  'contains(today)': False,\n",
       "  'contains(you)': False},\n",
       " 1)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is pretty simple. For each tweet, we are looping through our chosen_features and setting a boolean to True if the tweet contains that feature, False otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is what the classifier needs as input.\n",
    "\n",
    "We can now train the classifier with this training_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train vs test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train the classifer and then test its classifying ability on a brand new dataset that it has never seen before. \n",
    "\n",
    "Generally, 80/20 percent is a fair split between training and testing set:\n",
    "- training dataset (80% of the data)\n",
    "- testing dataset (20% of the data)\n",
    "\n",
    "Sklearn provides a function called train_test_split to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in train data: 161\n",
      "Number of tweets in test data: 41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(feature_set, test_size=0.20, train_size=0.80)\n",
    "print('Number of tweets in train data: {}'.format(len(train_data)))\n",
    "print('Number of tweets in test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\"contains(('fuck', 'off'))\": False,\n",
       "  \"contains(('fuck', 'off', 'you'))\": False,\n",
       "  \"contains(('off', 'you'))\": False,\n",
       "  \"contains(('you', 'fucking'))\": False,\n",
       "  'contains(back)': False,\n",
       "  'contains(bastard)': False,\n",
       "  'contains(bitch)': False,\n",
       "  'contains(black)': False,\n",
       "  'contains(brexit)': False,\n",
       "  'contains(country)': False,\n",
       "  'contains(cunt)': False,\n",
       "  'contains(cunts)': False,\n",
       "  'contains(faggot)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(fuck)': False,\n",
       "  'contains(fucking)': False,\n",
       "  'contains(get)': False,\n",
       "  'contains(go)': False,\n",
       "  'contains(good)': False,\n",
       "  'contains(hate)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(immigrants)': False,\n",
       "  'contains(leave)': False,\n",
       "  'contains(let)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(many)': False,\n",
       "  'contains(much)': False,\n",
       "  'contains(muslim)': False,\n",
       "  'contains(never)': False,\n",
       "  'contains(new)': False,\n",
       "  'contains(nigger)': False,\n",
       "  'contains(niggers)': False,\n",
       "  'contains(off)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(paki)': False,\n",
       "  'contains(pakis)': False,\n",
       "  'contains(people)': False,\n",
       "  'contains(polish)': False,\n",
       "  'contains(really)': False,\n",
       "  'contains(respect)': False,\n",
       "  'contains(right)': False,\n",
       "  'contains(road)': False,\n",
       "  'contains(scum)': False,\n",
       "  'contains(shut)': False,\n",
       "  'contains(terrorists)': False,\n",
       "  'contains(them)': False,\n",
       "  'contains(time)': False,\n",
       "  'contains(today)': False,\n",
       "  'contains(you)': False},\n",
       " 0)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of model to use as a classifier. The most common one is called Naive Bayesion Classifier and that is the one we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "    contains(immigrants) = True                1 : 0      =      5.7 : 1.0\n",
      "          contains(fuck) = True                1 : 0      =      5.0 : 1.0\n",
      "    contains(terrorists) = True                1 : 0      =      4.3 : 1.0\n",
      "       contains(fucking) = True                1 : 0      =      3.9 : 1.0\n",
      "          contains(hate) = True                1 : 0      =      3.3 : 1.0\n",
      "          contains(good) = True                0 : 1      =      2.3 : 1.0\n",
      "          contains(time) = True                1 : 0      =      2.3 : 1.0\n",
      "          contains(come) = True                0 : 1      =      2.3 : 1.0\n",
      "         contains(black) = True                1 : 0      =      2.2 : 1.0\n",
      "        contains(polish) = True                1 : 0      =      2.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(training_set)\n",
    "# SHOW FEATURES\n",
    "classifier1.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "# Save the model into a pickle file\n",
    "import pickle\n",
    "f = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier1, f)\n",
    "f.close()\n",
    "\n",
    "# DecisionTree\n",
    "#classifier2 = nltk.classify.DecisionTreeClassifier.train(training_set, entropy_cutoff=0,support_cutoff=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The model has been trained on the training_set. \n",
    "\n",
    "We can see which features the model considers important to decide between hateful speech and non-hateful speech.\n",
    "\n",
    "- Column 3 shows the ratio of occurence of each informative feature in both labels (hate is 1 : non-hate is 0).\n",
    "- Column 2 shows you the direction of the ratio (which label occurs more frequently). The label on the left is the label most associated with the corresponding feature.\n",
    "\n",
    "For example, tweets containing the word 'immigrants' are 5.7 times more likely to be hateful than not.\n",
    "\n",
    "Now let's test the accuracy of our model on the test_data that we set aside earlier. These are tweets that the model has never seen before. We'll ask the model to classify them and see how its outcome compares with the true label of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8536585365853658"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =  nltk.classify.util.accuracy(classifier1, test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Use the classifier to identify hateful speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's try our classifier on a new tweet of your choice. First we need to preprocess the tweet (clean, tokenize and remove stopwords). Then we need to extract its features to look like the right input for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTweet = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tweet: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tweet\n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # clean the tweet\n",
    "    tweet = cleanTweet(testTweet)\n",
    "    \n",
    "    # tokenize the cleaned tweet\n",
    "    tokenised_tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stop words\n",
    "    tokenised_tweet_stpwd = [item for item in tokenised_tweet if item not in stopWords]\n",
    "    print('Preprocessed tweet: {}'.format(tokenised_tweet_stpwd))\n",
    "    \n",
    "    return tokenised_tweet_stpwd\n",
    "\n",
    "\n",
    "preprocessed_tweet = preprocessTweet(testTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(you)': False, 'contains(fucking)': False, 'contains(off)': False, 'contains(fuck)': False, 'contains(get)': False, 'contains(go)': False, 'contains(hate)': False, \"contains(('fuck', 'off'))\": False, 'contains(bastard)': False, 'contains(bitch)': False, 'contains(out)': False, 'contains(paki)': False, 'contains(immigrants)': False, 'contains(back)': False, 'contains(home)': False, 'contains(country)': False, 'contains(like)': False, 'contains(polish)': False, 'contains(leave)': False, \"contains(('off', 'you'))\": False, 'contains(time)': False, 'contains(niggers)': False, 'contains(people)': False, 'contains(black)': False, 'contains(much)': False, 'contains(terrorists)': False, 'contains(cunt)': False, \"contains(('fuck', 'off', 'you'))\": False, 'contains(many)': False, 'contains(faggot)': False, 'contains(from)': False, 'contains(never)': False, 'contains(muslim)': False, 'contains(nigger)': False, 'contains(brexit)': False, 'contains(let)': False, 'contains(pakis)': False, 'contains(road)': False, 'contains(today)': False, 'contains(cunts)': False, 'contains(them)': False, 'contains(shut)': False, 'contains(one)': False, \"contains(('you', 'fucking'))\": False, 'contains(scum)': False, 'contains(good)': False, 'contains(right)': False, 'contains(new)': False, 'contains(really)': False, 'contains(respect)': False}\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "print(tweet_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier1.classify(tweet_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
