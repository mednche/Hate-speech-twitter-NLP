{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a classifier to recognise hate speech on Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/natachachenevoy/Documents/Twitter analysis/Classifier'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/natachachenevoy/Documents/Twitter Analysis/Classifier\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import labeled Twitter datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainingTweets.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 2)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>well yes i mean you started off saying third l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fucking fascist fucking liberal fucking racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>fucking annoying when meat dairy and eggs are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i hate people i was wrong when i said 97.5 of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "305  well yes i mean you started off saying third l...      0\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "307  fucking fascist fucking liberal fucking racist...      1\n",
       "308  fucking annoying when meat dairy and eggs are ...      0\n",
       "309  i hate people i was wrong when i said 97.5 of ...      1"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that each tweet has already been manually labeled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to ''\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    # removing the RT before the @user \n",
    "    tweet = re.sub('rt','',tweet) \n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Convert @username to ''\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    # remove non ASCII characters (emojies)\n",
    "    tweet= re.sub(r'[^\\x00-\\x7F]+','', tweet)\n",
    "    # remove punctuation \n",
    "    tweet = \"\".join(l for l in tweet if l not in string.punctuation)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    # remove beginning and end space\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  apply processing function\n",
    "data['tweet'] = data['tweet'].apply(cleanTweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>well yes i mean you staed off saying third leg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>so my neighbours complained about my shed in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fucking fascist fucking liberal fucking racist...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>fucking annoying when meat dairy and eggs are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i hate people i was wrong when i said 975 of p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "305  well yes i mean you staed off saying third leg...      0\n",
       "306  so my neighbours complained about my shed in t...      1\n",
       "307  fucking fascist fucking liberal fucking racist...      1\n",
       "308  fucking annoying when meat dairy and eggs are ...      0\n",
       "309  i hate people i was wrong when i said 975 of p...      1"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete empty tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the text of emtpy tweets '' by NA and then delete the row\n",
    "data['tweet'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 2)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How many empty tweets were removed in the process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Ensure balance in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Turn data into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 hateful tweets\n",
      "204 non hateful tweets\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pos_tweets = [tuple(x) for x in data[data[\"class\"] == 1].values]\n",
    "random.shuffle(pos_tweets)\n",
    "print(\"{} hateful tweets\".format(len(pos_tweets)))\n",
    "\n",
    "neg_tweets = [tuple(x) for x in data[data[\"class\"] == 0].values]\n",
    "random.shuffle(neg_tweets)\n",
    "print(\"{} non hateful tweets\".format(len(neg_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 hateful tweets\n",
      "204 non hateful tweets\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "pos_tweets = data[data[\"class\"] == 1]\n",
    "pos_tweets = shuffle(pos_tweets)\n",
    "print(\"{} hateful tweets\".format(len(pos_tweets)))\n",
    "\n",
    "neg_tweets = data[data[\"class\"] == 0]\n",
    "neg_tweets = shuffle(neg_tweets)\n",
    "print(\"{} non hateful tweets\".format(len(neg_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>my neighbour just called me a paki some people...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>i hate immigrants</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>why cheat fucking stupid ass prick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>saw some graffiti on the train today it said f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>i hate people i was wrong when i said 975 of p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "47   my neighbour just called me a paki some people...      1\n",
       "284                                  i hate immigrants      1\n",
       "296                 why cheat fucking stupid ass prick      1\n",
       "48   saw some graffiti on the train today it said f...      1\n",
       "309  i hate people i was wrong when i said 975 of p...      1"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>one ok rock</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>my best s this week came from thanksall who we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>win loose or draw i get sent off for just bein...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>rip to all my summer plans that never happened</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>yeah fucking right is abit to much tho bite li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  class\n",
       "95                                         one ok rock      0\n",
       "63   my best s this week came from thanksall who we...      0\n",
       "229  win loose or draw i get sent off for just bein...      0\n",
       "141     rip to all my summer plans that never happened      0\n",
       "233  yeah fucking right is abit to much tho bite li...      0"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Select as many hateful as non-hateful tweets for an equal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in balanced dataset: 202\n"
     ]
    }
   ],
   "source": [
    "num = min(len(pos_tweets), len(neg_tweets))\n",
    "data_balanced = pos_tweets[0:num] + neg_tweets[0:num]\n",
    "#data_balanced = pos_tweets + neg_tweets\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in balanced dataset: 202\n"
     ]
    }
   ],
   "source": [
    "num = min(len(pos_tweets), len(neg_tweets))\n",
    "data_balanced = pos_tweets[0:num].append(neg_tweets[0:num], ignore_index=True)\n",
    "#data_balanced = pos_tweets + neg_tweets\n",
    "print('Number of tweets in balanced dataset: {}'.format(len(data_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my neighbour just called me a paki some people...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i hate immigrants</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why cheat fucking stupid ass prick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saw some graffiti on the train today it said f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hate people i was wrong when i said 975 of p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  my neighbour just called me a paki some people...      1\n",
       "1                                  i hate immigrants      1\n",
       "2                 why cheat fucking stupid ass prick      1\n",
       "3  saw some graffiti on the train today it said f...      1\n",
       "4  i hate people i was wrong when i said 975 of p...      1"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the text of each tweet is a string. We would like to separate each word in that string. In NLP, this is called 'tokenising'.\n",
    "\n",
    "When tokenising, each tweet (intially a string of text) is chopped into a list of tokens. \n",
    "\n",
    "A token is a word or a combination of 2 (bigram) or 3 (trigram) words such as ('back', 'off)' or ('send', 'them', home'). In this example, the ngrams only have a hateful meaning when considered as a group but they are not hateful when taken individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[my, neighbour, just, called, me, a, paki, som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, hate, immigrants]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[why, cheat, fucking, stupid, ass, prick]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[saw, some, graffiti, on, the, train, today, i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, hate, people, i, was, wrong, when, i, said...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  [my, neighbour, just, called, me, a, paki, som...      1\n",
       "1                              [i, hate, immigrants]      1\n",
       "2          [why, cheat, fucking, stupid, ass, prick]      1\n",
       "3  [saw, some, graffiti, on, the, train, today, i...      1\n",
       "4  [i, hate, people, i, was, wrong, when, i, said...      1"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_balanced_tokenised = data_balanced.copy()\n",
    "\n",
    "data_balanced_tokenised['tweet'] = data_balanced['tweet'].apply(nltk.word_tokenize)\n",
    "\n",
    "data_balanced_tokenised.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can see that there are many words in the tweets that don't bring any meaning such as 'it', 'i' etc. These are called stopwords and need to be removed so that the classifier can focus on words that matter when telling the difference between hate and non-hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Import English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'd',\n",
       " 'y',\n",
       " 'there',\n",
       " 'did',\n",
       " 'here',\n",
       " \"isn't\",\n",
       " 'above',\n",
       " 'on',\n",
       " 'does',\n",
       " 'up',\n",
       " 'with',\n",
       " 'my',\n",
       " \"wasn't\",\n",
       " 'has',\n",
       " 'those',\n",
       " 'should',\n",
       " \"you'll\",\n",
       " 'into',\n",
       " 'against',\n",
       " 'don',\n",
       " 'didn',\n",
       " \"that'll\",\n",
       " 'who',\n",
       " 'through',\n",
       " 'their',\n",
       " \"shouldn't\",\n",
       " 'doing',\n",
       " 'wouldn',\n",
       " 'doesn',\n",
       " 'our',\n",
       " 'm',\n",
       " 'ourselves',\n",
       " 'itself',\n",
       " 'needn',\n",
       " 'between',\n",
       " 'o',\n",
       " 'was',\n",
       " 'not',\n",
       " \"weren't\",\n",
       " 'had',\n",
       " 'too',\n",
       " \"she's\",\n",
       " 'no',\n",
       " 'down',\n",
       " 'she',\n",
       " 'nor',\n",
       " 'his',\n",
       " 'won',\n",
       " 'ma',\n",
       " 'shan',\n",
       " \"mustn't\",\n",
       " 'which',\n",
       " 'any',\n",
       " 'were',\n",
       " 'to',\n",
       " 'your',\n",
       " \"couldn't\",\n",
       " 'a',\n",
       " 'aren',\n",
       " 'for',\n",
       " 'be',\n",
       " 'theirs',\n",
       " 'just',\n",
       " 'each',\n",
       " 'whom',\n",
       " 'at',\n",
       " 'until',\n",
       " 'own',\n",
       " 'herself',\n",
       " \"hasn't\",\n",
       " 'by',\n",
       " \"haven't\",\n",
       " \"wouldn't\",\n",
       " \"didn't\",\n",
       " 'her',\n",
       " 'they',\n",
       " \"you're\",\n",
       " 'what',\n",
       " 'being',\n",
       " 'me',\n",
       " 'him',\n",
       " 'some',\n",
       " 'hadn',\n",
       " 'i',\n",
       " 'few',\n",
       " 'is',\n",
       " 'hers',\n",
       " 'other',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'under',\n",
       " 'below',\n",
       " 'its',\n",
       " 'shouldn',\n",
       " 'll',\n",
       " 'as',\n",
       " 'an',\n",
       " 'more',\n",
       " \"you've\",\n",
       " 'once',\n",
       " 'myself',\n",
       " 'it',\n",
       " 'very',\n",
       " 'having',\n",
       " 'that',\n",
       " 'can',\n",
       " 'why',\n",
       " 'both',\n",
       " 'isn',\n",
       " 'or',\n",
       " 'ain',\n",
       " 'ours',\n",
       " 'these',\n",
       " 'over',\n",
       " 'where',\n",
       " \"mightn't\",\n",
       " 'in',\n",
       " 's',\n",
       " 'but',\n",
       " 'this',\n",
       " \"shan't\",\n",
       " 're',\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " 'than',\n",
       " 'been',\n",
       " \"you'd\",\n",
       " 'he',\n",
       " 'before',\n",
       " 'we',\n",
       " 'couldn',\n",
       " \"doesn't\",\n",
       " 'am',\n",
       " 'further',\n",
       " 'mustn',\n",
       " 'after',\n",
       " 'all',\n",
       " 'so',\n",
       " \"it's\",\n",
       " 'when',\n",
       " 'then',\n",
       " 'only',\n",
       " 'will',\n",
       " 'have',\n",
       " 'himself',\n",
       " 'are',\n",
       " 'haven',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " \"aren't\",\n",
       " 'about',\n",
       " 'again',\n",
       " 'how',\n",
       " \"won't\",\n",
       " 'mightn',\n",
       " 'most',\n",
       " 'do',\n",
       " 't',\n",
       " \"needn't\",\n",
       " 'while',\n",
       " 'hasn',\n",
       " 'yours',\n",
       " 'during',\n",
       " 'weren',\n",
       " 'wasn',\n",
       " 'if',\n",
       " 'yourselves',\n",
       " 've',\n",
       " 'such',\n",
       " 'youre',\n",
       " 'r',\n",
       " \"you're\",\n",
       " 'us',\n",
       " 'doesnt',\n",
       " 'im',\n",
       " 'hes',\n",
       " 'u',\n",
       " 'ya',\n",
       " 'ww',\n",
       " 'dont',\n",
       " 'https',\n",
       " 'aint',\n",
       " 'theres',\n",
       " 'shouldnt',\n",
       " 'thats',\n",
       " 'amp',\n",
       " 'wudnt',\n",
       " 'gonna',\n",
       " 'ur',\n",
       " 'cant']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "#remove some words from the list:\n",
    "item_to_delete = ['you', 'out', 'off', 'them', 'themselves', 'yourself', 'from', 'same']\n",
    "stopWords = [e for e in stops if e not in item_to_delete]\n",
    "item_to_add = [\"youre\", \"r\", \"you're\", \"us\", \"doesnt\", \"im\", \"hes\", \"u\", \"ya\", \"ww\", \"dont\", \"https\", \"aint\", \"theres\", \"shouldnt\", \"thats\", \"amp\", \"wudnt\", \"gonna\", \"ur\", \"cant\"]\n",
    "for e in item_to_add:\n",
    "    stopWords.append(e)\n",
    "\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neighbour, called, paki, people, respect]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hate, immigrants]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[cheat, fucking, stupid, ass, prick]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[saw, graffiti, train, today, said, fuck, off,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hate, people, wrong, said, 975, people, cunts...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0         [neighbour, called, paki, people, respect]      1\n",
       "1                                 [hate, immigrants]      1\n",
       "2               [cheat, fucking, stupid, ass, prick]      1\n",
       "3  [saw, graffiti, train, today, said, fuck, off,...      1\n",
       "4  [hate, people, wrong, said, 975, people, cunts...      1"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data = data_balanced_tokenised.copy()\n",
    "\n",
    "pre_processed_data['tweet'] = data_balanced_tokenised['tweet'].apply(lambda x: [item for item in x if item not in stopWords])\n",
    "\n",
    "pre_processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: At this point, we have a dataset that has been cleaned and tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the classfier won't be able to know which key words are responsible for a tweet being labelled 'hateful'. Is it because of the word 'everywhere' or the word 'fuck'?\n",
    "\n",
    "To be able to learn what counts as hateful and what doesn't, the classifier also needs to know the 'hateful value' of each word (or combination of word) in a tweet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text themselves cannot be used by machine learning models. They expect their input to be numeric. So we need some way that can transform input text into numeric feature in a meaningful way. There are several approaches for this and we’ll briefly go through some of them.\n",
    "\n",
    "In Part 2., we'll see how to convert the words into features so that we can feed it to a classifier for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare the data to train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the text into any category, we need to define some criteria. On the basis of those criteria, our classifier will learn that a particular kind of text falls in a particular category. This kind of criteria is known as feature. We can define one or more feature to train our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way we can convert text to numeric feature is via binary encoding. In this scheme, we create a vocabulary by looking at each distinct word in the whole dataset (corpus). For each document, the output of this scheme will be a vector of size N where N is the total number of words in our vocabulary. Initially all entries in the vector will be 0. If the word in the given document exists in the vocabulary then vector element at that position is set to 1. Let’s implement this to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a vocabulary: a set of unique words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1266\n"
     ]
    }
   ],
   "source": [
    "vocab = [word for tweet in pre_processed_data['tweet'] for word in tweet]\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbour',\n",
       " 'called',\n",
       " 'paki',\n",
       " 'people',\n",
       " 'respect',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'cheat',\n",
       " 'fucking',\n",
       " 'stupid',\n",
       " 'ass',\n",
       " 'prick',\n",
       " 'saw',\n",
       " 'graffiti',\n",
       " 'train',\n",
       " 'today',\n",
       " 'said',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'black',\n",
       " 'slagfound',\n",
       " 'pretty',\n",
       " 'funny',\n",
       " 'hate',\n",
       " 'people',\n",
       " 'wrong',\n",
       " 'said',\n",
       " '975',\n",
       " 'people',\n",
       " 'cunts',\n",
       " 'much',\n",
       " 'much',\n",
       " 'higher',\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " 'same',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'wearing',\n",
       " 'veil',\n",
       " 'fucking',\n",
       " 'fascist',\n",
       " 'fucking',\n",
       " 'liberal',\n",
       " 'fucking',\n",
       " 'racist',\n",
       " 'fucking',\n",
       " 'traitor',\n",
       " 'fucking',\n",
       " 'petrolhead',\n",
       " 'fucking',\n",
       " 'treehugger',\n",
       " 'steve',\n",
       " 'you',\n",
       " 'steph',\n",
       " 'mate',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " 'man',\n",
       " 'called',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'back',\n",
       " 'country',\n",
       " 'rude',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'along',\n",
       " 'europe',\n",
       " 'many',\n",
       " 'chinks',\n",
       " 'london',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'back',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'like',\n",
       " 'you',\n",
       " 'shut',\n",
       " 'nigger',\n",
       " 'whore',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'raped',\n",
       " 'one',\n",
       " 'animals',\n",
       " 'might',\n",
       " 'change',\n",
       " 'tune',\n",
       " 'black',\n",
       " 'bitch',\n",
       " 'next',\n",
       " 'shut',\n",
       " 'fucking',\n",
       " 'chinks',\n",
       " 'coloured',\n",
       " 'fuckers',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'brexit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " 'black',\n",
       " 'nigger',\n",
       " 'least',\n",
       " 'nigger',\n",
       " 'lmfao',\n",
       " 'neighbours',\n",
       " 'complained',\n",
       " 'shed',\n",
       " 'front',\n",
       " 'garden',\n",
       " 'whos',\n",
       " 'neighbour',\n",
       " 'church',\n",
       " 'england',\n",
       " 'bunch',\n",
       " 'god',\n",
       " 'bothering',\n",
       " 'cunts',\n",
       " 'bunch',\n",
       " 'paki',\n",
       " 'perves',\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'allah',\n",
       " 'lovers',\n",
       " 'fucking',\n",
       " 'immigrants',\n",
       " 'fucking',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " 'coming',\n",
       " 'taking',\n",
       " 'jobs',\n",
       " 'unacceptable',\n",
       " 'local',\n",
       " 'mosque',\n",
       " 'let',\n",
       " 'terrorists',\n",
       " 'out',\n",
       " 'racist',\n",
       " 'sexist',\n",
       " 'perv',\n",
       " 'clearly',\n",
       " 'wants',\n",
       " 'fuck',\n",
       " 'daughter',\n",
       " 'disgusting',\n",
       " 'pakis',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'please',\n",
       " 'hate',\n",
       " 'niggers',\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'hate',\n",
       " 'spicskkk',\n",
       " 'rally',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunts',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'myriah',\n",
       " 'chavcentral',\n",
       " 'chav',\n",
       " 'central',\n",
       " 'diy',\n",
       " 'terrorist',\n",
       " 'religion',\n",
       " 'fucking',\n",
       " 'joke',\n",
       " 'you',\n",
       " 'go',\n",
       " 'around',\n",
       " 'screaming',\n",
       " 'allah',\n",
       " 'akbar',\n",
       " 'terrorist',\n",
       " 'shit',\n",
       " 'diy',\n",
       " 'faggot',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'poland',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'scum',\n",
       " 'shut',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'sand',\n",
       " 'monkeys',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'irish',\n",
       " 'cow',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'you',\n",
       " 'muslim',\n",
       " 'prick',\n",
       " 'many',\n",
       " 'wogs',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'brexit',\n",
       " 'least',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jefree',\n",
       " 'starr',\n",
       " 'faggot',\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " 'benjamin',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " 'good',\n",
       " 'night',\n",
       " 'fags',\n",
       " 'fagettes',\n",
       " 'female',\n",
       " 'version',\n",
       " 'fags',\n",
       " 'like',\n",
       " 'fagettes',\n",
       " 'make',\n",
       " 'female',\n",
       " 'hate',\n",
       " 'you',\n",
       " 'thick',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'bastard',\n",
       " 'wog',\n",
       " 'road',\n",
       " 'annoying',\n",
       " 'polish',\n",
       " 'vermin',\n",
       " 'diy',\n",
       " 'paki',\n",
       " 'used',\n",
       " 'tie',\n",
       " 'ends',\n",
       " 'niggers',\n",
       " 'legs',\n",
       " '2',\n",
       " 'different',\n",
       " 'horses',\n",
       " 'beat',\n",
       " 'horses',\n",
       " 'spread',\n",
       " 'legs',\n",
       " 'break',\n",
       " 'smh',\n",
       " 'polish',\n",
       " 'bitch',\n",
       " 'last',\n",
       " 'time',\n",
       " 'preston',\n",
       " 'anymore',\n",
       " 'inconsiderate',\n",
       " 'twats',\n",
       " 'm6m61',\n",
       " 'way',\n",
       " 'home',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'cut',\n",
       " 'bitch',\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'fuck',\n",
       " 'anyone',\n",
       " 'british',\n",
       " 'you',\n",
       " 'even',\n",
       " 'country',\n",
       " 'you',\n",
       " 'pakis',\n",
       " 'fucking',\n",
       " 'white',\n",
       " 'bitch',\n",
       " 'road',\n",
       " 'needs',\n",
       " 'out',\n",
       " 'get',\n",
       " 'out',\n",
       " 'country',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'foreigners',\n",
       " 'enough',\n",
       " 'fucking',\n",
       " 'pakis',\n",
       " 'man',\n",
       " 'time',\n",
       " 'leave',\n",
       " 'europe',\n",
       " 'kill',\n",
       " 'you',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " 'you',\n",
       " 'ass',\n",
       " 'prick',\n",
       " 'smelly',\n",
       " 'pakis',\n",
       " 'niggers',\n",
       " 'road',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'shouting',\n",
       " 'go',\n",
       " 'round',\n",
       " 'shut',\n",
       " 'them',\n",
       " 'fucking',\n",
       " 'bitch',\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'dogs',\n",
       " 'fuck',\n",
       " 'dykes',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'fuck',\n",
       " 'jezanie',\n",
       " 'cunts',\n",
       " 'cbb',\n",
       " 'fat',\n",
       " 'little',\n",
       " 'wanker',\n",
       " 'you',\n",
       " 'problem',\n",
       " 'say',\n",
       " 'face',\n",
       " 'behind',\n",
       " 'back',\n",
       " 'faggot',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'immigrants',\n",
       " 'brexit',\n",
       " 'fuck',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'sheboon',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'strung',\n",
       " 'like',\n",
       " 'niggers',\n",
       " 'whitepower',\n",
       " '1488',\n",
       " 'bitch',\n",
       " 'immigrants',\n",
       " 'fucking',\n",
       " 'terrorists',\n",
       " 'come',\n",
       " 'out',\n",
       " 'you',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'cunt',\n",
       " 'go',\n",
       " 'home',\n",
       " 'you',\n",
       " 'immigrant',\n",
       " 'fucking',\n",
       " 'pakis',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " 'sitting',\n",
       " 'cinema',\n",
       " 'next',\n",
       " 'go',\n",
       " 'burn',\n",
       " 'hell',\n",
       " 'bitch',\n",
       " 'sly',\n",
       " 'bitch',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'europe',\n",
       " 'you',\n",
       " 'job',\n",
       " 'steal',\n",
       " 'immigrants',\n",
       " 'polish',\n",
       " 'bastard',\n",
       " 'depo',\n",
       " 'bastard',\n",
       " 'immigrants',\n",
       " 'keep',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'fucking',\n",
       " 'niggers',\n",
       " 'fucking',\n",
       " 'terrorists',\n",
       " 'everywhere',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'bastard',\n",
       " 'muslims',\n",
       " 'attention',\n",
       " 'seeking',\n",
       " 'bitch',\n",
       " 'muslim',\n",
       " 'scum',\n",
       " 'terrorists',\n",
       " 'local',\n",
       " 'paki',\n",
       " 'shop',\n",
       " 'disgusting',\n",
       " 'even',\n",
       " 'allowed',\n",
       " 'country',\n",
       " 'leave',\n",
       " 'fucking',\n",
       " 'country',\n",
       " 'you',\n",
       " 'white',\n",
       " 'hate',\n",
       " 'bitch',\n",
       " 'niggers',\n",
       " 'everywhere',\n",
       " 'days',\n",
       " 'make',\n",
       " 'britain',\n",
       " 'great',\n",
       " 'hate',\n",
       " 'yellow',\n",
       " 'fuckers',\n",
       " 'tired',\n",
       " 'polish',\n",
       " 'bitches',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'you',\n",
       " 'niggers',\n",
       " 'bruh',\n",
       " 'polish',\n",
       " 'scum',\n",
       " 'muslims',\n",
       " 'go',\n",
       " 'fucking',\n",
       " 'home',\n",
       " 'bye',\n",
       " 'bye',\n",
       " 'brexit',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'foreign',\n",
       " 'bastards',\n",
       " 'one',\n",
       " 'ok',\n",
       " 'rock',\n",
       " 'best',\n",
       " 'week',\n",
       " 'came',\n",
       " 'from',\n",
       " 'thanksall',\n",
       " 'win',\n",
       " 'loose',\n",
       " 'draw',\n",
       " 'get',\n",
       " 'sent',\n",
       " 'off',\n",
       " 'absolute',\n",
       " 'knobhead',\n",
       " 'rip',\n",
       " 'summer',\n",
       " 'plans',\n",
       " 'never',\n",
       " 'happened',\n",
       " 'yeah',\n",
       " 'fucking',\n",
       " 'right',\n",
       " 'abit',\n",
       " 'much',\n",
       " 'tho',\n",
       " 'bite',\n",
       " 'like',\n",
       " 'fuck',\n",
       " 'everything',\n",
       " 'sho',\n",
       " 'outreach',\n",
       " 'emails',\n",
       " 'get',\n",
       " 'you',\n",
       " 'links',\n",
       " 'school',\n",
       " 'stas',\n",
       " 'lighter',\n",
       " 'note',\n",
       " 'sky',\n",
       " 'pundits',\n",
       " 'get',\n",
       " 'clothing',\n",
       " 'budget',\n",
       " 'from',\n",
       " 'asda',\n",
       " 'dodgysuits',\n",
       " 'tess2016',\n",
       " 'you',\n",
       " 'need',\n",
       " 'prove',\n",
       " 'consistency',\n",
       " 'time',\n",
       " 'like',\n",
       " 'realize',\n",
       " 'immigrant',\n",
       " 'fact',\n",
       " 'body',\n",
       " '10',\n",
       " 'make',\n",
       " 'okay',\n",
       " 'justwondering',\n",
       " 'let',\n",
       " 'world',\n",
       " 'change',\n",
       " 'smile',\n",
       " 'let',\n",
       " 'smile',\n",
       " 'change',\n",
       " 'world',\n",
       " 'heaoverhate',\n",
       " 'kenchan',\n",
       " 'onbsc2016',\n",
       " 'david',\n",
       " 'icke',\n",
       " 'political',\n",
       " 'correctness',\n",
       " 'prison',\n",
       " 'mind',\n",
       " 'bomb',\n",
       " 'holy',\n",
       " 'fuck',\n",
       " 'counting',\n",
       " 'till',\n",
       " 'bryans',\n",
       " 'new',\n",
       " 'ep',\n",
       " '39',\n",
       " 'days',\n",
       " 'go',\n",
       " 'go',\n",
       " 'preorder',\n",
       " 'proud',\n",
       " 'watch',\n",
       " 'season',\n",
       " 'two',\n",
       " 'internet',\n",
       " 'decided',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'ive',\n",
       " 'heard',\n",
       " 'brexit',\n",
       " 'debate',\n",
       " 'american',\n",
       " 'made',\n",
       " 'stats',\n",
       " 'mud',\n",
       " 'slinging',\n",
       " 'fed',\n",
       " 'hate',\n",
       " 'division',\n",
       " 'really',\n",
       " 'gerald',\n",
       " 'life',\n",
       " 'offering',\n",
       " 'oppounity',\n",
       " 'pay',\n",
       " 'you',\n",
       " '50',\n",
       " 'get',\n",
       " 'back',\n",
       " 'feature',\n",
       " 'lost',\n",
       " 'americans',\n",
       " 'giving',\n",
       " 'citizenship',\n",
       " 'time',\n",
       " 'high',\n",
       " 'immigrants',\n",
       " 'applying',\n",
       " 'mostly',\n",
       " 'latinos',\n",
       " 'off',\n",
       " 'outside',\n",
       " 'leave',\n",
       " 'food',\n",
       " 'rudolph',\n",
       " 'thankyou',\n",
       " '10',\n",
       " 'off',\n",
       " 'dog',\n",
       " 'collars',\n",
       " 'leads',\n",
       " 'toys',\n",
       " 'weekend',\n",
       " 'pleasant',\n",
       " 'pets',\n",
       " 'dickson',\n",
       " 'road',\n",
       " 'blackpool',\n",
       " 'breaktime',\n",
       " 'crazy',\n",
       " 'car',\n",
       " 'crash',\n",
       " 'compilation',\n",
       " 'pa',\n",
       " '321',\n",
       " 'updated',\n",
       " 'database',\n",
       " 'respect',\n",
       " 'uncouple',\n",
       " 'records',\n",
       " 'online',\n",
       " 'emxzf',\n",
       " 'earning',\n",
       " 'mplusrewards',\n",
       " 'pick',\n",
       " 'lines',\n",
       " 'cooling',\n",
       " 'off',\n",
       " 'amazing',\n",
       " 'product',\n",
       " 'from',\n",
       " 'leave',\n",
       " 'conditioner',\n",
       " 'yet',\n",
       " 'company',\n",
       " 'still',\n",
       " 'hasnt',\n",
       " 'kicked',\n",
       " 'off',\n",
       " 'trythesewrestlingfanpodcasts',\n",
       " 'awwir',\n",
       " 'rasslekast',\n",
       " 'eu',\n",
       " 'sat',\n",
       " 'oh',\n",
       " 'god',\n",
       " 'board',\n",
       " 'thankfully',\n",
       " 'got',\n",
       " 'given',\n",
       " 'bonus',\n",
       " 'day',\n",
       " 'off',\n",
       " 'way',\n",
       " 'leave',\n",
       " 'booked',\n",
       " 'worked',\n",
       " 'out',\n",
       " 'well',\n",
       " 'write',\n",
       " 'margate',\n",
       " 'aug',\n",
       " '56',\n",
       " 'sponsored',\n",
       " 'ugly',\n",
       " 'but1',\n",
       " 'text',\n",
       " 'back',\n",
       " '3sec2',\n",
       " 'funny',\n",
       " '3',\n",
       " 'ill',\n",
       " 'buy',\n",
       " 'you',\n",
       " 'food',\n",
       " 'feeling',\n",
       " 'many',\n",
       " 'today',\n",
       " 'set',\n",
       " 'off',\n",
       " 'anni',\n",
       " 'never',\n",
       " 'sleep',\n",
       " 'iv',\n",
       " 'got',\n",
       " 'get',\n",
       " 'dead',\n",
       " 'early',\n",
       " '24',\n",
       " 'ways',\n",
       " 'get',\n",
       " 'clicks',\n",
       " 'callstoaction',\n",
       " 'seminary',\n",
       " 'bored',\n",
       " 'out',\n",
       " 'fucking',\n",
       " 'mind',\n",
       " 'help',\n",
       " 'go',\n",
       " 'home',\n",
       " 'oreo',\n",
       " 'drunk',\n",
       " 'yall',\n",
       " 'proud',\n",
       " 'baekhyun',\n",
       " 'working',\n",
       " 'abs',\n",
       " 'gotten',\n",
       " 'really',\n",
       " 'skinny',\n",
       " 'morning',\n",
       " 'alarm',\n",
       " 'went',\n",
       " 'off',\n",
       " '0230',\n",
       " 'didnt',\n",
       " 'know',\n",
       " 'fuck',\n",
       " 'going',\n",
       " 'x',\n",
       " 'fuckin',\n",
       " 'get',\n",
       " 'you',\n",
       " 'never',\n",
       " 'tell',\n",
       " 'happy',\n",
       " 'bihday',\n",
       " 'reminder',\n",
       " 'prospects',\n",
       " 'arent',\n",
       " 'woh',\n",
       " 'nearly',\n",
       " 'much',\n",
       " 'you',\n",
       " 'think',\n",
       " 'iphones',\n",
       " 'need',\n",
       " 'feature',\n",
       " 'incoming',\n",
       " 'call',\n",
       " 'take',\n",
       " 'whole',\n",
       " 'screen',\n",
       " 'things',\n",
       " 'ignore',\n",
       " 'phon',\n",
       " 'david',\n",
       " 'duke',\n",
       " 'wouldnt',\n",
       " 'republicans',\n",
       " 'vote',\n",
       " 'new',\n",
       " 'reason',\n",
       " 'tlot',\n",
       " 'amagi',\n",
       " 'tcot',\n",
       " 'saw',\n",
       " 'crow',\n",
       " 'gallop',\n",
       " 'across',\n",
       " 'road',\n",
       " 'mmkay',\n",
       " 'wut',\n",
       " '128514',\n",
       " 'rev',\n",
       " 'william',\n",
       " 'barber',\n",
       " 'dropped',\n",
       " 'mic',\n",
       " 'nods',\n",
       " 'confirming',\n",
       " 'boy',\n",
       " 'true',\n",
       " 'really',\n",
       " 'asked',\n",
       " 'eyes',\n",
       " 'twinkling',\n",
       " 'said',\n",
       " 'would',\n",
       " 'accompany',\n",
       " 'excited',\n",
       " 'traveling',\n",
       " 'back',\n",
       " 'home',\n",
       " 'daysprobably',\n",
       " 'wont',\n",
       " 'look',\n",
       " 'like',\n",
       " 'though',\n",
       " 'cake',\n",
       " 'sale',\n",
       " 'kicked',\n",
       " 'off',\n",
       " 'many',\n",
       " 'delicious',\n",
       " 'treats',\n",
       " 'hello',\n",
       " 'goodonight',\n",
       " 'everyone',\n",
       " 'special',\n",
       " 'number',\n",
       " '1',\n",
       " 'station',\n",
       " 'tv',\n",
       " 'indonesia',\n",
       " 'please',\n",
       " 'invite',\n",
       " 'come',\n",
       " 'off',\n",
       " 'you',\n",
       " 'go',\n",
       " 'christmas',\n",
       " 'google',\n",
       " 'adsense',\n",
       " 'mobile',\n",
       " 'text',\n",
       " 'ads',\n",
       " 'get',\n",
       " 'new',\n",
       " 'look',\n",
       " 'a4b',\n",
       " '36',\n",
       " 'hoursdishoomincinemas',\n",
       " 'health',\n",
       " 'family',\n",
       " 'breastfeeding',\n",
       " 'premature',\n",
       " 'babies',\n",
       " 'boosts',\n",
       " 'iq',\n",
       " 'later',\n",
       " 'life',\n",
       " 'research',\n",
       " 'finds',\n",
       " 'scooby',\n",
       " 'doo',\n",
       " 'dog',\n",
       " 'bitch',\n",
       " 'you',\n",
       " 'fan',\n",
       " 'even',\n",
       " 'like',\n",
       " 'respect',\n",
       " 'family',\n",
       " 'prayforhayes',\n",
       " 'maniq',\n",
       " 'gel',\n",
       " 'polish',\n",
       " 'natural',\n",
       " 'nails',\n",
       " 'violet',\n",
       " '102',\n",
       " '10',\n",
       " 'off',\n",
       " 'nail',\n",
       " 'fresh',\n",
       " 'out',\n",
       " 'pack',\n",
       " 'bedding',\n",
       " 'fuck',\n",
       " 'yeah',\n",
       " 'listen',\n",
       " 'eloq',\n",
       " 'qrion',\n",
       " 'beach',\n",
       " '20',\n",
       " 'moving',\n",
       " 'castle',\n",
       " 'np',\n",
       " 'soundcloudnowplaying',\n",
       " 'people',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'level1',\n",
       " 'level',\n",
       " 'one',\n",
       " 'darwen',\n",
       " 'refer',\n",
       " 'source',\n",
       " 'you',\n",
       " 'crop',\n",
       " 'pass',\n",
       " 'off',\n",
       " 'hate',\n",
       " 'behaviour',\n",
       " 'hate',\n",
       " 'wake',\n",
       " 'early',\n",
       " 'morning',\n",
       " 'rohith',\n",
       " 'vemula',\n",
       " 'awakened',\n",
       " 'let',\n",
       " 'sacrifice',\n",
       " 'go',\n",
       " 'vain',\n",
       " 'dalitsnotcows',\n",
       " 'muslimsnotcows',\n",
       " 'dalitmusl',\n",
       " 'you',\n",
       " 'love',\n",
       " 'someone',\n",
       " 'tell',\n",
       " 'them',\n",
       " 'thing',\n",
       " 'right',\n",
       " 'time',\n",
       " 'right',\n",
       " 'place',\n",
       " 'long',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'boxing',\n",
       " 'tonight',\n",
       " 'great',\n",
       " 'fight',\n",
       " 'joshuawhyte',\n",
       " 'lets',\n",
       " 'get',\n",
       " 'ready',\n",
       " 'rumble',\n",
       " 'joshua',\n",
       " 'forget',\n",
       " 'robot',\n",
       " 'aiming',\n",
       " 'message',\n",
       " 'wafc',\n",
       " 'fans',\n",
       " 'happy',\n",
       " 'another',\n",
       " 'football',\n",
       " 'club',\n",
       " 'may',\n",
       " 'go',\n",
       " 'ie',\n",
       " 'bolton',\n",
       " 'sho',\n",
       " 'outreach',\n",
       " 'emails',\n",
       " 'get',\n",
       " 'you',\n",
       " 'links',\n",
       " 'kondobyjaymoni',\n",
       " '20',\n",
       " 'brazils',\n",
       " 'beautiful',\n",
       " ...]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natachachenevoy/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_vocabulary(tweets):\n",
    "    all_words = []\n",
    "    for word_list in tweets:\n",
    "        # unigrams\n",
    "        all_words.extend(word_list)\n",
    "        \n",
    "        # bigrams\n",
    "        bigrams = list(ngrams(word_list, 2))\n",
    "        \n",
    "        #trigrams \n",
    "        trigrams = list(ngrams(word_list, 3))\n",
    "        \n",
    "        all_words.extend(bigrams)\n",
    "        all_words.extend(trigrams)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "vocab = get_vocabulary(pre_processed_data['tweet'])\n",
    "print('Vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbour',\n",
       " 'called',\n",
       " 'paki',\n",
       " 'people',\n",
       " 'respect',\n",
       " ('neighbour', 'called'),\n",
       " ('called', 'paki'),\n",
       " ('paki', 'people'),\n",
       " ('people', 'respect'),\n",
       " ('neighbour', 'called', 'paki'),\n",
       " ('called', 'paki', 'people'),\n",
       " ('paki', 'people', 'respect'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'cheat',\n",
       " 'fucking',\n",
       " 'stupid',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('cheat', 'fucking'),\n",
       " ('fucking', 'stupid'),\n",
       " ('stupid', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('cheat', 'fucking', 'stupid'),\n",
       " ('fucking', 'stupid', 'ass'),\n",
       " ('stupid', 'ass', 'prick'),\n",
       " 'saw',\n",
       " 'graffiti',\n",
       " 'train',\n",
       " 'today',\n",
       " 'said',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'home',\n",
       " 'you',\n",
       " 'black',\n",
       " 'slagfound',\n",
       " 'pretty',\n",
       " 'funny',\n",
       " ('saw', 'graffiti'),\n",
       " ('graffiti', 'train'),\n",
       " ('train', 'today'),\n",
       " ('today', 'said'),\n",
       " ('said', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'home'),\n",
       " ('home', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'slagfound'),\n",
       " ('slagfound', 'pretty'),\n",
       " ('pretty', 'funny'),\n",
       " ('saw', 'graffiti', 'train'),\n",
       " ('graffiti', 'train', 'today'),\n",
       " ('train', 'today', 'said'),\n",
       " ('today', 'said', 'fuck'),\n",
       " ('said', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'home'),\n",
       " ('off', 'home', 'you'),\n",
       " ('home', 'you', 'black'),\n",
       " ('you', 'black', 'slagfound'),\n",
       " ('black', 'slagfound', 'pretty'),\n",
       " ('slagfound', 'pretty', 'funny'),\n",
       " 'hate',\n",
       " 'people',\n",
       " 'wrong',\n",
       " 'said',\n",
       " '975',\n",
       " 'people',\n",
       " 'cunts',\n",
       " 'much',\n",
       " 'much',\n",
       " 'higher',\n",
       " ('hate', 'people'),\n",
       " ('people', 'wrong'),\n",
       " ('wrong', 'said'),\n",
       " ('said', '975'),\n",
       " ('975', 'people'),\n",
       " ('people', 'cunts'),\n",
       " ('cunts', 'much'),\n",
       " ('much', 'much'),\n",
       " ('much', 'higher'),\n",
       " ('hate', 'people', 'wrong'),\n",
       " ('people', 'wrong', 'said'),\n",
       " ('wrong', 'said', '975'),\n",
       " ('said', '975', 'people'),\n",
       " ('975', 'people', 'cunts'),\n",
       " ('people', 'cunts', 'much'),\n",
       " ('cunts', 'much', 'much'),\n",
       " ('much', 'much', 'higher'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'terrorists'),\n",
       " ('bastard', 'muslim', 'terrorists'),\n",
       " 'muslim',\n",
       " 'terrorists',\n",
       " 'same',\n",
       " ('muslim', 'terrorists'),\n",
       " ('terrorists', 'same'),\n",
       " ('muslim', 'terrorists', 'same'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " 'wearing',\n",
       " 'veil',\n",
       " ('paki', 'cunt'),\n",
       " ('cunt', 'wearing'),\n",
       " ('wearing', 'veil'),\n",
       " ('paki', 'cunt', 'wearing'),\n",
       " ('cunt', 'wearing', 'veil'),\n",
       " 'fucking',\n",
       " 'fascist',\n",
       " 'fucking',\n",
       " 'liberal',\n",
       " 'fucking',\n",
       " 'racist',\n",
       " 'fucking',\n",
       " 'traitor',\n",
       " 'fucking',\n",
       " 'petrolhead',\n",
       " 'fucking',\n",
       " 'treehugger',\n",
       " 'steve',\n",
       " 'you',\n",
       " 'steph',\n",
       " 'mate',\n",
       " ('fucking', 'fascist'),\n",
       " ('fascist', 'fucking'),\n",
       " ('fucking', 'liberal'),\n",
       " ('liberal', 'fucking'),\n",
       " ('fucking', 'racist'),\n",
       " ('racist', 'fucking'),\n",
       " ('fucking', 'traitor'),\n",
       " ('traitor', 'fucking'),\n",
       " ('fucking', 'petrolhead'),\n",
       " ('petrolhead', 'fucking'),\n",
       " ('fucking', 'treehugger'),\n",
       " ('treehugger', 'steve'),\n",
       " ('steve', 'you'),\n",
       " ('you', 'steph'),\n",
       " ('steph', 'mate'),\n",
       " ('fucking', 'fascist', 'fucking'),\n",
       " ('fascist', 'fucking', 'liberal'),\n",
       " ('fucking', 'liberal', 'fucking'),\n",
       " ('liberal', 'fucking', 'racist'),\n",
       " ('fucking', 'racist', 'fucking'),\n",
       " ('racist', 'fucking', 'traitor'),\n",
       " ('fucking', 'traitor', 'fucking'),\n",
       " ('traitor', 'fucking', 'petrolhead'),\n",
       " ('fucking', 'petrolhead', 'fucking'),\n",
       " ('petrolhead', 'fucking', 'treehugger'),\n",
       " ('fucking', 'treehugger', 'steve'),\n",
       " ('treehugger', 'steve', 'you'),\n",
       " ('steve', 'you', 'steph'),\n",
       " ('you', 'steph', 'mate'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunt'),\n",
       " 'man',\n",
       " 'called',\n",
       " 'black',\n",
       " 'bastard',\n",
       " 'go',\n",
       " 'back',\n",
       " 'country',\n",
       " 'rude',\n",
       " ('man', 'called'),\n",
       " ('called', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('bastard', 'go'),\n",
       " ('go', 'back'),\n",
       " ('back', 'country'),\n",
       " ('country', 'rude'),\n",
       " ('man', 'called', 'black'),\n",
       " ('called', 'black', 'bastard'),\n",
       " ('black', 'bastard', 'go'),\n",
       " ('bastard', 'go', 'back'),\n",
       " ('go', 'back', 'country'),\n",
       " ('back', 'country', 'rude'),\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " 'get',\n",
       " 'rid',\n",
       " 'them',\n",
       " 'along',\n",
       " 'europe',\n",
       " ('scottish', 'bastard'),\n",
       " ('bastard', 'get'),\n",
       " ('get', 'rid'),\n",
       " ('rid', 'them'),\n",
       " ('them', 'along'),\n",
       " ('along', 'europe'),\n",
       " ('scottish', 'bastard', 'get'),\n",
       " ('bastard', 'get', 'rid'),\n",
       " ('get', 'rid', 'them'),\n",
       " ('rid', 'them', 'along'),\n",
       " ('them', 'along', 'europe'),\n",
       " 'many',\n",
       " 'chinks',\n",
       " 'london',\n",
       " ('many', 'chinks'),\n",
       " ('chinks', 'london'),\n",
       " ('many', 'chinks', 'london'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'back',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'back'),\n",
       " ('back', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'back'),\n",
       " ('you', 'back', 'paki'),\n",
       " ('back', 'paki', 'bastard'),\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'like',\n",
       " 'you',\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'like'),\n",
       " ('like', 'you'),\n",
       " ('hate', 'faggots', 'like'),\n",
       " ('faggots', 'like', 'you'),\n",
       " 'shut',\n",
       " 'nigger',\n",
       " 'whore',\n",
       " 'hope',\n",
       " 'get',\n",
       " 'raped',\n",
       " 'one',\n",
       " 'animals',\n",
       " 'might',\n",
       " 'change',\n",
       " 'tune',\n",
       " ('shut', 'nigger'),\n",
       " ('nigger', 'whore'),\n",
       " ('whore', 'hope'),\n",
       " ('hope', 'get'),\n",
       " ('get', 'raped'),\n",
       " ('raped', 'one'),\n",
       " ('one', 'animals'),\n",
       " ('animals', 'might'),\n",
       " ('might', 'change'),\n",
       " ('change', 'tune'),\n",
       " ('shut', 'nigger', 'whore'),\n",
       " ('nigger', 'whore', 'hope'),\n",
       " ('whore', 'hope', 'get'),\n",
       " ('hope', 'get', 'raped'),\n",
       " ('get', 'raped', 'one'),\n",
       " ('raped', 'one', 'animals'),\n",
       " ('one', 'animals', 'might'),\n",
       " ('animals', 'might', 'change'),\n",
       " ('might', 'change', 'tune'),\n",
       " 'black',\n",
       " 'bitch',\n",
       " 'next',\n",
       " 'shut',\n",
       " ('black', 'bitch'),\n",
       " ('bitch', 'next'),\n",
       " ('next', 'shut'),\n",
       " ('black', 'bitch', 'next'),\n",
       " ('bitch', 'next', 'shut'),\n",
       " 'fucking',\n",
       " 'chinks',\n",
       " ('fucking', 'chinks'),\n",
       " 'coloured',\n",
       " 'fuckers',\n",
       " 'leave',\n",
       " 'country',\n",
       " ('coloured', 'fuckers'),\n",
       " ('fuckers', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('coloured', 'fuckers', 'leave'),\n",
       " ('fuckers', 'leave', 'country'),\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'cunt'),\n",
       " ('fucking', 'paki', 'cunt'),\n",
       " 'brexit',\n",
       " 'time',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " ('brexit', 'time'),\n",
       " ('time', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('brexit', 'time', 'fuck'),\n",
       " ('time', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'polish'),\n",
       " ('you', 'polish', 'aliens'),\n",
       " 'black',\n",
       " 'nigger',\n",
       " ('black', 'nigger'),\n",
       " 'least',\n",
       " 'nigger',\n",
       " 'lmfao',\n",
       " ('least', 'nigger'),\n",
       " ('nigger', 'lmfao'),\n",
       " ('least', 'nigger', 'lmfao'),\n",
       " 'neighbours',\n",
       " 'complained',\n",
       " 'shed',\n",
       " 'front',\n",
       " 'garden',\n",
       " 'whos',\n",
       " 'neighbour',\n",
       " 'church',\n",
       " 'england',\n",
       " 'bunch',\n",
       " 'god',\n",
       " 'bothering',\n",
       " 'cunts',\n",
       " ('neighbours', 'complained'),\n",
       " ('complained', 'shed'),\n",
       " ('shed', 'front'),\n",
       " ('front', 'garden'),\n",
       " ('garden', 'whos'),\n",
       " ('whos', 'neighbour'),\n",
       " ('neighbour', 'church'),\n",
       " ('church', 'england'),\n",
       " ('england', 'bunch'),\n",
       " ('bunch', 'god'),\n",
       " ('god', 'bothering'),\n",
       " ('bothering', 'cunts'),\n",
       " ('neighbours', 'complained', 'shed'),\n",
       " ('complained', 'shed', 'front'),\n",
       " ('shed', 'front', 'garden'),\n",
       " ('front', 'garden', 'whos'),\n",
       " ('garden', 'whos', 'neighbour'),\n",
       " ('whos', 'neighbour', 'church'),\n",
       " ('neighbour', 'church', 'england'),\n",
       " ('church', 'england', 'bunch'),\n",
       " ('england', 'bunch', 'god'),\n",
       " ('bunch', 'god', 'bothering'),\n",
       " ('god', 'bothering', 'cunts'),\n",
       " 'bunch',\n",
       " 'paki',\n",
       " 'perves',\n",
       " ('bunch', 'paki'),\n",
       " ('paki', 'perves'),\n",
       " ('bunch', 'paki', 'perves'),\n",
       " 'fucking',\n",
       " 'hate',\n",
       " 'allah',\n",
       " 'lovers',\n",
       " ('fucking', 'hate'),\n",
       " ('hate', 'allah'),\n",
       " ('allah', 'lovers'),\n",
       " ('fucking', 'hate', 'allah'),\n",
       " ('hate', 'allah', 'lovers'),\n",
       " 'fucking',\n",
       " 'immigrants',\n",
       " ('fucking', 'immigrants'),\n",
       " 'fucking',\n",
       " 'polish',\n",
       " 'aliens',\n",
       " 'coming',\n",
       " 'taking',\n",
       " 'jobs',\n",
       " 'unacceptable',\n",
       " ('fucking', 'polish'),\n",
       " ('polish', 'aliens'),\n",
       " ('aliens', 'coming'),\n",
       " ('coming', 'taking'),\n",
       " ('taking', 'jobs'),\n",
       " ('jobs', 'unacceptable'),\n",
       " ('fucking', 'polish', 'aliens'),\n",
       " ('polish', 'aliens', 'coming'),\n",
       " ('aliens', 'coming', 'taking'),\n",
       " ('coming', 'taking', 'jobs'),\n",
       " ('taking', 'jobs', 'unacceptable'),\n",
       " 'local',\n",
       " 'mosque',\n",
       " 'let',\n",
       " 'terrorists',\n",
       " 'out',\n",
       " ('local', 'mosque'),\n",
       " ('mosque', 'let'),\n",
       " ('let', 'terrorists'),\n",
       " ('terrorists', 'out'),\n",
       " ('local', 'mosque', 'let'),\n",
       " ('mosque', 'let', 'terrorists'),\n",
       " ('let', 'terrorists', 'out'),\n",
       " 'racist',\n",
       " 'sexist',\n",
       " 'perv',\n",
       " 'clearly',\n",
       " 'wants',\n",
       " 'fuck',\n",
       " 'daughter',\n",
       " 'disgusting',\n",
       " ('racist', 'sexist'),\n",
       " ('sexist', 'perv'),\n",
       " ('perv', 'clearly'),\n",
       " ('clearly', 'wants'),\n",
       " ('wants', 'fuck'),\n",
       " ('fuck', 'daughter'),\n",
       " ('daughter', 'disgusting'),\n",
       " ('racist', 'sexist', 'perv'),\n",
       " ('sexist', 'perv', 'clearly'),\n",
       " ('perv', 'clearly', 'wants'),\n",
       " ('clearly', 'wants', 'fuck'),\n",
       " ('wants', 'fuck', 'daughter'),\n",
       " ('fuck', 'daughter', 'disgusting'),\n",
       " 'pakis',\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'please',\n",
       " ('pakis', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('off', 'please'),\n",
       " ('pakis', 'fuck', 'off'),\n",
       " ('fuck', 'off', 'please'),\n",
       " 'hate',\n",
       " 'niggers',\n",
       " 'hate',\n",
       " 'faggots',\n",
       " 'hate',\n",
       " 'spicskkk',\n",
       " 'rally',\n",
       " ('hate', 'niggers'),\n",
       " ('niggers', 'hate'),\n",
       " ('hate', 'faggots'),\n",
       " ('faggots', 'hate'),\n",
       " ('hate', 'spicskkk'),\n",
       " ('spicskkk', 'rally'),\n",
       " ('hate', 'niggers', 'hate'),\n",
       " ('niggers', 'hate', 'faggots'),\n",
       " ('hate', 'faggots', 'hate'),\n",
       " ('faggots', 'hate', 'spicskkk'),\n",
       " ('hate', 'spicskkk', 'rally'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'cunts',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'cunts'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'cunts'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'paki'),\n",
       " ('fucking', 'paki', 'bastard'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'myriah',\n",
       " 'chavcentral',\n",
       " 'chav',\n",
       " 'central',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'myriah'),\n",
       " ('myriah', 'chavcentral'),\n",
       " ('chavcentral', 'chav'),\n",
       " ('chav', 'central'),\n",
       " ('fuck', 'off', 'myriah'),\n",
       " ('off', 'myriah', 'chavcentral'),\n",
       " ('myriah', 'chavcentral', 'chav'),\n",
       " ('chavcentral', 'chav', 'central'),\n",
       " 'diy',\n",
       " 'terrorist',\n",
       " 'religion',\n",
       " 'fucking',\n",
       " 'joke',\n",
       " 'you',\n",
       " 'go',\n",
       " 'around',\n",
       " 'screaming',\n",
       " 'allah',\n",
       " 'akbar',\n",
       " 'terrorist',\n",
       " 'shit',\n",
       " 'diy',\n",
       " 'faggot',\n",
       " ('diy', 'terrorist'),\n",
       " ('terrorist', 'religion'),\n",
       " ('religion', 'fucking'),\n",
       " ('fucking', 'joke'),\n",
       " ('joke', 'you'),\n",
       " ('you', 'go'),\n",
       " ('go', 'around'),\n",
       " ('around', 'screaming'),\n",
       " ('screaming', 'allah'),\n",
       " ('allah', 'akbar'),\n",
       " ('akbar', 'terrorist'),\n",
       " ('terrorist', 'shit'),\n",
       " ('shit', 'diy'),\n",
       " ('diy', 'faggot'),\n",
       " ('diy', 'terrorist', 'religion'),\n",
       " ('terrorist', 'religion', 'fucking'),\n",
       " ('religion', 'fucking', 'joke'),\n",
       " ('fucking', 'joke', 'you'),\n",
       " ('joke', 'you', 'go'),\n",
       " ('you', 'go', 'around'),\n",
       " ('go', 'around', 'screaming'),\n",
       " ('around', 'screaming', 'allah'),\n",
       " ('screaming', 'allah', 'akbar'),\n",
       " ('allah', 'akbar', 'terrorist'),\n",
       " ('akbar', 'terrorist', 'shit'),\n",
       " ('terrorist', 'shit', 'diy'),\n",
       " ('shit', 'diy', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'back',\n",
       " 'poland',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'back'),\n",
       " ('back', 'poland'),\n",
       " ('fuck', 'off', 'back'),\n",
       " ('off', 'back', 'poland'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'you',\n",
       " 'scum',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'you'),\n",
       " ('you', 'scum'),\n",
       " ('fuck', 'off', 'you'),\n",
       " ('off', 'you', 'scum'),\n",
       " 'shut',\n",
       " 'fuck',\n",
       " ('shut', 'fuck'),\n",
       " 'fucking',\n",
       " 'sand',\n",
       " 'monkeys',\n",
       " ('fucking', 'sand'),\n",
       " ('sand', 'monkeys'),\n",
       " ('fucking', 'sand', 'monkeys'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('you', 'fucking', 'faggot'),\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'irish',\n",
       " 'cow',\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'irish'),\n",
       " ('irish', 'cow'),\n",
       " ('you', 'fucking', 'irish'),\n",
       " ('fucking', 'irish', 'cow'),\n",
       " 'paki',\n",
       " 'cunt',\n",
       " ('paki', 'cunt'),\n",
       " 'you',\n",
       " 'muslim',\n",
       " 'prick',\n",
       " ('you', 'muslim'),\n",
       " ('muslim', 'prick'),\n",
       " ('you', 'muslim', 'prick'),\n",
       " 'many',\n",
       " 'wogs',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'brexit',\n",
       " ('many', 'wogs'),\n",
       " ('wogs', 'leave'),\n",
       " ('leave', 'country'),\n",
       " ('country', 'brexit'),\n",
       " ('many', 'wogs', 'leave'),\n",
       " ('wogs', 'leave', 'country'),\n",
       " ('leave', 'country', 'brexit'),\n",
       " 'least',\n",
       " 'look',\n",
       " 'like',\n",
       " 'jefree',\n",
       " 'starr',\n",
       " 'faggot',\n",
       " ('least', 'look'),\n",
       " ('look', 'like'),\n",
       " ('like', 'jefree'),\n",
       " ('jefree', 'starr'),\n",
       " ('starr', 'faggot'),\n",
       " ('least', 'look', 'like'),\n",
       " ('look', 'like', 'jefree'),\n",
       " ('like', 'jefree', 'starr'),\n",
       " ('jefree', 'starr', 'faggot'),\n",
       " 'stupid',\n",
       " 'bitch',\n",
       " ('stupid', 'bitch'),\n",
       " 'benjamin',\n",
       " 'fucking',\n",
       " 'faggot',\n",
       " ('benjamin', 'fucking'),\n",
       " ('fucking', 'faggot'),\n",
       " ('benjamin', 'fucking', 'faggot'),\n",
       " 'good',\n",
       " 'night',\n",
       " 'fags',\n",
       " 'fagettes',\n",
       " 'female',\n",
       " 'version',\n",
       " 'fags',\n",
       " 'like',\n",
       " 'fagettes',\n",
       " 'make',\n",
       " 'female',\n",
       " 'hate',\n",
       " 'you',\n",
       " ('good', 'night'),\n",
       " ('night', 'fags'),\n",
       " ('fags', 'fagettes'),\n",
       " ('fagettes', 'female'),\n",
       " ('female', 'version'),\n",
       " ('version', 'fags'),\n",
       " ('fags', 'like'),\n",
       " ('like', 'fagettes'),\n",
       " ('fagettes', 'make'),\n",
       " ('make', 'female'),\n",
       " ('female', 'hate'),\n",
       " ('hate', 'you'),\n",
       " ('good', 'night', 'fags'),\n",
       " ('night', 'fags', 'fagettes'),\n",
       " ('fags', 'fagettes', 'female'),\n",
       " ('fagettes', 'female', 'version'),\n",
       " ('female', 'version', 'fags'),\n",
       " ('version', 'fags', 'like'),\n",
       " ('fags', 'like', 'fagettes'),\n",
       " ('like', 'fagettes', 'make'),\n",
       " ('fagettes', 'make', 'female'),\n",
       " ('make', 'female', 'hate'),\n",
       " ('female', 'hate', 'you'),\n",
       " 'thick',\n",
       " 'scottish',\n",
       " 'bastard',\n",
       " ('thick', 'scottish'),\n",
       " ('scottish', 'bastard'),\n",
       " ('thick', 'scottish', 'bastard'),\n",
       " 'bastard',\n",
       " 'wog',\n",
       " 'road',\n",
       " 'annoying',\n",
       " ('bastard', 'wog'),\n",
       " ('wog', 'road'),\n",
       " ('road', 'annoying'),\n",
       " ('bastard', 'wog', 'road'),\n",
       " ('wog', 'road', 'annoying'),\n",
       " 'polish',\n",
       " 'vermin',\n",
       " ('polish', 'vermin'),\n",
       " 'diy',\n",
       " 'paki',\n",
       " ('diy', 'paki'),\n",
       " 'used',\n",
       " 'tie',\n",
       " 'ends',\n",
       " 'niggers',\n",
       " 'legs',\n",
       " '2',\n",
       " 'different',\n",
       " 'horses',\n",
       " 'beat',\n",
       " 'horses',\n",
       " 'spread',\n",
       " 'legs',\n",
       " 'break',\n",
       " 'smh',\n",
       " ('used', 'tie'),\n",
       " ('tie', 'ends'),\n",
       " ('ends', 'niggers'),\n",
       " ('niggers', 'legs'),\n",
       " ('legs', '2'),\n",
       " ('2', 'different'),\n",
       " ('different', 'horses'),\n",
       " ('horses', 'beat'),\n",
       " ('beat', 'horses'),\n",
       " ('horses', 'spread'),\n",
       " ('spread', 'legs'),\n",
       " ('legs', 'break'),\n",
       " ('break', 'smh'),\n",
       " ('used', 'tie', 'ends'),\n",
       " ('tie', 'ends', 'niggers'),\n",
       " ('ends', 'niggers', 'legs'),\n",
       " ('niggers', 'legs', '2'),\n",
       " ('legs', '2', 'different'),\n",
       " ('2', 'different', 'horses'),\n",
       " ('different', 'horses', 'beat'),\n",
       " ('horses', 'beat', 'horses'),\n",
       " ('beat', 'horses', 'spread'),\n",
       " ('horses', 'spread', 'legs'),\n",
       " ('spread', 'legs', 'break'),\n",
       " ('legs', 'break', 'smh'),\n",
       " 'polish',\n",
       " 'bitch',\n",
       " ('polish', 'bitch'),\n",
       " 'last',\n",
       " 'time',\n",
       " 'preston',\n",
       " 'anymore',\n",
       " 'inconsiderate',\n",
       " 'twats',\n",
       " 'm6m61',\n",
       " 'way',\n",
       " 'home',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'cut',\n",
       " 'bitch',\n",
       " ('last', 'time'),\n",
       " ('time', 'preston'),\n",
       " ('preston', 'anymore'),\n",
       " ('anymore', 'inconsiderate'),\n",
       " ('inconsiderate', 'twats'),\n",
       " ('twats', 'm6m61'),\n",
       " ('m6m61', 'way'),\n",
       " ('way', 'home'),\n",
       " ('home', 'gon'),\n",
       " ('gon', 'na'),\n",
       " ('na', 'cut'),\n",
       " ('cut', 'bitch'),\n",
       " ('last', 'time', 'preston'),\n",
       " ('time', 'preston', 'anymore'),\n",
       " ('preston', 'anymore', 'inconsiderate'),\n",
       " ('anymore', 'inconsiderate', 'twats'),\n",
       " ('inconsiderate', 'twats', 'm6m61'),\n",
       " ('twats', 'm6m61', 'way'),\n",
       " ('m6m61', 'way', 'home'),\n",
       " ('way', 'home', 'gon'),\n",
       " ('home', 'gon', 'na'),\n",
       " ('gon', 'na', 'cut'),\n",
       " ('na', 'cut', 'bitch'),\n",
       " 'nigger',\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " 'fuck',\n",
       " 'anyone',\n",
       " 'british',\n",
       " ('hate', 'immigrants'),\n",
       " ('immigrants', 'fuck'),\n",
       " ('fuck', 'anyone'),\n",
       " ('anyone', 'british'),\n",
       " ('hate', 'immigrants', 'fuck'),\n",
       " ('immigrants', 'fuck', 'anyone'),\n",
       " ('fuck', 'anyone', 'british'),\n",
       " 'you',\n",
       " 'even',\n",
       " 'country',\n",
       " 'you',\n",
       " 'pakis',\n",
       " ('you', 'even'),\n",
       " ('even', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'pakis'),\n",
       " ('you', 'even', 'country'),\n",
       " ('even', 'country', 'you'),\n",
       " ('country', 'you', 'pakis'),\n",
       " 'fucking',\n",
       " 'white',\n",
       " 'bitch',\n",
       " 'road',\n",
       " 'needs',\n",
       " 'out',\n",
       " ('fucking', 'white'),\n",
       " ('white', 'bitch'),\n",
       " ('bitch', 'road'),\n",
       " ('road', 'needs'),\n",
       " ('needs', 'out'),\n",
       " ('fucking', 'white', 'bitch'),\n",
       " ('white', 'bitch', 'road'),\n",
       " ('bitch', 'road', 'needs'),\n",
       " ('road', 'needs', 'out'),\n",
       " 'get',\n",
       " 'out',\n",
       " 'country',\n",
       " 'you',\n",
       " 'fucking',\n",
       " 'foreigners',\n",
       " ('get', 'out'),\n",
       " ('out', 'country'),\n",
       " ('country', 'you'),\n",
       " ('you', 'fucking'),\n",
       " ('fucking', 'foreigners'),\n",
       " ('get', 'out', 'country'),\n",
       " ('out', 'country', 'you'),\n",
       " ('country', 'you', 'fucking'),\n",
       " ('you', 'fucking', 'foreigners'),\n",
       " 'enough',\n",
       " 'fucking',\n",
       " 'pakis',\n",
       " 'man',\n",
       " 'time',\n",
       " 'leave',\n",
       " 'europe',\n",
       " ('enough', 'fucking'),\n",
       " ('fucking', 'pakis'),\n",
       " ('pakis', 'man'),\n",
       " ('man', 'time'),\n",
       " ('time', 'leave'),\n",
       " ('leave', 'europe'),\n",
       " ('enough', 'fucking', 'pakis'),\n",
       " ('fucking', 'pakis', 'man'),\n",
       " ('pakis', 'man', 'time'),\n",
       " ('man', 'time', 'leave'),\n",
       " ('time', 'leave', 'europe'),\n",
       " 'kill',\n",
       " 'you',\n",
       " 'you',\n",
       " 'paki',\n",
       " 'bastard',\n",
       " ('kill', 'you'),\n",
       " ('you', 'you'),\n",
       " ('you', 'paki'),\n",
       " ('paki', 'bastard'),\n",
       " ('kill', 'you', 'you'),\n",
       " ('you', 'you', 'paki'),\n",
       " ('you', 'paki', 'bastard'),\n",
       " 'you',\n",
       " 'ass',\n",
       " 'prick',\n",
       " ('you', 'ass'),\n",
       " ('ass', 'prick'),\n",
       " ('you', 'ass', 'prick'),\n",
       " 'smelly',\n",
       " 'pakis',\n",
       " ('smelly', 'pakis'),\n",
       " 'niggers',\n",
       " 'road',\n",
       " 'better',\n",
       " 'stop',\n",
       " 'shouting',\n",
       " 'go',\n",
       " 'round',\n",
       " 'shut',\n",
       " 'them',\n",
       " ('niggers', 'road'),\n",
       " ('road', 'better'),\n",
       " ('better', 'stop'),\n",
       " ('stop', 'shouting'),\n",
       " ('shouting', 'go'),\n",
       " ('go', 'round'),\n",
       " ('round', 'shut'),\n",
       " ('shut', 'them'),\n",
       " ('niggers', 'road', 'better'),\n",
       " ('road', 'better', 'stop'),\n",
       " ('better', 'stop', 'shouting'),\n",
       " ('stop', 'shouting', 'go'),\n",
       " ('shouting', 'go', 'round'),\n",
       " ('go', 'round', 'shut'),\n",
       " ('round', 'shut', 'them'),\n",
       " 'fucking',\n",
       " 'bitch',\n",
       " ('fucking', 'bitch'),\n",
       " 'cunt',\n",
       " 'fuck',\n",
       " 'off',\n",
       " ('cunt', 'fuck'),\n",
       " ('fuck', 'off'),\n",
       " ('cunt', 'fuck', 'off'),\n",
       " 'bastard',\n",
       " 'muslim',\n",
       " 'dogs',\n",
       " ('bastard', 'muslim'),\n",
       " ('muslim', 'dogs'),\n",
       " ('bastard', 'muslim', 'dogs'),\n",
       " 'fuck',\n",
       " 'dykes',\n",
       " ('fuck', 'dykes'),\n",
       " 'hate',\n",
       " 'immigrants',\n",
       " ('hate', 'immigrants'),\n",
       " 'fuck',\n",
       " 'jezanie',\n",
       " 'cunts',\n",
       " 'cbb',\n",
       " ('fuck', 'jezanie'),\n",
       " ('jezanie', 'cunts'),\n",
       " ('cunts', 'cbb'),\n",
       " ('fuck', 'jezanie', 'cunts'),\n",
       " ('jezanie', 'cunts', 'cbb'),\n",
       " 'fat',\n",
       " 'little',\n",
       " 'wanker',\n",
       " 'you',\n",
       " 'problem',\n",
       " 'say',\n",
       " 'face',\n",
       " 'behind',\n",
       " 'back',\n",
       " 'faggot',\n",
       " ('fat', 'little'),\n",
       " ('little', 'wanker'),\n",
       " ('wanker', 'you'),\n",
       " ('you', 'problem'),\n",
       " ('problem', 'say'),\n",
       " ('say', 'face'),\n",
       " ('face', 'behind'),\n",
       " ('behind', 'back'),\n",
       " ('back', 'faggot'),\n",
       " ('fat', 'little', 'wanker'),\n",
       " ('little', 'wanker', 'you'),\n",
       " ('wanker', 'you', 'problem'),\n",
       " ('you', 'problem', 'say'),\n",
       " ('problem', 'say', 'face'),\n",
       " ('say', 'face', 'behind'),\n",
       " ('face', 'behind', 'back'),\n",
       " ('behind', 'back', 'faggot'),\n",
       " 'fuck',\n",
       " 'off',\n",
       " 'immigrants',\n",
       " 'brexit',\n",
       " ('fuck', 'off'),\n",
       " ('off', 'immigrants'),\n",
       " ('immigrants', 'brexit'),\n",
       " ('fuck', 'off', 'immigrants'),\n",
       " ('off', 'immigrants', 'brexit'),\n",
       " 'fuck',\n",
       " 'you',\n",
       " 'nigger',\n",
       " 'sheboon',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'strung',\n",
       " 'like',\n",
       " 'niggers',\n",
       " 'whitepower',\n",
       " '1488',\n",
       " ('fuck', 'you'),\n",
       " ('you', 'nigger'),\n",
       " ('nigger', 'sheboon'),\n",
       " ('sheboon', 'hope'),\n",
       " ('hope', 'you'),\n",
       " ('you', 'strung'),\n",
       " ('strung', 'like'),\n",
       " ('like', 'niggers'),\n",
       " ('niggers', 'whitepower'),\n",
       " ('whitepower', '1488'),\n",
       " ('fuck', 'you', 'nigger'),\n",
       " ('you', 'nigger', 'sheboon'),\n",
       " ('nigger', 'sheboon', 'hope'),\n",
       " ('sheboon', 'hope', 'you'),\n",
       " ('hope', 'you', 'strung'),\n",
       " ('you', 'strung', 'like'),\n",
       " ('strung', 'like', 'niggers'),\n",
       " ('like', 'niggers', 'whitepower'),\n",
       " ('niggers', 'whitepower', '1488'),\n",
       " 'bitch',\n",
       " 'immigrants',\n",
       " 'fucking',\n",
       " 'terrorists',\n",
       " ('immigrants', 'fucking'),\n",
       " ('fucking', 'terrorists'),\n",
       " ('immigrants', 'fucking', 'terrorists'),\n",
       " 'come',\n",
       " 'out',\n",
       " 'you',\n",
       " 'black',\n",
       " 'bastard',\n",
       " ('come', 'out'),\n",
       " ('out', 'you'),\n",
       " ('you', 'black'),\n",
       " ('black', 'bastard'),\n",
       " ('come', 'out', 'you'),\n",
       " ('out', 'you', 'black'),\n",
       " ('you', 'black', 'bastard'),\n",
       " 'go',\n",
       " 'fuck',\n",
       " 'yourself',\n",
       " 'you',\n",
       " 'cunt',\n",
       " ('go', 'fuck'),\n",
       " ('fuck', 'yourself'),\n",
       " ('yourself', 'you'),\n",
       " ('you', 'cunt'),\n",
       " ('go', 'fuck', 'yourself'),\n",
       " ('fuck', 'yourself', 'you'),\n",
       " ('yourself', 'you', 'cunt'),\n",
       " 'go',\n",
       " 'home',\n",
       " ...]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some of the tokens are duplicates. This could be because (1) they are repeated within a tweet or (2) they are present in multiple tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that features are tokens or combination of tokens (ngrams).\n",
    "\n",
    "Let's have a look at how frequent each feature is in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAE4CAYAAAC0d+/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(vocab)\n",
    "fd.plot(20, cumulative=True)\n",
    "#fd.xlabel('Most common features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 46),\n",
       " ('fucking', 33),\n",
       " ('off', 30),\n",
       " ('fuck', 27),\n",
       " ('get', 19),\n",
       " ('go', 18),\n",
       " ('hate', 17),\n",
       " (('fuck', 'off'), 16),\n",
       " ('bastard', 13),\n",
       " ('bitch', 12),\n",
       " ('out', 12),\n",
       " ('paki', 10),\n",
       " ('immigrants', 10),\n",
       " ('back', 10),\n",
       " ('home', 9),\n",
       " ('country', 9),\n",
       " ('like', 9),\n",
       " ('polish', 9),\n",
       " ('leave', 8),\n",
       " (('off', 'you'), 7),\n",
       " ('time', 7),\n",
       " ('niggers', 7),\n",
       " ('people', 6),\n",
       " ('black', 6),\n",
       " ('much', 6),\n",
       " ('terrorists', 6),\n",
       " ('cunt', 6),\n",
       " (('fuck', 'off', 'you'), 6),\n",
       " ('many', 6),\n",
       " ('faggot', 6),\n",
       " ('from', 6),\n",
       " ('never', 6),\n",
       " ('muslim', 5),\n",
       " ('nigger', 5),\n",
       " ('brexit', 5),\n",
       " ('let', 5),\n",
       " ('pakis', 5),\n",
       " ('road', 5),\n",
       " ('today', 4),\n",
       " ('cunts', 4),\n",
       " ('them', 4),\n",
       " ('shut', 4),\n",
       " ('one', 4),\n",
       " (('you', 'fucking'), 4),\n",
       " ('scum', 4),\n",
       " ('good', 4),\n",
       " ('right', 4),\n",
       " ('new', 4),\n",
       " ('really', 4),\n",
       " ('respect', 3)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important for model running time and accuracy of the model: only give the classifier what matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing rare and common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping the 50 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features (tokens of ngrams) are only present in one or two tweet. We know that these are not going to be very useful to teach the model to recognise hate speech.  Let's only keep the top 50 most frequent features in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of interest here are the top 50 most frequent words. THIS IS NOT THE BEST WAY OF DOING IT! IT SHOULD REMOVE THE MOST FREQUENT WORDS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chosen features: 50/3199\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "\n",
    "def get_word_features(wordlist, n):\n",
    "    fd = nltk.FreqDist(wordlist)\n",
    "    \n",
    "    word_features = sorted(fd.items(), key=operator.itemgetter(1), reverse=True)[0:n] \n",
    "    word_features = [i[0] for i in word_features ]\n",
    "    return word_features\n",
    "\n",
    "# Only keep the top 50 most frequent words\n",
    "chosen_features = get_word_features(vocab, 50)\n",
    "print('Number of chosen features: {}/{}'.format(len(chosen_features), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'fucking', 'off', 'fuck', 'get', 'go', 'hate', ('fuck', 'off'), 'bastard', 'bitch', 'out', 'paki', 'immigrants', 'back', 'home', 'country', 'like', 'polish', 'leave', ('off', 'you'), 'time', 'niggers', 'people', 'black', 'much', 'terrorists', 'cunt', ('fuck', 'off', 'you'), 'many', 'faggot', 'from', 'never', 'muslim', 'nigger', 'brexit', 'let', 'pakis', 'road', 'today', 'cunts', 'them', 'shut', 'one', ('you', 'fucking'), 'scum', 'good', 'right', 'new', 'really', 'respect']\n"
     ]
    }
   ],
   "source": [
    "print(chosen_features[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input data for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have chosen a list of features that we think are important for the model to learn the difference between hateful speech and non-hateful speech.\n",
    "\n",
    "We now need to somehow tell the model:\n",
    "- which features are typically present in hateful tweets and which are not,\n",
    "- which features are typically present in non-hateful tweets and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pandas dataframes are handy to manipulate data, the classifier needs each tweet to be a dictionary of words mapped to True booleans. Something like this: \n",
    "\n",
    "[({'contains('go', 'home')' : TRUE, 'contains('good')' : FALSE}, 1), ({'contains('love')' : TRUE}, 0)]\n",
    "\n",
    "0 and 1 are the labels:\n",
    "- 1 means hateful\n",
    "- 0 means non-hateful\n",
    "\n",
    "Let's make this list of tuples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    feature_set = {}\n",
    "    for feature in chosen_features:\n",
    "        feature_set['contains({})'.format(feature)] = (feature in document_words)\n",
    "    return feature_set\n",
    "\n",
    "tweets = [tuple(x) for x in pre_processed_data.values]\n",
    "\n",
    "feature_set = nltk.classify.apply_features(extract_features, tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to feed in training_set: 202\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets to feed in training_set: {}'.format(len(feature_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets' look at the first tweet. When scrolling all the way to the end, we see that this is a hateful tweet (label = 1). This will help the classifier know which features lead to being labelled hateful and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\"contains(('fuck', 'off'))\": False,\n",
       "  \"contains(('fuck', 'off', 'you'))\": False,\n",
       "  \"contains(('off', 'you'))\": False,\n",
       "  \"contains(('you', 'fucking'))\": False,\n",
       "  'contains(back)': False,\n",
       "  'contains(bastard)': False,\n",
       "  'contains(bitch)': False,\n",
       "  'contains(black)': False,\n",
       "  'contains(brexit)': False,\n",
       "  'contains(country)': False,\n",
       "  'contains(cunt)': False,\n",
       "  'contains(cunts)': False,\n",
       "  'contains(faggot)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(fuck)': False,\n",
       "  'contains(fucking)': False,\n",
       "  'contains(get)': False,\n",
       "  'contains(go)': False,\n",
       "  'contains(good)': False,\n",
       "  'contains(hate)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(immigrants)': False,\n",
       "  'contains(leave)': False,\n",
       "  'contains(let)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(many)': False,\n",
       "  'contains(much)': False,\n",
       "  'contains(muslim)': False,\n",
       "  'contains(never)': False,\n",
       "  'contains(new)': False,\n",
       "  'contains(nigger)': False,\n",
       "  'contains(niggers)': False,\n",
       "  'contains(off)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(paki)': True,\n",
       "  'contains(pakis)': False,\n",
       "  'contains(people)': True,\n",
       "  'contains(polish)': False,\n",
       "  'contains(really)': False,\n",
       "  'contains(respect)': True,\n",
       "  'contains(right)': False,\n",
       "  'contains(road)': False,\n",
       "  'contains(scum)': False,\n",
       "  'contains(shut)': False,\n",
       "  'contains(terrorists)': False,\n",
       "  'contains(them)': False,\n",
       "  'contains(time)': False,\n",
       "  'contains(today)': False,\n",
       "  'contains(you)': False},\n",
       " 1)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method is pretty simple. For each tweet, we are looping through our chosen_features and setting a boolean to True if the tweet contains that feature, False otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is what the classifier needs as input.\n",
    "\n",
    "We can now train the classifier with this training_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train vs test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train the classifer and then test its classifying ability on a brand new dataset that it has never seen before. \n",
    "\n",
    "Generally, 80/20 percent is a fair split between training and testing set:\n",
    "- training dataset (80% of the data)\n",
    "- testing dataset (20% of the data)\n",
    "\n",
    "Sklearn provides a function called train_test_split to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in train data: 161\n",
      "Number of tweets in test data: 41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(feature_set, test_size=0.20, train_size=0.80)\n",
    "print('Number of tweets in train data: {}'.format(len(train_data)))\n",
    "print('Number of tweets in test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\"contains(('fuck', 'off'))\": False,\n",
       "  \"contains(('fuck', 'off', 'you'))\": False,\n",
       "  \"contains(('off', 'you'))\": False,\n",
       "  \"contains(('you', 'fucking'))\": False,\n",
       "  'contains(back)': False,\n",
       "  'contains(bastard)': False,\n",
       "  'contains(bitch)': False,\n",
       "  'contains(black)': False,\n",
       "  'contains(brexit)': False,\n",
       "  'contains(country)': False,\n",
       "  'contains(cunt)': False,\n",
       "  'contains(cunts)': False,\n",
       "  'contains(faggot)': False,\n",
       "  'contains(from)': False,\n",
       "  'contains(fuck)': False,\n",
       "  'contains(fucking)': False,\n",
       "  'contains(get)': False,\n",
       "  'contains(go)': False,\n",
       "  'contains(good)': False,\n",
       "  'contains(hate)': False,\n",
       "  'contains(home)': False,\n",
       "  'contains(immigrants)': False,\n",
       "  'contains(leave)': False,\n",
       "  'contains(let)': False,\n",
       "  'contains(like)': False,\n",
       "  'contains(many)': False,\n",
       "  'contains(much)': False,\n",
       "  'contains(muslim)': False,\n",
       "  'contains(never)': False,\n",
       "  'contains(new)': False,\n",
       "  'contains(nigger)': False,\n",
       "  'contains(niggers)': False,\n",
       "  'contains(off)': False,\n",
       "  'contains(one)': False,\n",
       "  'contains(out)': False,\n",
       "  'contains(paki)': False,\n",
       "  'contains(pakis)': False,\n",
       "  'contains(people)': False,\n",
       "  'contains(polish)': False,\n",
       "  'contains(really)': False,\n",
       "  'contains(respect)': False,\n",
       "  'contains(right)': False,\n",
       "  'contains(road)': False,\n",
       "  'contains(scum)': False,\n",
       "  'contains(shut)': False,\n",
       "  'contains(terrorists)': False,\n",
       "  'contains(them)': False,\n",
       "  'contains(time)': False,\n",
       "  'contains(today)': False,\n",
       "  'contains(you)': False},\n",
       " 0)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of model to use as a classifier. The most common one is called Naive Bayesion Classifier and that is the one we are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "    contains(immigrants) = True                1 : 0      =      5.7 : 1.0\n",
      "          contains(fuck) = True                1 : 0      =      5.0 : 1.0\n",
      "    contains(terrorists) = True                1 : 0      =      4.3 : 1.0\n",
      "       contains(fucking) = True                1 : 0      =      3.9 : 1.0\n",
      "          contains(hate) = True                1 : 0      =      3.3 : 1.0\n",
      "          contains(good) = True                0 : 1      =      2.3 : 1.0\n",
      "          contains(time) = True                1 : 0      =      2.3 : 1.0\n",
      "          contains(come) = True                0 : 1      =      2.3 : 1.0\n",
      "         contains(black) = True                1 : 0      =      2.2 : 1.0\n",
      "        contains(polish) = True                1 : 0      =      2.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayesian\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(training_set)\n",
    "# SHOW FEATURES\n",
    "classifier1.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "# Save the model into a pickle file\n",
    "import pickle\n",
    "f = open('classifier.pickle', 'wb')\n",
    "pickle.dump(classifier1, f)\n",
    "f.close()\n",
    "\n",
    "# DecisionTree\n",
    "#classifier2 = nltk.classify.DecisionTreeClassifier.train(training_set, entropy_cutoff=0,support_cutoff=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! The model has been trained on the training_set. \n",
    "\n",
    "We can see which features the model considers important to decide between hateful speech and non-hateful speech.\n",
    "\n",
    "- Column 3 shows the ratio of occurence of each informative feature in both labels (hate is 1 : non-hate is 0).\n",
    "- Column 2 shows you the direction of the ratio (which label occurs more frequently). The label on the left is the label most associated with the corresponding feature.\n",
    "\n",
    "For example, tweets containing the word 'immigrants' are 5.7 times more likely to be hateful than not.\n",
    "\n",
    "Now let's test the accuracy of our model on the test_data that we set aside earlier. These are tweets that the model has never seen before. We'll ask the model to classify them and see how its outcome compares with the true label of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8536585365853658"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =  nltk.classify.util.accuracy(classifier1, test_data)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Use the classifier to identify hateful speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's try our classifier on a new tweet of your choice. First we need to preprocess the tweet (clean, tokenize and remove stopwords). Then we need to extract its features to look like the right input for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testTweet = 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed tweet: ['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tweet\n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # clean the tweet\n",
    "    tweet = cleanTweet(testTweet)\n",
    "    \n",
    "    # tokenize the cleaned tweet\n",
    "    tokenised_tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stop words\n",
    "    tokenised_tweet_stpwd = [item for item in tokenised_tweet if item not in stopWords]\n",
    "    print('Preprocessed tweet: {}'.format(tokenised_tweet_stpwd))\n",
    "    \n",
    "    return tokenised_tweet_stpwd\n",
    "\n",
    "\n",
    "preprocessed_tweet = preprocessTweet(testTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains(you)': False, 'contains(fucking)': False, 'contains(off)': False, 'contains(fuck)': False, 'contains(get)': False, 'contains(go)': False, 'contains(hate)': False, \"contains(('fuck', 'off'))\": False, 'contains(bastard)': False, 'contains(bitch)': False, 'contains(out)': False, 'contains(paki)': False, 'contains(immigrants)': False, 'contains(back)': False, 'contains(home)': False, 'contains(country)': False, 'contains(like)': False, 'contains(polish)': False, 'contains(leave)': False, \"contains(('off', 'you'))\": False, 'contains(time)': False, 'contains(niggers)': False, 'contains(people)': False, 'contains(black)': False, 'contains(much)': False, 'contains(terrorists)': False, 'contains(cunt)': False, \"contains(('fuck', 'off', 'you'))\": False, 'contains(many)': False, 'contains(faggot)': False, 'contains(from)': False, 'contains(never)': False, 'contains(muslim)': False, 'contains(nigger)': False, 'contains(brexit)': False, 'contains(let)': False, 'contains(pakis)': False, 'contains(road)': False, 'contains(today)': False, 'contains(cunts)': False, 'contains(them)': False, 'contains(shut)': False, 'contains(one)': False, \"contains(('you', 'fucking'))\": False, 'contains(scum)': False, 'contains(good)': False, 'contains(right)': False, 'contains(new)': False, 'contains(really)': False, 'contains(respect)': False}\n"
     ]
    }
   ],
   "source": [
    "# extract features\n",
    "tweet_feature_set = extract_features(preprocessed_tweet) \n",
    "print(tweet_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier1.classify(tweet_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
